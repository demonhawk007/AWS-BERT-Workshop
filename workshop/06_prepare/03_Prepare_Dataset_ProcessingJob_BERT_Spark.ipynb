{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with Amazon a SageMaker Processing Job and Apache Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Apache Spark are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Apache Spark in a managed SageMaker environment to run our processing workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)\n",
    "\n",
    "![](img/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-393371431575/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 17:13:26   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-07-25 17:13:29   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Spark Docker Image to Run the Processing Job\n",
    "\n",
    "An example Spark container is included in the `./container` directory of this example. The container handles the bootstrapping of all Spark configuration, and serves as a wrapper around the `spark-submit` CLI. At a high level the container provides:\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed Spark application that performs our dataset processing.\n",
    "\n",
    "Build the example Spark container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-processor'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.441MB\n",
      "Step 1/37 : FROM openjdk:8-jre-slim\n",
      " ---> f2e91f81bf2c\n",
      "Step 2/37 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> a2e6bbe0e5e2\n",
      "Step 3/37 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> d89d8588c27a\n",
      "Step 4/37 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> 9c4ed3a036d1\n",
      "Step 5/37 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 4c0b61a8e5a6\n",
      "Step 6/37 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> a29d440c3512\n",
      "Step 7/37 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 44e45fa97bce\n",
      "Step 8/37 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 3a635a1b310f\n",
      "Step 9/37 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> 00533a1ded45\n",
      "Step 10/37 : ENV HADOOP_VERSION 3.2.1\n",
      " ---> Using cache\n",
      " ---> 56e303fe9daa\n",
      "Step 11/37 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> bd2df5877dee\n",
      "Step 12/37 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> 6189ad488052\n",
      "Step 13/37 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> 12a5e364a8ed\n",
      "Step 14/37 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Using cache\n",
      " ---> 98ba1276c7f8\n",
      "Step 15/37 : ENV SPARK_VERSION 2.4.6\n",
      " ---> Using cache\n",
      " ---> 64cb230b7a57\n",
      "Step 16/37 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Using cache\n",
      " ---> 1940b57ce3d8\n",
      "Step 17/37 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Using cache\n",
      " ---> 35f8ea4b7276\n",
      "Step 18/37 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Using cache\n",
      " ---> 6b6747835e06\n",
      "Step 19/37 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Using cache\n",
      " ---> a29a1fcdd319\n",
      "Step 20/37 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> bcbfea97284f\n",
      "Step 21/37 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Using cache\n",
      " ---> 0ec54dee1cae\n",
      "Step 22/37 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 33266959a9b5\n",
      "Step 23/37 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 83967dfd82da\n",
      "Step 24/37 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> c7bce1cee8ef\n",
      "Step 25/37 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> aa534af44386\n",
      "Step 26/37 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 41f36fd517f3\n",
      "Step 27/37 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 8a79798967a5\n",
      "Step 28/37 : COPY program /opt/program\n",
      " ---> d23184f6cebf\n",
      "Step 29/37 : RUN chmod +x /opt/program/submit\n",
      " ---> Running in b63edbb5d45c\n",
      "Removing intermediate container b63edbb5d45c\n",
      " ---> be80bc6257e8\n",
      "Step 30/37 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> f3fd0c3ef556\n",
      "Step 31/37 : COPY jars /usr/jars\n",
      " ---> 73b714a155f1\n",
      "Step 32/37 : WORKDIR $SPARK_HOME\n",
      " ---> Running in d4855b11eb08\n",
      "Removing intermediate container d4855b11eb08\n",
      " ---> 9b04cbcfb117\n",
      "Step 33/37 : RUN pip3 install -q pip --upgrade\n",
      " ---> Running in 0fe8512a4af1\n",
      "Removing intermediate container 0fe8512a4af1\n",
      " ---> 917fc7ef93d3\n",
      "Step 34/37 : RUN pip3 install -q wrapt --upgrade --ignore-installed\n",
      " ---> Running in e455d8846d97\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mRemoving intermediate container e455d8846d97\n",
      " ---> 159785dfa957\n",
      "Step 35/37 : RUN pip3 install -q transformers==2.8.0\n",
      " ---> Running in baa974ec69ef\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mRemoving intermediate container baa974ec69ef\n",
      " ---> 4df6d4f1033a\n",
      "Step 36/37 : RUN pip3 install -q tensorflow==2.1.0 --upgrade --ignore-installed\n",
      " ---> Running in 31e5d6292ecf\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mRemoving intermediate container 31e5d6292ecf\n",
      " ---> e692a0f41cfd\n",
      "Step 37/37 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Running in bb0cfb479784\n",
      "Removing intermediate container bb0cfb479784\n",
      " ---> a1e560b5f735\n",
      "Successfully built a1e560b5f735\n",
      "Successfully tagged amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Spark container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393371431575.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ECR repository and push docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore any `RepositoryNotFoundException` error, we are creating the repo right after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryNotFoundException) when calling the DescribeRepositories operation: The repository with name 'amazon-reviews-spark-processor' does not exist in the registry with id '393371431575'\n",
      "{\n",
      "    \"repository\": {\n",
      "        \"repositoryArn\": \"arn:aws:ecr:us-west-2:393371431575:repository/amazon-reviews-spark-processor\",\n",
      "        \"registryId\": \"393371431575\",\n",
      "        \"repositoryName\": \"amazon-reviews-spark-processor\",\n",
      "        \"repositoryUri\": \"393371431575.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor\",\n",
      "        \"createdAt\": 1595705481.0,\n",
      "        \"imageTagMutability\": \"MUTABLE\",\n",
      "        \"imageScanningConfiguration\": {\n",
      "            \"scanOnPush\": false\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [393371431575.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor]\n",
      "\n",
      "\u001b[1B7ab02bbf: Preparing \n",
      "\u001b[1Bee1fa09f: Preparing \n",
      "\u001b[1B248e9c4d: Preparing \n",
      "\u001b[1B778ed779: Preparing \n",
      "\u001b[1B2d7f1f65: Preparing \n",
      "\u001b[1B4b621a5b: Preparing \n",
      "\u001b[1B803f711d: Preparing \n",
      "\u001b[1Bcb33680a: Preparing \n",
      "\u001b[1B04eaa3b2: Preparing \n",
      "\u001b[1B5d56d659: Preparing \n",
      "\u001b[1B20285432: Preparing \n",
      "\u001b[1B333168eb: Preparing \n",
      "\u001b[1B2c370ca9: Preparing \n",
      "\u001b[1B37da49ee: Preparing \n",
      "\u001b[1Bd076f217: Preparing \n",
      "\u001b[1Bc95dcfbb: Preparing \n",
      "\u001b[9B04eaa3b2: Waiting g \n",
      "\u001b[12B03f711d: Waiting g \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[19Bab02bbf: Pushing  1.833GB/2.011GB\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[16A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[15A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[12A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[18A\u001b[2K\u001b[11A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[11A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[18A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[2A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2KPushing   1.76GB/2.011GB\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[19Bab02bbf: Pushed   2.021GB/2.011GB\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2Klatest: digest: sha256:7d4b6f4465e20d0ba2299481bd029c690be7d972d2bdb84c96a6e6f16b86ae39 size: 4318\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Job using Amazon SageMaker Processing Jobs\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built, and a Spark ML script for processing in the job configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Spark processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pip', '--upgrade'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'wrapt', '--upgrade', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[36mprint\u001b[39;49;00m(tf.__version__)\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlinalg\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DenseVector\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m udf, col\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtypes\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m# We set sequences to be at most 128 tokens long.\u001b[39;49;00m\r\n",
      "MAX_SEQ_LENGTH = \u001b[34m128\u001b[39;49;00m\r\n",
      "DATA_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "\r\n",
      "label_map = {}\r\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id):\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\r\n",
      "    \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label_id = label_id\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, label=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\r\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\r\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "    \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(label, text):\r\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#    tokens = tokenizer.tokenize(text_input.text)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(text,\r\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                               max_length=MAX_SEQ_LENGTH)\r\n",
      "\r\n",
      "    \u001b[37m# Convert the text-based tokens to ids from the pre-trained BERT vocabulary\u001b[39;49;00m\r\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1)\u001b[39;49;00m\r\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\u001b[39;49;00m\r\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * MAX_SEQ_LENGTH\r\n",
      "\r\n",
      "    \u001b[37m# Label for our training data (star_rating 1 through 5)\u001b[39;49;00m\r\n",
      "    label_id = label_map[label]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_ids, \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_mask, \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: segment_ids, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [label_id]}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform\u001b[39;49;00m(spark, s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data): \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mProcessing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m => \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data))\r\n",
      " \r\n",
      "    schema = StructType([\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mcustomer_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_parent\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_title\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_category\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mvine\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mverified_purchase\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_headline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_date\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    ])\r\n",
      "    \r\n",
      "    df_csv = spark.read.csv(path=s3_input_data,\r\n",
      "                            sep=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                            schema=schema,\r\n",
      "                            header=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                            quote=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    df_csv.show()\r\n",
      "\r\n",
      "    \u001b[37m# This dataset should already be clean, but always good to double-check\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing null review_body rows...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv.where(col(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).isNull()).show()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing cleaned csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv_dropped = df_csv.na.drop(subset=[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    df_csv_dropped.show()\r\n",
      "\r\n",
      "    \u001b[37m# TODO:  Balance\u001b[39;49;00m\r\n",
      "    \r\n",
      "    features_df = df_csv_dropped.select([\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    features_df.show()\r\n",
      "\r\n",
      "    tfrecord_schema = StructType([\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m))\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ])\r\n",
      "\r\n",
      "    bert_transformer = udf(\u001b[34mlambda\u001b[39;49;00m text, label: convert_input(text, label), tfrecord_schema)\r\n",
      "\r\n",
      "    spark.udf.register(\u001b[33m'\u001b[39;49;00m\u001b[33mbert_transformer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, bert_transformer)\r\n",
      "\r\n",
      "    transformed_df = features_df.select(bert_transformer(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).alias(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    transformed_df.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "    flattened_df = transformed_df.select(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords.*\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    flattened_df.show()\r\n",
      "\r\n",
      "    \u001b[37m# Split 90-5-5%\u001b[39;49;00m\r\n",
      "    train_df, validation_df, test_df = flattened_df.randomSplit([\u001b[34m0.9\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m])\r\n",
      "\r\n",
      "    train_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_train_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_train_data))\r\n",
      "    \r\n",
      "    validation_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_validation_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_validation_data))\r\n",
      "\r\n",
      "    test_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_test_data)    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_test_data))\r\n",
      "\r\n",
      "    restored_test_df = spark.read.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).load(path=s3_output_test_data)\r\n",
      "    restored_test_df.show()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    spark = SparkSession.builder.appName(\u001b[33m'\u001b[39;49;00m\u001b[33mAmazonReviewsSparkProcessor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).getOrCreate()\r\n",
      "\r\n",
      "    \u001b[37m# Convert command line args into a map of args\u001b[39;49;00m\r\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\r\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\r\n",
      "\r\n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\r\n",
      "    s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\r\n",
      "\r\n",
      "    s3_output_train_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_train_data)\r\n",
      "\r\n",
      "    s3_output_validation_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_validation_data)\r\n",
      "\r\n",
      "    s3_output_test_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_test_data)\r\n",
      "\r\n",
      "    transform(spark, \r\n",
      "              s3_input_data, \r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        \u001b[37m# s3_output_train_data, s3_output_validation_data, s3_output_test_data\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-spark-text-to-bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-processor',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.xlarge',\n",
    "                            env={'mode': 'python'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-processor-{}'.format(timestamp_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/output/bert-train\n",
      "s3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/output/bert-validation\n",
      "s3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "train_data_bert_output = 's3://{}/{}/output/bert-train'.format(bucket, output_prefix)\n",
    "validation_data_bert_output = 's3://{}/{}/output/bert-validation'.format(bucket, output_prefix)\n",
    "test_data_bert_output = 's3://{}/{}/output/bert-test'.format(bucket, output_prefix)\n",
    "\n",
    "print(train_data_bert_output)\n",
    "print(validation_data_bert_output)\n",
    "print(test_data_bert_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-processor-2020-07-25-19-32-49-384\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-processor-2020-07-25-19-32-49-384/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-processor-2020-07-25-19-32-49-384/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-processor-2020-07-25-19-32-49-384/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-processor-2020-07-25-19-32-49-384/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-spark-text-to-bert.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_train_data', train_data_bert_output,\n",
    "                         's3_output_validation_data', validation_data_bert_output,\n",
    "                         's3_output_test_data', test_data_bert_output,                         \n",
    "              ],\n",
    "              # We need this dummy output to allow us to call \n",
    "              #    ProcessingJob.from_processing_name() later \n",
    "              #    to describe the job and poll for Completed status                            \n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-train',\n",
    "                                        source='/opt/ml/processing/output/bert/train'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-validation',\n",
    "                                        source='/opt/ml/processing/output/bert/validation'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-test',\n",
    "                                        source='/opt/ml/processing/output/bert/test'),\n",
    "              ],          \n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-processor-2020-07-25-19-32-49-384;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "spark_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, spark_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# This is different than the job name because we are not using ProcessingOutput's in this Spark ML case.\n",
    "spark_processing_job_s3_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(bucket, spark_processing_job_s3_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Processing Jobs through boto3 Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingJobSummaries': [{'ProcessingJobName': 'spark-amazon-reviews-processor-2020-07-25-19-32-49-384',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:393371431575:processing-job/spark-amazon-reviews-processor-2020-07-25-19-32-49-384',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 19, 32, 49, 851000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 19, 32, 49, 851000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'InProgress'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-07-25-19-21-49-035',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:393371431575:processing-job/sagemaker-scikit-learn-2020-07-25-19-21-49-035',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 19, 21, 49, 485000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 19, 26, 35, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 19, 26, 35, 842000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-07-25-19-18-17-480',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:393371431575:processing-job/sagemaker-scikit-learn-2020-07-25-19-18-17-480',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 19, 18, 17, 950000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 19, 22, 42, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 19, 22, 42, 347000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:393371431575:processing-job/spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 18, 49, 53, 361000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 18, 55, 31, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 18, 55, 31, 728000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'pr-1-8d450126ead6407bace15e074bed40a0dded058a7691487dabb2444b4a',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:393371431575:processing-job/pr-1-8d450126ead6407bace15e074bed40a0dded058a7691487dabb2444b4a',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 16, 15, 56, 750000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 16, 20, 8, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 16, 20, 8, 630000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'db-1-b606debcfcaf4f24b63bca34ee9e8892b5597f8f98db4fd7b3ed854316',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:393371431575:processing-job/db-1-b606debcfcaf4f24b63bca34ee9e8892b5597f8f98db4fd7b3ed854316',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 16, 12, 7, 327000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 16, 15, 53, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 16, 15, 53, 631000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'}],\n",
       " 'ResponseMetadata': {'RequestId': 'c4951db8-34f4-42fc-8df6-b174e478ca45',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c4951db8-34f4-42fc-8df6-b174e478ca45',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2076',\n",
       "   'date': 'Sat, 25 Jul 2020 19:32:49 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "client.list_processing_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes\n",
    "Re-run this next cell until the job status shows `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-processor-2020-07-25-19-32-49-384/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-processor-2020-07-25-19-32-49-384/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-processor-2020-07-25-19-32-49-384/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-processor-2020-07-25-19-32-49-384/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'spark-amazon-reviews-processor-2020-07-25-19-32-49-384', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '393371431575.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-spark-text-to-bert.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-west-2-393371431575/amazon-reviews-pds/tsv/', 's3_output_train_data', 's3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/output/bert-train', 's3_output_validation_data', 's3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/output/bert-validation', 's3_output_test_data', 's3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/output/bert-test']}, 'Environment': {'mode': 'python'}, 'RoleArn': 'arn:aws:iam::393371431575:role/TeamRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:393371431575:processing-job/spark-amazon-reviews-processor-2020-07-25-19-32-49-384', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 7, 25, 19, 32, 49, 851000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 7, 25, 19, 32, 49, 851000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'ef079788-5426-48e9-a03e-63e18bc8a74c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef079788-5426-48e9-a03e-63e18bc8a74c', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2420', 'date': 'Sat, 25 Jul 2020 19:32:49 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=spark_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................\u001b[34mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,115 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.237.72\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.2.1/etc/hadoop:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.375.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-aws-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/had\u001b[0m\n",
      "\u001b[34moop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_262\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,122 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,205 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-28683260-ddd6-4d37-9e22-e28edf8c322a\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,573 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,587 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,588 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,588 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,592 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,592 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,593 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,593 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,634 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,643 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,643 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,648 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,651 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Jul 25 19:36:26\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,652 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,652 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,653 INFO util.GSet: 2.0% max memory 6.7 GB = 136.4 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,654 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,693 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,694 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,700 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,700 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,700 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,700 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,701 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,701 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,701 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,701 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,701 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,701 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,701 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,725 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,725 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,725 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,725 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,737 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,737 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,737 INFO util.GSet: 1.0% max memory 6.7 GB = 68.2 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,737 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,754 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,755 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,755 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,755 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,759 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,760 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,764 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,764 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,764 INFO util.GSet: 0.25% max memory 6.7 GB = 17.0 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,764 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,772 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,772 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,772 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,774 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,775 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,776 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,776 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,776 INFO util.GSet: 0.029999999329447746% max memory 6.7 GB = 2.0 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,776 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,797 INFO namenode.FSImage: Allocated new BlockPoolId: BP-365671189-10.0.237.72-1595705786791\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,809 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,831 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,913 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,925 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,928 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:26,929 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.237.72\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-0-237-72.us-west-2.compute.internal]\u001b[0m\n",
      "\u001b[34mip-10-0-237-72.us-west-2.compute.internal: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:39,337 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:40.256785: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:40.256873: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:40.256882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\u001b[0m\n",
      "\u001b[34m2.1.0\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading:  15%|        | 34.8k/232k [00:00<00:00, 274kB/s]#015Downloading:  90%| | 209k/232k [00:00<00:00, 360kB/s] #015Downloading: 100%|| 232k/232k [00:00<00:00, 902kB/s]\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,203 INFO spark.SparkContext: Running Spark version 2.4.6\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,220 INFO spark.SparkContext: Submitted application: AmazonReviewsSparkProcessor\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,254 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,254 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,254 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,254 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,255 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,478 INFO util.Utils: Successfully started service 'sparkDriver' on port 44281.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,496 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,508 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,510 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,510 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,516 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-103be617-4246-49ad-8853-a5b1b73fe50b\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,527 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,564 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,624 INFO util.log: Logging initialized @4279ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,670 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,680 INFO server.Server: Started @4336ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,692 INFO server.AbstractConnector: Started ServerConnector@50422ff7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,693 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,711 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e895861{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,712 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12c481f9{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,712 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ca4242{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,715 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e24537f{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,715 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b99deab{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,716 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b13e090{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,717 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d0a46d6{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,718 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5013ccc1{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,718 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ff00ce3{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,719 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@549ff2e5{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,720 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@559316ae{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,720 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b28bd19{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,721 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14b03d8b{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,722 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f13ed22{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,722 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7278ba08{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,723 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce6112b{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,724 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@392f3d59{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,724 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1975bda3{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,725 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2eeaec7f{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,726 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3219cfc0{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,731 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@729ce92f{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,731 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5932fc34{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,732 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d62f312{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,733 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ed3f6e9{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,734 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7538969e{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:42,735 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.237.72:4040\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,440 INFO client.RMProxy: Connecting to ResourceManager at /10.0.237.72:8032\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,700 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,761 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,761 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,775 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (31706 MB per container)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,776 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,776 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,779 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,785 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:43,829 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:44,662 INFO yarn.Client: Uploading resource file:/tmp/spark-3f0ea3a3-aa2c-4036-84b6-ce36f1055462/__spark_libs__5077253052054289101.zip -> hdfs://10.0.237.72/user/root/.sparkStaging/application_1595705797766_0001/__spark_libs__5077253052054289101.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:45,033 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:45,769 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:45,973 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/pyspark.zip -> hdfs://10.0.237.72/user/root/.sparkStaging/application_1595705797766_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:45,981 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:46,003 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.237.72/user/root/.sparkStaging/application_1595705797766_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:46,010 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:46,150 INFO yarn.Client: Uploading resource file:/tmp/spark-3f0ea3a3-aa2c-4036-84b6-ce36f1055462/__spark_conf__625008457065150456.zip -> hdfs://10.0.237.72/user/root/.sparkStaging/application_1595705797766_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:46,158 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:46,589 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:46,590 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:46,590 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:46,590 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:46,590 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 19:36:47,343 INFO yarn.Client: Submitting application application_1595705797766_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:47,562 INFO impl.YarnClientImpl: Submitted application application_1595705797766_0001\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:47,565 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1595705797766_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:48,583 INFO yarn.Client: Application report for application_1595705797766_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:48,586 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1595705807449\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1595705797766_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:49,588 INFO yarn.Client: Application report for application_1595705797766_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:50,591 INFO yarn.Client: Application report for application_1595705797766_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:51,593 INFO yarn.Client: Application report for application_1595705797766_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:52,597 INFO yarn.Client: Application report for application_1595705797766_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:52,891 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1595705797766_0001), /proxy/application_1595705797766_0001\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,087 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,599 INFO yarn.Client: Application report for application_1595705797766_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,600 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.251.55\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1595705807449\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1595705797766_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,601 INFO cluster.YarnClientSchedulerBackend: Application application_1595705797766_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,629 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35827.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,630 INFO netty.NettyBlockTransferService: Server created on 10.0.237.72:35827\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,631 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,647 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.237.72, 35827, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,649 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.237.72:35827 with 366.3 MB RAM, BlockManagerId(driver, 10.0.237.72, 35827, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,651 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.237.72, 35827, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,652 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.237.72, 35827, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,748 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:53,754 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@175e798a{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:55,933 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.251.55:41854) with ID 1\u001b[0m\n",
      "\u001b[34m2020-07-25 19:36:56,029 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:33601 with 11.9 GB RAM, BlockManagerId(1, algo-2, 33601, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:12,801 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,005 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.4.6/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,005 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.4.6/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,010 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,011 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3eaa2219{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,011 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,012 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@240ab0c5{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,012 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,013 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a27deda{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,013 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,013 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44f1fae3{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,014 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,014 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51d46812{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,303 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-393371431575/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/output/bert-train\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/output/bert-validation\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-processor-2020-07-25-19-32-49/output/bert-test\u001b[0m\n",
      "\u001b[34mProcessing s3a://sagemaker-us-west-2-393371431575/amazon-reviews-pds/tsv/ => /opt/ml/processing/output/bert/train\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,481 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,530 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:13,530 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:14,775 INFO datasources.InMemoryFileIndex: It took 103 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,194 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,199 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,201 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,206 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,560 INFO codegen.CodeGenerator: Code generated in 207.076075 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,745 INFO codegen.CodeGenerator: Code generated in 36.168051 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,790 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,831 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,833 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.237.72:35827 (size: 42.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,835 INFO spark.SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,852 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,924 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,937 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,937 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,937 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,938 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:15,941 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:16,001 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:16,003 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:16,004 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.237.72:35827 (size: 7.3 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:16,005 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:16,014 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:16,014 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:16,035 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:16,215 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:33601 (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:16,772 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:33601 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 19:37:18,565 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2536 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,568 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,572 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.620 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,575 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.651232 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   21269168| RSH1OZ87OYK92|B013PURRZW|     603406193|Madden NFL 16 - X...|Digital_Video_Games|          2|            2|          3|   N|                N|A slight improvem...|I keep buying mad...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     133437|R1WFOQ3N9BO65I|B00F4CEHNK|     341969535| Xbox Live Gift Card|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Awesome| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R3YOOS71KM5M9|B00DNHLFQA|     951665344|Command & Conquer...|Digital_Video_Games|          5|            0|          0|   N|                Y|Hail to the great...|If you are preppi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     113118|R3R14UATT3OUFU|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Perfect| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364| RV2W9SGDNQA2C|B00G9BNLQE|     640460561|Saints Row IV - E...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R3CFKLIZ0I2KOB|B00IMIL498|     621922192|Double Dragon: Ne...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   38426028|R1LRYU1V0T3O38|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          4|            0|          0|   N|                Y|i like the new sk...|i like the new sk...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6057518| R44QKV6FE5CJ2|B004RMK4BC|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Super| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   20715661|R2TX1KLPXXXNYS|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|         Easy & Fast|Excellent, fast a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   26540306|R1JEEW4C6R89BA|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|                  Ok| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    8926809|R3B3UHK1FO0ERS|B004774IPU|     151985175|Sid Meier's Civil...|Digital_Video_Games|          1|            0|          0|   N|                N|I am still playin...|As has been writt...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   31525534|R2GVSDHW513SS1|B002LIT9EC|     695277014|Build-a-lot 4: Po...|Digital_Video_Games|          5|            0|          0|   N|                Y|Probably the best...|Probably the best...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R1R1NT516PYT73|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22977584|R3K624QDQKENN9|B010KYDNDG|     835376637|Minecraft for PC/...|Digital_Video_Games|          4|            0|          0|   N|                Y|                 FUN|COOL BUT IT LAGES...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R1FOXH7PCJX3V|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          1|            0|          2|   N|                Y|            One Star|Lames purchase I ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    2239522| RA1246M1OMDWC|B004RMK4P8|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Great| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   48805811|R2I9SXWB0PAEKQ|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|          Awesome!!!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   18646481|R3UGL544NA0G9C|B00BI16Z22|     552981447|Brink of Consciou...|Digital_Video_Games|          4|            0|          0|   N|                Y|       worth playing|pretty good but n...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10310935|R1CBA4Y92GVAVM|B004VSTQ2A|     232803743|Xbox Live Subscri...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|What can I say......| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5587610|R24NEKNR01VEHU|B00GAC1D2G|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|        Just amazing|Very fast to rece...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34mShowing null review_body rows...\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,655 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,657 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnull(review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,658 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,660 INFO execution.FileSourceScanExec: Pushed Filters: IsNull(review_body)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,725 INFO codegen.CodeGenerator: Code generated in 29.207143 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,733 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 401.9 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,751 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,752 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.237.72:35827 (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,753 INFO spark.SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,753 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,763 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,765 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,765 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,765 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,765 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,765 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,770 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.6 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,772 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,772 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.237.72:35827 (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,775 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,776 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,777 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,778 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,826 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:33601 (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:18,941 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:33601 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,246 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2468 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,246 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,247 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 2.480 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,248 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 2.484419 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,251 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,252 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,252 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,252 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,252 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,252 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,255 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 16.6 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,257 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,257 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.237.72:35827 (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,258 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,259 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,259 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,260 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:21,272 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:33601 (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,646 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1387 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,646 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,647 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 1.394 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,647 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 1.396279 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   44947401| RKHI9WDB7S5WR|B008D7F47Q|     886655228|      FIFA Soccer 13|Digital_Video_Games|          5|            0|          0|   N|                N|          Five Stars|       null| 2015-05-08|\u001b[0m\n",
      "\u001b[34m|         US|    4190665|R2UJ6J6TUON90A|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          2|            0|          2|   N|                N|           Two Stars|       null| 2015-04-14|\u001b[0m\n",
      "\u001b[34m|         US|   51305484| RJVXCTIK9SIC0|B0040V4T8O|      97283480|Medieval II: Tota...|Digital_Video_Games|          5|            0|          0|   N|                Y|It's an amazing g...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   48052209| R8PISK7HHT6BH|B004ZUFKEC|     321025745|The Sims 3 [Mac D...|Digital_Video_Games|          1|            0|          0|   N|                N| What a good wast...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   17134921|R2H22164810P1S|B007Z3RN2I|     137803476|Call of Duty: Bla...|Digital_Video_Games|          3|            0|          1|   N|                Y|I won't be wronge...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   47668954|R1N1YQ8RZOGAMP|B00506X3Y4|     692311664|  Duke Nukem Forever|Digital_Video_Games|          5|            1|          2|   N|                Y|What you waiting ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   48104724|R2WUQWZTYWZYWY|B00PG8FFFQ|     245597084|Block Financial H...|   Digital_Software|          1|            0|          0|   N|                Y| I am very disapp...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   49035882|R3A8HHF16GNB9M|B005WX2X1Y|      35439242|        Rostta Stone|   Digital_Software|          1|            1|          4|   N|                Y| Very disappointe...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   46107169|R1G1KQDXWWAZGU|B00NG7JVSQ|     811978073|TurboTax Deluxe F...|   Digital_Software|          5|            0|          2|   N|                N| just as I would ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   18699318| RVYUT2W57G45K|B00NG7K2RA|     349370473|TurboTax Premier ...|   Digital_Software|          1|           24|         39|   N|                Y| Huge waste of ti...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   14757352|R1EAMCG69CDL91|B00I9FX20S|     676852534|Dupe Eliminator 1...|   Digital_Software|          5|            0|          0|   N|                N|The best tool for...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   52995039|R1VR3Q470S6Y5Q|B008S0IMCC|     534964191| Quicken Deluxe 2013|   Digital_Software|          1|            0|          0|   N|                Y|Tis Pity#011I have b...|       null|       null|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\n",
      "\u001b[0m\n",
      "\u001b[34mShowing cleaned csv\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,696 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,696 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,697 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,697 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,757 INFO codegen.CodeGenerator: Code generated in 26.223774 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,762 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 401.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,775 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,775 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.237.72:35827 (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,776 INFO spark.SparkContext: Created broadcast 5 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,776 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,784 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,785 INFO scheduler.DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,786 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,786 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,786 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,787 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,790 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 16.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,792 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,793 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.237.72:35827 (size: 7.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,793 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,794 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,794 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,795 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,806 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:33601 (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,852 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:33601 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,911 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 116 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,912 INFO scheduler.DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.124 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,912 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,912 INFO scheduler.DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.128048 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   21269168| RSH1OZ87OYK92|B013PURRZW|     603406193|Madden NFL 16 - X...|Digital_Video_Games|          2|            2|          3|   N|                N|A slight improvem...|I keep buying mad...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     133437|R1WFOQ3N9BO65I|B00F4CEHNK|     341969535| Xbox Live Gift Card|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Awesome| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R3YOOS71KM5M9|B00DNHLFQA|     951665344|Command & Conquer...|Digital_Video_Games|          5|            0|          0|   N|                Y|Hail to the great...|If you are preppi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     113118|R3R14UATT3OUFU|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Perfect| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364| RV2W9SGDNQA2C|B00G9BNLQE|     640460561|Saints Row IV - E...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R3CFKLIZ0I2KOB|B00IMIL498|     621922192|Double Dragon: Ne...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   38426028|R1LRYU1V0T3O38|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          4|            0|          0|   N|                Y|i like the new sk...|i like the new sk...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6057518| R44QKV6FE5CJ2|B004RMK4BC|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Super| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   20715661|R2TX1KLPXXXNYS|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|         Easy & Fast|Excellent, fast a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   26540306|R1JEEW4C6R89BA|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|                  Ok| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    8926809|R3B3UHK1FO0ERS|B004774IPU|     151985175|Sid Meier's Civil...|Digital_Video_Games|          1|            0|          0|   N|                N|I am still playin...|As has been writt...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   31525534|R2GVSDHW513SS1|B002LIT9EC|     695277014|Build-a-lot 4: Po...|Digital_Video_Games|          5|            0|          0|   N|                Y|Probably the best...|Probably the best...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R1R1NT516PYT73|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22977584|R3K624QDQKENN9|B010KYDNDG|     835376637|Minecraft for PC/...|Digital_Video_Games|          4|            0|          0|   N|                Y|                 FUN|COOL BUT IT LAGES...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R1FOXH7PCJX3V|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          1|            0|          2|   N|                Y|            One Star|Lames purchase I ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    2239522| RA1246M1OMDWC|B004RMK4P8|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Great| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   48805811|R2I9SXWB0PAEKQ|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|          Awesome!!!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   18646481|R3UGL544NA0G9C|B00BI16Z22|     552981447|Brink of Consciou...|Digital_Video_Games|          4|            0|          0|   N|                Y|       worth playing|pretty good but n...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10310935|R1CBA4Y92GVAVM|B004VSTQ2A|     232803743|Xbox Live Subscri...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|What can I say......| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5587610|R24NEKNR01VEHU|B00GAC1D2G|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|        Just amazing|Very fast to rece...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,954 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,954 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,955 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,955 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,974 INFO codegen.CodeGenerator: Code generated in 9.87436 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,988 INFO codegen.CodeGenerator: Code generated in 9.369138 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:22,993 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 401.9 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,010 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,011 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.237.72:35827 (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,011 INFO spark.SparkContext: Created broadcast 7 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,012 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,019 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,020 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,020 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,020 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,020 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,021 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,024 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.2 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,025 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.4 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,026 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.237.72:35827 (size: 6.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,026 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,027 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,027 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,028 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,037 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:33601 (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,071 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:33601 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,169 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 142 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,170 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,171 INFO scheduler.DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.148 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,171 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.152038 s\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|star_rating|         review_body|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|          2|I keep buying mad...|\u001b[0m\n",
      "\u001b[34m|          5|             Awesome|\u001b[0m\n",
      "\u001b[34m|          5|If you are preppi...|\u001b[0m\n",
      "\u001b[34m|          5|             Perfect|\u001b[0m\n",
      "\u001b[34m|          5|            Awesome!|\u001b[0m\n",
      "\u001b[34m|          5|            Awesome!|\u001b[0m\n",
      "\u001b[34m|          4|i like the new sk...|\u001b[0m\n",
      "\u001b[34m|          5|               Super|\u001b[0m\n",
      "\u001b[34m|          5|Excellent, fast a...|\u001b[0m\n",
      "\u001b[34m|          5|                  Ok|\u001b[0m\n",
      "\u001b[34m|          1|As has been writt...|\u001b[0m\n",
      "\u001b[34m|          5|Probably the best...|\u001b[0m\n",
      "\u001b[34m|          5|            Awesome!|\u001b[0m\n",
      "\u001b[34m|          4|COOL BUT IT LAGES...|\u001b[0m\n",
      "\u001b[34m|          1|Lames purchase I ...|\u001b[0m\n",
      "\u001b[34m|          5|               Great|\u001b[0m\n",
      "\u001b[34m|          5|          Awesome!!!|\u001b[0m\n",
      "\u001b[34m|          4|pretty good but n...|\u001b[0m\n",
      "\u001b[34m|          5|What can I say......|\u001b[0m\n",
      "\u001b[34m|          5|Very fast to rece...|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,468 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,468 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,468 INFO spark.ContextCleaner: Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,468 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,468 INFO spark.ContextCleaner: Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,468 INFO spark.ContextCleaner: Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,479 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.237.72:35827 in memory (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,486 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:33601 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 15\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 16\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 17\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,492 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,493 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,498 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.237.72:35827 in memory (size: 6.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,499 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:33601 in memory (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,509 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,509 INFO spark.ContextCleaner: Cleaned accumulator 8\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,509 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,509 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,509 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,509 INFO spark.ContextCleaner: Cleaned accumulator 22\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,513 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:33601 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,514 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.237.72:35827 in memory (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 10\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 9\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 12\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 21\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,522 INFO spark.ContextCleaner: Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,528 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:33601 in memory (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,529 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.237.72:35827 in memory (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,542 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.237.72:35827 in memory (size: 7.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,543 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:33601 in memory (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,549 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.237.72:35827 in memory (size: 7.3 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,549 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:33601 in memory (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,553 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,553 INFO spark.ContextCleaner: Cleaned accumulator 14\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,553 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,553 INFO spark.ContextCleaner: Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,553 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,553 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,553 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,553 INFO spark.ContextCleaner: Cleaned accumulator 18\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,554 INFO spark.ContextCleaner: Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,554 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,559 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:33601 in memory (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,560 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.237.72:35827 in memory (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 13\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 23\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 20\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 19\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,573 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,576 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:33601 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,581 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.237.72:35827 in memory (size: 42.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 11\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,585 INFO spark.ContextCleaner: Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,591 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:33601 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,592 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.237.72:35827 in memory (size: 42.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,838 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,839 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,839 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,839 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,858 INFO codegen.CodeGenerator: Code generated in 7.489242 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,886 INFO codegen.CodeGenerator: Code generated in 21.076052 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,899 INFO codegen.CodeGenerator: Code generated in 9.142754 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,905 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,922 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,922 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.237.72:35827 (size: 42.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,923 INFO spark.SparkContext: Created broadcast 9 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,925 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,979 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,980 INFO scheduler.DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,980 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,980 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,981 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,981 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,988 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 849.5 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,994 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 644.8 KB, free 364.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,995 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.237.72:35827 (size: 644.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,995 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,996 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,996 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:23,997 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:24,015 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:33601 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:24,519 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:33601 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,623 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2626 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,623 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,624 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37223\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,625 INFO scheduler.DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 2.643 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,625 INFO scheduler.DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 2.645629 s\u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|tfrecords                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2562, 9343, 24890, 2296, 2095, 5327, 2027, 2131, 2067, 2000, 2374, 1012, 2023, 2086, 2544, 2003, 1037, 2210, 2488, 2084, 2197, 2086, 1011, 1011, 2021, 2008, 1005, 1055, 2025, 3038, 2172, 1012, 1996, 2208, 3504, 2307, 1012, 1996, 2069, 2518, 3308, 2007, 1996, 7284, 1010, 2003, 1996, 2126, 1996, 2867, 2024, 2467, 4440, 4691, 2006, 2169, 2060, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 11247, 2003, 2145, 9784, 2091, 2011, 1996, 1038, 4135, 4383, 3653, 1011, 2377, 7711, 1012, 2054, 2109, 2000, 2202, 2048, 11287, 2003, 2085, 1037, 5016, 6770, 2050, 2000, 2131, 2589, 2077, 2019, 7116, 20057, 1996, 3608, 2030, 1996, 2377, 5119, 3216, 2041, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 15386, 6462, 2003, 2067, 1010, 2021, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1]]   |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                              |\u001b[0m\n",
      "\u001b[34m|[[101, 2065, 2017, 2024, 17463, 4691, 2005, 1996, 2203, 1997, 1996, 2088, 2023, 2003, 2028, 1997, 2216, 2477, 2008, 2017, 2323, 2031, 5361, 2006, 2115, 1011, 2203, 1011, 1997, 1011, 1996, 1011, 2088, 1011, 6947, 7473, 1012, 16889, 2000, 1996, 2307, 14331, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                  |\u001b[0m\n",
      "\u001b[34m|[[101, 3819, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                            |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                            |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2066, 1996, 2047, 4813, 2066, 27849, 2964, 1999, 2023, 1010, 1998, 13215, 2003, 4569, 1012, 1045, 2036, 2066, 2035, 1996, 2047, 3857, 5549, 5167, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                                                                                                                                                                                                                     |\u001b[0m\n",
      "\u001b[34m|[[101, 3565, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 6581, 1010, 3435, 1998, 5851, 999, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                             |\u001b[0m\n",
      "\u001b[34m|[[101, 7929, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 2004, 2038, 2042, 2517, 2011, 2061, 2116, 2500, 1010, 1045, 2855, 2439, 3037, 1999, 2023, 2208, 1012, 1045, 2572, 2145, 2652, 25022, 2615, 1018, 1998, 2293, 2009, 1012, 2009, 1005, 1055, 1037, 9467, 2138, 1045, 1005, 1049, 3201, 2005, 2019, 4423, 2544, 1997, 25022, 2615, 1018, 1998, 2031, 4741, 2005, 2055, 1037, 5476, 2005, 1037, 2488, 2544, 1997, 2009, 1012, 25022, 2615, 1019, 2001, 2025, 2019, 6622, 2021, 1037, 2561, 2128, 26373, 1998, 2009, 2439, 2035, 2008, 2001, 2204, 1999, 25022, 2615, 1018, 1012, 1045, 2428, 3246, 2008, 2043, 25022, 2615, 1020, 3310, 2041, 2027, 2224, 25022, 2615, 1018, 2004, 1996, 3225, 2391, 1998, 5293, 25022, 2615, 1019, 2412, 3047, 1012, 7989, 2008, 2045, 2003, 1037, 2173, 1999, 1996, 3006, 2005, 1037, 5656, 2208, 2008, 7336, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]]|\u001b[0m\n",
      "\u001b[34m|[[101, 2763, 1996, 2190, 2208, 2005, 4083, 5919, 1997, 2613, 3776, 2800, 1012, 6700, 15794, 2428, 2718, 1996, 3608, 2041, 1997, 1996, 2380, 2007, 2023, 2028, 1010, 2007, 2152, 4547, 3643, 1006, 2004, 2092, 2004, 2019, 14036, 2208, 1007, 1999, 3408, 1997, 2877, 2017, 2083, 1996, 24078, 1997, 2613, 3776, 2458, 1012, 2130, 2295, 2023, 2003, 2195, 2086, 2214, 2113, 1996, 11343, 1997, 22956, 1998, 18726, 2005, 2023, 2208, 2965, 2023, 2003, 2145, 1037, 2442, 2031, 2005, 13007, 4667, 2613, 3776, 9587, 24848, 2015, 1997, 4826, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                        |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                            |\u001b[0m\n",
      "\u001b[34m|[[101, 4658, 2021, 2009, 2474, 8449, 2632, 4140, 1997, 1996, 2051, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m|[[101, 20342, 2015, 5309, 1045, 2471, 2196, 2081, 1012, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]]                                                                                                                                                                                                                                                                                                                                                                   |\u001b[0m\n",
      "\u001b[34m|[[101, 2307, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                        |\u001b[0m\n",
      "\u001b[34m|[[101, 3492, 2204, 2021, 2025, 2004, 2204, 2004, 1996, 2034, 20911, 1997, 8298, 2208, 1011, 1011, 16092, 1026, 7987, 1013, 1028, 3897, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                                                                                                                                                                                                                                 |\u001b[0m\n",
      "\u001b[34m|[[101, 2054, 2064, 1045, 2360, 1012, 1012, 1012, 12202, 2444, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m|[[101, 2200, 3435, 2000, 4374, 1010, 1998, 1997, 2278, 1037, 3404, 13966, 1998, 3647, 2173, 2000, 4965, 2478, 2115, 4923, 4003, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                   |\u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,712 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,712 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,713 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,713 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,739 INFO codegen.CodeGenerator: Code generated in 10.664464 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,764 INFO codegen.CodeGenerator: Code generated in 16.785991 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,772 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 401.9 KB, free 364.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,787 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,787 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.237.72:35827 (size: 42.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,788 INFO spark.SparkContext: Created broadcast 11 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,788 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,812 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,814 INFO scheduler.DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,814 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,814 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,814 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,814 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,820 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 849.1 KB, free 363.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,824 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 644.8 KB, free 362.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,825 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.237.72:35827 (size: 644.8 KB, free: 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,825 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,825 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,826 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,827 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,838 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:33601 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:26,878 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:33601 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 19:37:28,768 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1942 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,768 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,769 INFO scheduler.DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 1.954 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,769 INFO scheduler.DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 1.957557 s\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|           input_ids|          input_mask|         segment_ids|label_ids|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2562,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [1]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 102,...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2065, 2017,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 3819, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2066,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 3565, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 6581, 1010,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 7929, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2004, 2038,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [0]|\u001b[0m\n",
      "\u001b[34m|[101, 2763, 1996,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 4658, 2021,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 20342, 2015...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [0]|\u001b[0m\n",
      "\u001b[34m|[101, 2307, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 3492, 2204,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 2054, 2064,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2200, 3435,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,890 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,893 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.237.72:35827 in memory (size: 42.8 KB, free: 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,895 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:33601 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,902 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,902 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,902 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,904 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.237.72:35827 in memory (size: 644.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,907 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:33601 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,912 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,912 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,912 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,916 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:33601 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,917 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.237.72:35827 in memory (size: 42.8 KB, free: 365.7 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,922 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,922 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,924 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:33601 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,925 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.237.72:35827 in memory (size: 644.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,929 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,929 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,929 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,929 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,930 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,970 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,970 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,971 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:28,971 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,013 INFO codegen.CodeGenerator: Code generated in 29.046508 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,021 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,036 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,036 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.237.72:35827 (size: 42.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,037 INFO spark.SparkContext: Created broadcast 13 from rdd at DefaultSource.scala:58\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,037 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,116 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,116 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,133 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,134 INFO scheduler.DAGScheduler: Got job 7 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,134 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,134 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,135 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,135 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[38] at map at DefaultSource.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,164 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 979.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,169 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 692.2 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,169 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.237.72:35827 (size: 692.2 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,170 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,171 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[38] at map at DefaultSource.scala:58) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,171 INFO cluster.YarnScheduler: Adding task set 7.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,172 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,172 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 8, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,187 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:33601 (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:37:29,285 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:33601 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 19:46:44,233 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 8) in 555061 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,538 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 766367 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,539 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,539 INFO scheduler.DAGScheduler: ResultStage 7 (runJob at SparkHadoopWriter.scala:78) finished in 766.402 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,540 INFO scheduler.DAGScheduler: Job 7 finished: runJob at SparkHadoopWriter.scala:78, took 766.406970 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,560 INFO io.SparkHadoopWriter: Job job_20200725193729_0038 committed.\u001b[0m\n",
      "\u001b[34mWrote to output file:  /opt/ml/processing/output/bert/train\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,603 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,604 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,604 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,604 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,635 INFO codegen.CodeGenerator: Code generated in 23.468594 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,640 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 401.9 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,652 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 42.8 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,652 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.237.72:35827 (size: 42.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,653 INFO spark.SparkContext: Created broadcast 15 from rdd at DefaultSource.scala:58\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,653 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,678 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,678 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,692 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,693 INFO scheduler.DAGScheduler: Got job 8 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,693 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,693 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,693 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,693 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[49] at map at DefaultSource.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,711 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 979.8 KB, free 362.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,715 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 692.2 KB, free 362.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,716 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.237.72:35827 (size: 692.2 KB, free: 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,716 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,717 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[49] at map at DefaultSource.scala:58) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,717 INFO cluster.YarnScheduler: Adding task set 8.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,718 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 9, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,718 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 10, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,728 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:33601 (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:50:15,770 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:33601 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:59:24,467 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 10) in 548749 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,877 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 9) in 758160 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,877 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,877 INFO scheduler.DAGScheduler: ResultStage 8 (runJob at SparkHadoopWriter.scala:78) finished in 758.183 s\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,878 INFO scheduler.DAGScheduler: Job 8 finished: runJob at SparkHadoopWriter.scala:78, took 758.185443 s\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,887 INFO io.SparkHadoopWriter: Job job_20200725195015_0049 committed.\u001b[0m\n",
      "\u001b[34mWrote to output file:  /opt/ml/processing/output/bert/validation\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,921 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,921 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,921 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,921 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,952 INFO codegen.CodeGenerator: Code generated in 23.411844 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,958 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 401.9 KB, free 361.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,969 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 42.8 KB, free 361.7 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,970 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.237.72:35827 (size: 42.8 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,970 INFO spark.SparkContext: Created broadcast 17 from rdd at DefaultSource.scala:58\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,970 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,998 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:53,998 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,011 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,012 INFO scheduler.DAGScheduler: Got job 9 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,012 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,012 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,012 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,012 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[60] at map at DefaultSource.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,029 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 979.8 KB, free 360.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,033 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 692.2 KB, free 360.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,033 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.237.72:35827 (size: 692.2 KB, free: 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,033 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,034 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[60] at map at DefaultSource.scala:58) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,034 INFO cluster.YarnScheduler: Adding task set 9.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,034 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,035 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 12, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,044 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:33601 (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:02:54,088 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:33601 (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 20:06:53,941 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,941 INFO spark.ContextCleaner: Cleaned accumulator 284\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,941 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,941 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,941 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,941 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,941 INFO spark.ContextCleaner: Cleaned accumulator 269\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,941 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,943 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.237.72:35827 in memory (size: 692.2 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,946 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:33601 in memory (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,950 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.237.72:35827 in memory (size: 692.2 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,951 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-2:33601 in memory (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 280\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 270\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 281\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 271\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,954 INFO spark.ContextCleaner: Cleaned accumulator 276\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,955 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.237.72:35827 in memory (size: 42.8 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,957 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:33601 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 268\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 277\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 278\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,960 INFO spark.ContextCleaner: Cleaned accumulator 282\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 274\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 283\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,961 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,962 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.237.72:35827 in memory (size: 42.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,965 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:33601 in memory (size: 42.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 279\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 273\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 272\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 275\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m2020-07-25 20:06:53,967 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m2020-07-25 20:12:02,838 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 12) in 548803 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,350 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 758316 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,350 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,351 INFO scheduler.DAGScheduler: ResultStage 9 (runJob at SparkHadoopWriter.scala:78) finished in 758.338 s\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,353 INFO scheduler.DAGScheduler: Job 9 finished: runJob at SparkHadoopWriter.scala:78, took 758.341944 s\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,366 INFO io.SparkHadoopWriter: Job job_20200725200253_0060 committed.\u001b[0m\n",
      "\u001b[34mWrote to output file:  /opt/ml/processing/output/bert/test\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,382 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 389.7 KB, free 363.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,397 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 42.3 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,398 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.237.72:35827 (size: 42.3 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,398 INFO spark.SparkContext: Created broadcast 19 from newAPIHadoopFile at TensorflowRelation.scala:33\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,455 INFO input.FileInputFormat: Total input files to process : 2\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,476 INFO spark.SparkContext: Starting job: aggregate at TensorFlowInferSchema.scala:39\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,477 INFO scheduler.DAGScheduler: Got job 10 (aggregate at TensorFlowInferSchema.scala:39) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,477 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (aggregate at TensorFlowInferSchema.scala:39)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,477 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,477 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,477 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[64] at map at TensorflowRelation.scala:39), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,482 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 3.7 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,483 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.2 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,483 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.237.72:35827 (size: 2.2 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,484 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,486 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[64] at map at TensorflowRelation.scala:39) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,486 INFO cluster.YarnScheduler: Adding task set 10.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,497 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 13, algo-2, executor 1, partition 0, NODE_LOCAL, 7979 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,497 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 14, algo-2, executor 1, partition 1, NODE_LOCAL, 7979 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,516 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:33601 (size: 2.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,535 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:33601 (size: 42.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:32,998 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 14) in 501 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,159 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 13) in 670 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,159 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,159 INFO scheduler.DAGScheduler: ResultStage 10 (aggregate at TensorFlowInferSchema.scala:39) finished in 0.678 s\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,160 INFO scheduler.DAGScheduler: Job 10 finished: aggregate at TensorFlowInferSchema.scala:39, took 0.683135 s\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,211 INFO codegen.CodeGenerator: Code generated in 16.023682 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,217 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,217 INFO scheduler.DAGScheduler: Got job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,218 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,218 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,218 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,218 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[69] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,223 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 12.7 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,224 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.4 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,225 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.237.72:35827 (size: 5.4 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,225 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,226 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[69] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,226 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,227 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 15, algo-2, executor 1, partition 0, NODE_LOCAL, 7979 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,242 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-2:33601 (size: 5.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,326 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 15) in 100 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,326 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,327 INFO scheduler.DAGScheduler: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.108 s\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,328 INFO scheduler.DAGScheduler: Job 11 finished: showString at NativeMethodAccessorImpl.java:0, took 0.111261 s\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|label_ids|           input_ids|         segment_ids|          input_mask|\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        2|[101, 1002, 1016,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 1019,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2184,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2184,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2184,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2322,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2322,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2322,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        0|[101, 1002, 2753,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        0|[101, 1002, 2871,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 4464,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 4749,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        0|[101, 1003, 3178,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,575 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,579 INFO server.AbstractConnector: Stopped Spark@50422ff7{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,584 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.237.72:4040\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,588 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,612 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,613 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,615 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,616 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,620 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,628 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,628 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,629 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,630 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,637 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,637 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,638 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9e43f8d4-cd5c-4b90-b42f-92f7703ce842\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,640 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3f0ea3a3-aa2c-4036-84b6-ce36f1055462/pyspark-7c5292e5-2ec4-4f51-978a-37793d3b124c\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,642 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3f0ea3a3-aa2c-4036-84b6-ce36f1055462\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,645 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,645 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2020-07-25 20:15:33,645 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2020-07-25 20:15:34\u001b[0m\n",
      "\u001b[35mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mReceived end of job signal, exiting...\u001b[0m\n",
      "\u001b[34mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output Dataset\n",
    "\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._\n",
    "\n",
    "\n",
    "Take a look at a few rows of the transformed dataset to make sure the processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $train_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $validation_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $test_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
