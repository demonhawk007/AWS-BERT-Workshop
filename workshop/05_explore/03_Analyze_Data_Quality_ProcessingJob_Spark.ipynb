{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analyze Data Quality with SageMaker Processing Jobs and Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to process and analyze data sets in order to detect data quality issues and prepare them for model training.  \n",
    "\n",
    "In this notebook we'll use Amazon SageMaker Processing with a library called [Deequ](https://github.com/awslabs/deequ), and leverage the power of Spark with a managed SageMaker Processing Job to run our data processing workloads.\n",
    "\n",
    "Here is a great blog post on Deequ for more information:  https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/\n",
    "\n",
    "![Deequ](img/deequ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/processing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Dataset\n",
    "\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/readme.html\n",
    "\n",
    "### Dataset Columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Docker Container with Spark and our Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example Spark container is included in the `./container` directory of this example. The container handles the bootstrapping of all Spark configuration, and serves as a wrapper around the `spark-submit` CLI. At a high level the container provides:\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed Spark application that performs our dataset preprocessing.\n",
    "\n",
    "Build the example Spark container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM openjdk:8-jre-slim\r\n",
      "\r\n",
      "RUN apt-get update\r\n",
      "RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\r\n",
      "RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\r\n",
      "RUN apt-get clean\r\n",
      "RUN rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "# http://blog.stuart.axelbrooke.com/python-3-on-spark-return-of-the-pythonhashseed\r\n",
      "ENV PYTHONHASHSEED 0\r\n",
      "ENV PYTHONIOENCODING UTF-8\r\n",
      "ENV PIP_DISABLE_PIP_VERSION_CHECK 1\r\n",
      "\r\n",
      "# Install Hadoop\r\n",
      "ENV HADOOP_VERSION 3.2.1\r\n",
      "ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\r\n",
      "ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\n",
      "ENV PATH $PATH:$HADOOP_HOME/bin\r\n",
      "RUN curl -sL --retry 3 \\\r\n",
      "  \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\" \\\r\n",
      "  | gunzip \\\r\n",
      "  | tar -x -C /usr/ \\\r\n",
      " && rm -rf $HADOOP_HOME/share/doc \\\r\n",
      " && chown -R root:root $HADOOP_HOME\r\n",
      "\r\n",
      "# Install Spark\r\n",
      "ENV SPARK_VERSION 2.4.6\r\n",
      "ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\r\n",
      "ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\r\n",
      "ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\r\n",
      "ENV PATH $PATH:${SPARK_HOME}/bin\r\n",
      "RUN curl -sL --retry 3 \\\r\n",
      "  \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\" \\\r\n",
      "  | gunzip \\\r\n",
      "  | tar x -C /usr/ \\\r\n",
      " && mv /usr/$SPARK_PACKAGE $SPARK_HOME \\\r\n",
      " && chown -R root:root $SPARK_HOME\r\n",
      " \r\n",
      "# Point Spark at proper python binary\r\n",
      "ENV PYSPARK_PYTHON=/usr/bin/python3\r\n",
      "\r\n",
      "# Setup Spark/Yarn/HDFS user as root\r\n",
      "ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\r\n",
      "ENV YARN_RESOURCEMANAGER_USER=\"root\"\r\n",
      "ENV YARN_NODEMANAGER_USER=\"root\"\r\n",
      "ENV HDFS_NAMENODE_USER=\"root\"\r\n",
      "ENV HDFS_DATANODE_USER=\"root\"\r\n",
      "ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\r\n",
      "\r\n",
      "# Set up bootstrapping program and Spark configuration\r\n",
      "COPY program /opt/program\r\n",
      "RUN chmod +x /opt/program/submit\r\n",
      "COPY hadoop-config /opt/hadoop-config\r\n",
      "\r\n",
      "COPY jars /usr/jars\r\n",
      "\r\n",
      "WORKDIR $SPARK_HOME\r\n",
      "\r\n",
      "# Install Transformers and TensorFlow\r\n",
      "#RUN pip3 install -q pip --upgrade\r\n",
      "#RUN pip3 install -q wrapt --upgrade --ignore-installed\r\n",
      "#RUN pip3 install -q transformers==2.8.0\r\n",
      "#RUN pip3 install -q tensorflow==2.1.0 --upgrade --ignore-installed\r\n",
      "\r\n",
      "ENTRYPOINT [\"/opt/program/submit\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-analyzer'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.441MB\n",
      "Step 1/33 : FROM openjdk:8-jre-slim\n",
      "8-jre-slim: Pulling from library/openjdk\n",
      "\n",
      "\u001b[1Bc9369e08: Pulling fs layer \n",
      "\u001b[1Be9b77806: Pulling fs layer \n",
      "\u001b[1B6b4d80d1: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:bcbdeafc77c4ed16ae625f7b02f233ad5d21d070f72a0ee44710923f0bd7d13c\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for openjdk:8-jre-slim\n",
      " ---> f2e91f81bf2c\n",
      "Step 2/33 : RUN apt-get update\n",
      " ---> Running in 2dee308e47ea\n",
      "Get:1 http://deb.debian.org/debian buster InRelease [121 kB]\n",
      "Get:2 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]\n",
      "Get:3 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]\n",
      "Get:4 http://deb.debian.org/debian buster/main amd64 Packages [7905 kB]\n",
      "Get:5 http://security.debian.org/debian-security buster/updates/main amd64 Packages [213 kB]\n",
      "Get:6 http://deb.debian.org/debian buster-updates/main amd64 Packages [7868 B]\n",
      "Fetched 8365 kB in 2s (5259 kB/s)\n",
      "Reading package lists...\n",
      "Removing intermediate container 2dee308e47ea\n",
      " ---> a2e6bbe0e5e2\n",
      "Step 3/33 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Running in d5bb702f8a9c\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu build-essential bzip2 cpp\n",
      "  cpp-8 dbus dh-python dirmngr dpkg-dev fakeroot file g++ g++-8 gcc gcc-8\n",
      "  gir1.2-glib-2.0 gnupg gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client\n",
      "  gpg-wks-server gpgconf gpgsm krb5-locales libalgorithm-diff-perl\n",
      "  libalgorithm-diff-xs-perl libalgorithm-merge-perl libapparmor1 libasan5\n",
      "  libassuan0 libatomic1 libbinutils libc-dev-bin libc6-dev libcc1-0 libcurl4\n",
      "  libdbus-1-3 libdpkg-perl libexpat1 libexpat1-dev libfakeroot\n",
      "  libfile-fcntllock-perl libgcc-8-dev libgdbm-compat4 libgdbm6\n",
      "  libgirepository-1.0-1 libglib2.0-0 libglib2.0-data libgomp1 libgssapi-krb5-2\n",
      "  libicu63 libisl19 libitm1 libk5crypto3 libkeyutils1 libkrb5-3\n",
      "  libkrb5support0 libksba8 libldap-2.4-2 libldap-common liblocale-gettext-perl\n",
      "  liblsan0 libmagic-mgc libmagic1 libmpc3 libmpdec2 libmpfr6 libmpx2\n",
      "  libnghttp2-14 libnpth0 libperl5.28 libpsl5 libpython-dev libpython-stdlib\n",
      "  libpython2-dev libpython2-stdlib libpython2.7 libpython2.7-dev\n",
      "  libpython2.7-minimal libpython2.7-stdlib libpython3-dev libpython3-stdlib\n",
      "  libpython3.7 libpython3.7-dev libpython3.7-minimal libpython3.7-stdlib\n",
      "  libquadmath0 libreadline7 librtmp1 libsasl2-2 libsasl2-modules\n",
      "  libsasl2-modules-db libsqlite3-0 libssh2-1 libstdc++-8-dev libtsan0\n",
      "  libubsan1 libxml2 linux-libc-dev make manpages manpages-dev mime-support\n",
      "  netbase patch perl perl-modules-5.28 pinentry-curses publicsuffix python\n",
      "  python-minimal python-pip-whl python2 python2-dev python2-minimal python2.7\n",
      "  python2.7-dev python2.7-minimal python3-asn1crypto python3-cffi-backend\n",
      "  python3-crypto python3-cryptography python3-dbus python3-distutils\n",
      "  python3-entrypoints python3-gi python3-keyring python3-keyrings.alt\n",
      "  python3-lib2to3 python3-minimal python3-pkg-resources python3-secretstorage\n",
      "  python3-six python3-wheel python3-xdg python3.7 python3.7-dev\n",
      "  python3.7-minimal readline-common shared-mime-info xdg-user-dirs xz-utils\n",
      "Suggested packages:\n",
      "  binutils-doc bzip2-doc cpp-doc gcc-8-locales default-dbus-session-bus\n",
      "  | dbus-session-bus dbus-user-session libpam-systemd pinentry-gnome3 tor\n",
      "  debian-keyring g++-multilib g++-8-multilib gcc-8-doc libstdc++6-8-dbg\n",
      "  gcc-multilib autoconf automake libtool flex bison gdb gcc-doc gcc-8-multilib\n",
      "  libgcc1-dbg libgomp1-dbg libitm1-dbg libatomic1-dbg libasan5-dbg\n",
      "  liblsan0-dbg libtsan0-dbg libubsan1-dbg libmpx2-dbg libquadmath0-dbg\n",
      "  parcimonie xloadimage scdaemon glibc-doc sensible-utils git bzr gdbm-l10n\n",
      "  krb5-doc krb5-user libsasl2-modules-gssapi-mit\n",
      "  | libsasl2-modules-gssapi-heimdal libsasl2-modules-ldap libsasl2-modules-otp\n",
      "  libsasl2-modules-sql libstdc++-8-doc make-doc man-browser ed diffutils-doc\n",
      "  perl-doc libterm-readline-gnu-perl | libterm-readline-perl-perl\n",
      "  libb-debug-perl liblocale-codes-perl pinentry-doc python-doc python-tk\n",
      "  python-psutil-doc python2-doc python2.7-doc binfmt-support python3-doc\n",
      "  python3-tk python3-venv python-crypto-doc python-cryptography-doc\n",
      "  python3-cryptography-vectors python-dbus-doc python3-dbus-dbg gnome-keyring\n",
      "  libkf5wallet-bin gir1.2-gnomekeyring-1.0 python-secretstorage-doc\n",
      "  python-setuptools-doc python3.7-venv python3.7-doc readline-doc zip\n",
      "The following NEW packages will be installed:\n",
      "  binutils binutils-common binutils-x86-64-linux-gnu build-essential bzip2 cpp\n",
      "  cpp-8 curl dbus dh-python dirmngr dpkg-dev fakeroot file g++ g++-8 gcc gcc-8\n",
      "  gir1.2-glib-2.0 gnupg gnupg-l10n gnupg-utils gpg gpg-agent gpg-wks-client\n",
      "  gpg-wks-server gpgconf gpgsm krb5-locales libalgorithm-diff-perl\n",
      "  libalgorithm-diff-xs-perl libalgorithm-merge-perl libapparmor1 libasan5\n",
      "  libassuan0 libatomic1 libbinutils libc-dev-bin libc6-dev libcc1-0 libcurl4\n",
      "  libdbus-1-3 libdpkg-perl libexpat1 libexpat1-dev libfakeroot\n",
      "  libfile-fcntllock-perl libgcc-8-dev libgdbm-compat4 libgdbm6\n",
      "  libgirepository-1.0-1 libglib2.0-0 libglib2.0-data libgomp1 libgssapi-krb5-2\n",
      "  libicu63 libisl19 libitm1 libk5crypto3 libkeyutils1 libkrb5-3\n",
      "  libkrb5support0 libksba8 libldap-2.4-2 libldap-common liblocale-gettext-perl\n",
      "  liblsan0 libmagic-mgc libmagic1 libmpc3 libmpdec2 libmpfr6 libmpx2\n",
      "  libnghttp2-14 libnpth0 libperl5.28 libpsl5 libpython-dev libpython-stdlib\n",
      "  libpython2-dev libpython2-stdlib libpython2.7 libpython2.7-dev\n",
      "  libpython2.7-minimal libpython2.7-stdlib libpython3-dev libpython3-stdlib\n",
      "  libpython3.7 libpython3.7-dev libpython3.7-minimal libpython3.7-stdlib\n",
      "  libquadmath0 libreadline7 librtmp1 libsasl2-2 libsasl2-modules\n",
      "  libsasl2-modules-db libsqlite3-0 libssh2-1 libstdc++-8-dev libtsan0\n",
      "  libubsan1 libxml2 linux-libc-dev make manpages manpages-dev mime-support\n",
      "  netbase patch perl perl-modules-5.28 pinentry-curses publicsuffix python\n",
      "  python-dev python-minimal python-pip-whl python-psutil python2 python2-dev\n",
      "  python2-minimal python2.7 python2.7-dev python2.7-minimal python3\n",
      "  python3-asn1crypto python3-cffi-backend python3-crypto python3-cryptography\n",
      "  python3-dbus python3-dev python3-distutils python3-entrypoints python3-gi\n",
      "  python3-keyring python3-keyrings.alt python3-lib2to3 python3-minimal\n",
      "  python3-pip python3-pkg-resources python3-secretstorage python3-setuptools\n",
      "  python3-six python3-wheel python3-xdg python3.7 python3.7-dev\n",
      "  python3.7-minimal readline-common shared-mime-info unzip xdg-user-dirs\n",
      "  xz-utils\n",
      "0 upgraded, 154 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 178 MB of archives.\n",
      "After this operation, 521 MB of additional disk space will be used.\n",
      "Get:1 http://security.debian.org/debian-security buster/updates/main amd64 linux-libc-dev amd64 4.19.118-2+deb10u1 [1354 kB]\n",
      "Get:2 http://deb.debian.org/debian buster/main amd64 perl-modules-5.28 all 5.28.1-6 [2873 kB]\n",
      "Get:3 http://deb.debian.org/debian buster/main amd64 libgdbm6 amd64 1.18.1-4 [64.7 kB]\n",
      "Get:4 http://deb.debian.org/debian buster/main amd64 libgdbm-compat4 amd64 1.18.1-4 [44.1 kB]\n",
      "Get:5 http://deb.debian.org/debian buster/main amd64 libperl5.28 amd64 5.28.1-6 [3883 kB]\n",
      "Get:6 http://deb.debian.org/debian buster/main amd64 perl amd64 5.28.1-6 [204 kB]\n",
      "Get:7 http://deb.debian.org/debian buster/main amd64 libpython2.7-minimal amd64 2.7.16-2+deb10u1 [395 kB]\n",
      "Get:8 http://deb.debian.org/debian buster/main amd64 python2.7-minimal amd64 2.7.16-2+deb10u1 [1369 kB]\n",
      "Get:9 http://deb.debian.org/debian buster/main amd64 python2-minimal amd64 2.7.16-1 [41.4 kB]\n",
      "Get:10 http://deb.debian.org/debian buster/main amd64 python-minimal amd64 2.7.16-1 [21.0 kB]\n",
      "Get:11 http://deb.debian.org/debian buster/main amd64 mime-support all 3.62 [37.2 kB]\n",
      "Get:12 http://deb.debian.org/debian buster/main amd64 libexpat1 amd64 2.2.6-2+deb10u1 [106 kB]\n",
      "Get:13 http://deb.debian.org/debian buster/main amd64 readline-common all 7.0-5 [70.6 kB]\n",
      "Get:14 http://deb.debian.org/debian buster/main amd64 libreadline7 amd64 7.0-5 [151 kB]\n",
      "Get:15 http://deb.debian.org/debian buster/main amd64 libsqlite3-0 amd64 3.27.2-3 [641 kB]\n",
      "Get:16 http://deb.debian.org/debian buster/main amd64 libpython2.7-stdlib amd64 2.7.16-2+deb10u1 [1912 kB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:17 http://deb.debian.org/debian buster/main amd64 python2.7 amd64 2.7.16-2+deb10u1 [305 kB]\n",
      "Get:18 http://deb.debian.org/debian buster/main amd64 libpython2-stdlib amd64 2.7.16-1 [20.8 kB]\n",
      "Get:19 http://deb.debian.org/debian buster/main amd64 libpython-stdlib amd64 2.7.16-1 [20.8 kB]\n",
      "Get:20 http://deb.debian.org/debian buster/main amd64 python2 amd64 2.7.16-1 [41.6 kB]\n",
      "Get:21 http://deb.debian.org/debian buster/main amd64 python amd64 2.7.16-1 [22.8 kB]\n",
      "Get:22 http://deb.debian.org/debian buster/main amd64 liblocale-gettext-perl amd64 1.07-3+b4 [18.9 kB]\n",
      "Get:23 http://deb.debian.org/debian buster/main amd64 libpython3.7-minimal amd64 3.7.3-2+deb10u1 [589 kB]\n",
      "Get:24 http://deb.debian.org/debian buster/main amd64 python3.7-minimal amd64 3.7.3-2+deb10u1 [1736 kB]\n",
      "Get:25 http://deb.debian.org/debian buster/main amd64 python3-minimal amd64 3.7.3-1 [36.6 kB]\n",
      "Get:26 http://deb.debian.org/debian buster/main amd64 libmpdec2 amd64 2.4.2-2 [87.2 kB]\n",
      "Get:27 http://deb.debian.org/debian buster/main amd64 libpython3.7-stdlib amd64 3.7.3-2+deb10u1 [1734 kB]\n",
      "Get:28 http://deb.debian.org/debian buster/main amd64 python3.7 amd64 3.7.3-2+deb10u1 [330 kB]\n",
      "Get:29 http://deb.debian.org/debian buster/main amd64 libpython3-stdlib amd64 3.7.3-1 [20.0 kB]\n",
      "Get:30 http://deb.debian.org/debian buster/main amd64 python3 amd64 3.7.3-1 [61.5 kB]\n",
      "Get:31 http://deb.debian.org/debian buster/main amd64 netbase all 5.6 [19.4 kB]\n",
      "Get:32 http://deb.debian.org/debian buster/main amd64 bzip2 amd64 1.0.6-9.2~deb10u1 [48.4 kB]\n",
      "Get:33 http://deb.debian.org/debian buster/main amd64 libapparmor1 amd64 2.13.2-10 [94.7 kB]\n",
      "Get:34 http://deb.debian.org/debian buster/main amd64 libdbus-1-3 amd64 1.12.16-1 [214 kB]\n",
      "Get:35 http://deb.debian.org/debian buster/main amd64 dbus amd64 1.12.16-1 [235 kB]\n",
      "Get:36 http://deb.debian.org/debian buster/main amd64 libmagic-mgc amd64 1:5.35-4+deb10u1 [242 kB]\n",
      "Get:37 http://deb.debian.org/debian buster/main amd64 libmagic1 amd64 1:5.35-4+deb10u1 [117 kB]\n",
      "Get:38 http://deb.debian.org/debian buster/main amd64 file amd64 1:5.35-4+deb10u1 [66.4 kB]\n",
      "Get:39 http://deb.debian.org/debian buster/main amd64 krb5-locales all 1.17-3 [95.4 kB]\n",
      "Get:40 http://deb.debian.org/debian buster/main amd64 manpages all 4.16-2 [1295 kB]\n",
      "Get:41 http://deb.debian.org/debian buster/main amd64 xz-utils amd64 5.2.4-1 [183 kB]\n",
      "Get:42 http://deb.debian.org/debian buster/main amd64 binutils-common amd64 2.31.1-16 [2073 kB]\n",
      "Get:43 http://deb.debian.org/debian buster/main amd64 libbinutils amd64 2.31.1-16 [478 kB]\n",
      "Get:44 http://deb.debian.org/debian buster/main amd64 binutils-x86-64-linux-gnu amd64 2.31.1-16 [1823 kB]\n",
      "Get:45 http://deb.debian.org/debian buster/main amd64 binutils amd64 2.31.1-16 [56.8 kB]\n",
      "Get:46 http://deb.debian.org/debian buster/main amd64 libc-dev-bin amd64 2.28-10 [275 kB]\n",
      "Get:47 http://deb.debian.org/debian buster/main amd64 libc6-dev amd64 2.28-10 [2691 kB]\n",
      "Get:48 http://deb.debian.org/debian buster/main amd64 libisl19 amd64 0.20-2 [587 kB]\n",
      "Get:49 http://deb.debian.org/debian buster/main amd64 libmpfr6 amd64 4.0.2-1 [775 kB]\n",
      "Get:50 http://deb.debian.org/debian buster/main amd64 libmpc3 amd64 1.1.0-1 [41.3 kB]\n",
      "Get:51 http://deb.debian.org/debian buster/main amd64 cpp-8 amd64 8.3.0-6 [8914 kB]\n",
      "Get:52 http://deb.debian.org/debian buster/main amd64 cpp amd64 4:8.3.0-1 [19.4 kB]\n",
      "Get:53 http://deb.debian.org/debian buster/main amd64 libcc1-0 amd64 8.3.0-6 [46.6 kB]\n",
      "Get:54 http://deb.debian.org/debian buster/main amd64 libgomp1 amd64 8.3.0-6 [75.8 kB]\n",
      "Get:55 http://deb.debian.org/debian buster/main amd64 libitm1 amd64 8.3.0-6 [27.7 kB]\n",
      "Get:56 http://deb.debian.org/debian buster/main amd64 libatomic1 amd64 8.3.0-6 [9032 B]\n",
      "Get:57 http://deb.debian.org/debian buster/main amd64 libasan5 amd64 8.3.0-6 [362 kB]\n",
      "Get:58 http://deb.debian.org/debian buster/main amd64 liblsan0 amd64 8.3.0-6 [131 kB]\n",
      "Get:59 http://deb.debian.org/debian buster/main amd64 libtsan0 amd64 8.3.0-6 [283 kB]\n",
      "Get:60 http://deb.debian.org/debian buster/main amd64 libubsan1 amd64 8.3.0-6 [120 kB]\n",
      "Get:61 http://deb.debian.org/debian buster/main amd64 libmpx2 amd64 8.3.0-6 [11.4 kB]\n",
      "Get:62 http://deb.debian.org/debian buster/main amd64 libquadmath0 amd64 8.3.0-6 [133 kB]\n",
      "Get:63 http://deb.debian.org/debian buster/main amd64 libgcc-8-dev amd64 8.3.0-6 [2298 kB]\n",
      "Get:64 http://deb.debian.org/debian buster/main amd64 gcc-8 amd64 8.3.0-6 [9452 kB]\n",
      "Get:65 http://deb.debian.org/debian buster/main amd64 gcc amd64 4:8.3.0-1 [5196 B]\n",
      "Get:66 http://deb.debian.org/debian buster/main amd64 libstdc++-8-dev amd64 8.3.0-6 [1532 kB]\n",
      "Get:67 http://deb.debian.org/debian buster/main amd64 g++-8 amd64 8.3.0-6 [9752 kB]\n",
      "Get:68 http://deb.debian.org/debian buster/main amd64 g++ amd64 4:8.3.0-1 [1644 B]\n",
      "Get:69 http://deb.debian.org/debian buster/main amd64 make amd64 4.2.1-1.2 [341 kB]\n",
      "Get:70 http://deb.debian.org/debian buster/main amd64 libdpkg-perl all 1.19.7 [1414 kB]\n",
      "Get:71 http://deb.debian.org/debian buster/main amd64 patch amd64 2.7.6-3+deb10u1 [126 kB]\n",
      "Get:72 http://deb.debian.org/debian buster/main amd64 dpkg-dev all 1.19.7 [1773 kB]\n",
      "Get:73 http://deb.debian.org/debian buster/main amd64 build-essential amd64 12.6 [7576 B]\n",
      "Get:74 http://deb.debian.org/debian buster/main amd64 libkeyutils1 amd64 1.6-6 [15.0 kB]\n",
      "Get:75 http://deb.debian.org/debian buster/main amd64 libkrb5support0 amd64 1.17-3 [65.6 kB]\n",
      "Get:76 http://deb.debian.org/debian buster/main amd64 libk5crypto3 amd64 1.17-3 [121 kB]\n",
      "Get:77 http://deb.debian.org/debian buster/main amd64 libkrb5-3 amd64 1.17-3 [370 kB]\n",
      "Get:78 http://deb.debian.org/debian buster/main amd64 libgssapi-krb5-2 amd64 1.17-3 [158 kB]\n",
      "Get:79 http://deb.debian.org/debian buster/main amd64 libsasl2-modules-db amd64 2.1.27+dfsg-1+deb10u1 [69.1 kB]\n",
      "Get:80 http://deb.debian.org/debian buster/main amd64 libsasl2-2 amd64 2.1.27+dfsg-1+deb10u1 [106 kB]\n",
      "Get:81 http://deb.debian.org/debian buster/main amd64 libldap-common all 2.4.47+dfsg-3+deb10u2 [89.7 kB]\n",
      "Get:82 http://deb.debian.org/debian buster/main amd64 libldap-2.4-2 amd64 2.4.47+dfsg-3+deb10u2 [224 kB]\n",
      "Get:83 http://deb.debian.org/debian buster/main amd64 libnghttp2-14 amd64 1.36.0-2+deb10u1 [85.0 kB]\n",
      "Get:84 http://deb.debian.org/debian buster/main amd64 libpsl5 amd64 0.20.2-2 [53.7 kB]\n",
      "Get:85 http://deb.debian.org/debian buster/main amd64 librtmp1 amd64 2.4+20151223.gitfa8646d.1-2 [60.5 kB]\n",
      "Get:86 http://deb.debian.org/debian buster/main amd64 libssh2-1 amd64 1.8.0-2.1 [140 kB]\n",
      "Get:87 http://deb.debian.org/debian buster/main amd64 libcurl4 amd64 7.64.0-4+deb10u1 [331 kB]\n",
      "Get:88 http://deb.debian.org/debian buster/main amd64 curl amd64 7.64.0-4+deb10u1 [264 kB]\n",
      "Get:89 http://deb.debian.org/debian buster/main amd64 python3-lib2to3 all 3.7.3-1 [76.7 kB]\n",
      "Get:90 http://deb.debian.org/debian buster/main amd64 python3-distutils all 3.7.3-1 [142 kB]\n",
      "Get:91 http://deb.debian.org/debian buster/main amd64 dh-python all 3.20190308 [99.3 kB]\n",
      "Get:92 http://deb.debian.org/debian buster/main amd64 libassuan0 amd64 2.5.2-1 [49.4 kB]\n",
      "Get:93 http://deb.debian.org/debian buster/main amd64 gpgconf amd64 2.2.12-1+deb10u1 [510 kB]\n",
      "Get:94 http://deb.debian.org/debian buster/main amd64 libksba8 amd64 1.3.5-2 [99.7 kB]\n",
      "Get:95 http://deb.debian.org/debian buster/main amd64 libnpth0 amd64 1.6-1 [18.4 kB]\n",
      "Get:96 http://deb.debian.org/debian buster/main amd64 dirmngr amd64 2.2.12-1+deb10u1 [712 kB]\n",
      "Get:97 http://deb.debian.org/debian buster/main amd64 libfakeroot amd64 1.23-1 [45.9 kB]\n",
      "Get:98 http://deb.debian.org/debian buster/main amd64 fakeroot amd64 1.23-1 [85.8 kB]\n",
      "Get:99 http://deb.debian.org/debian buster/main amd64 libglib2.0-0 amd64 2.58.3-2+deb10u2 [1258 kB]\n",
      "Get:100 http://deb.debian.org/debian buster/main amd64 libgirepository-1.0-1 amd64 1.58.3-2 [92.8 kB]\n",
      "Get:101 http://deb.debian.org/debian buster/main amd64 gir1.2-glib-2.0 amd64 1.58.3-2 [143 kB]\n",
      "Get:102 http://deb.debian.org/debian buster/main amd64 gnupg-l10n all 2.2.12-1+deb10u1 [1010 kB]\n",
      "Get:103 http://deb.debian.org/debian buster/main amd64 gnupg-utils amd64 2.2.12-1+deb10u1 [861 kB]\n",
      "Get:104 http://deb.debian.org/debian buster/main amd64 gpg amd64 2.2.12-1+deb10u1 [865 kB]\n",
      "Get:105 http://deb.debian.org/debian buster/main amd64 pinentry-curses amd64 1.1.0-2 [64.5 kB]\n",
      "Get:106 http://deb.debian.org/debian buster/main amd64 gpg-agent amd64 2.2.12-1+deb10u1 [617 kB]\n",
      "Get:107 http://deb.debian.org/debian buster/main amd64 gpg-wks-client amd64 2.2.12-1+deb10u1 [485 kB]\n",
      "Get:108 http://deb.debian.org/debian buster/main amd64 gpg-wks-server amd64 2.2.12-1+deb10u1 [478 kB]\n",
      "Get:109 http://deb.debian.org/debian buster/main amd64 gpgsm amd64 2.2.12-1+deb10u1 [604 kB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:110 http://deb.debian.org/debian buster/main amd64 gnupg all 2.2.12-1+deb10u1 [715 kB]\n",
      "Get:111 http://deb.debian.org/debian buster/main amd64 libalgorithm-diff-perl all 1.19.03-2 [47.9 kB]\n",
      "Get:112 http://deb.debian.org/debian buster/main amd64 libalgorithm-diff-xs-perl amd64 0.04-5+b1 [11.8 kB]\n",
      "Get:113 http://deb.debian.org/debian buster/main amd64 libalgorithm-merge-perl all 0.08-3 [12.7 kB]\n",
      "Get:114 http://deb.debian.org/debian buster/main amd64 libexpat1-dev amd64 2.2.6-2+deb10u1 [153 kB]\n",
      "Get:115 http://deb.debian.org/debian buster/main amd64 libfile-fcntllock-perl amd64 0.22-3+b5 [35.4 kB]\n",
      "Get:116 http://deb.debian.org/debian buster/main amd64 libglib2.0-data all 2.58.3-2+deb10u2 [1110 kB]\n",
      "Get:117 http://deb.debian.org/debian buster/main amd64 libicu63 amd64 63.1-6+deb10u1 [8300 kB]\n",
      "Get:118 http://deb.debian.org/debian buster/main amd64 libpython2.7 amd64 2.7.16-2+deb10u1 [1036 kB]\n",
      "Get:119 http://deb.debian.org/debian buster/main amd64 libpython2.7-dev amd64 2.7.16-2+deb10u1 [31.6 MB]\n",
      "Get:120 http://deb.debian.org/debian buster/main amd64 libpython2-dev amd64 2.7.16-1 [20.9 kB]\n",
      "Get:121 http://deb.debian.org/debian buster/main amd64 libpython-dev amd64 2.7.16-1 [20.9 kB]\n",
      "Get:122 http://deb.debian.org/debian buster/main amd64 libpython3.7 amd64 3.7.3-2+deb10u1 [1498 kB]\n",
      "Get:123 http://deb.debian.org/debian buster/main amd64 libpython3.7-dev amd64 3.7.3-2+deb10u1 [48.4 MB]\n",
      "Get:124 http://deb.debian.org/debian buster/main amd64 libpython3-dev amd64 3.7.3-1 [20.1 kB]\n",
      "Get:125 http://deb.debian.org/debian buster/main amd64 libsasl2-modules amd64 2.1.27+dfsg-1+deb10u1 [104 kB]\n",
      "Get:126 http://deb.debian.org/debian buster/main amd64 libxml2 amd64 2.9.4+dfsg1-7+b3 [687 kB]\n",
      "Get:127 http://deb.debian.org/debian buster/main amd64 manpages-dev all 4.16-2 [2232 kB]\n",
      "Get:128 http://deb.debian.org/debian buster/main amd64 publicsuffix all 20190415.1030-1 [116 kB]\n",
      "Get:129 http://deb.debian.org/debian buster/main amd64 python2.7-dev amd64 2.7.16-2+deb10u1 [294 kB]\n",
      "Get:130 http://deb.debian.org/debian buster/main amd64 python2-dev amd64 2.7.16-1 [1212 B]\n",
      "Get:131 http://deb.debian.org/debian buster/main amd64 python-dev amd64 2.7.16-1 [1192 B]\n",
      "Get:132 http://deb.debian.org/debian buster/main amd64 python-pip-whl all 18.1-5 [1591 kB]\n",
      "Get:133 http://deb.debian.org/debian buster/main amd64 python-psutil amd64 5.5.1-1 [166 kB]\n",
      "Get:134 http://deb.debian.org/debian buster/main amd64 python3-asn1crypto all 0.24.0-1 [78.2 kB]\n",
      "Get:135 http://deb.debian.org/debian buster/main amd64 python3-cffi-backend amd64 1.12.2-1 [79.7 kB]\n",
      "Get:136 http://deb.debian.org/debian buster/main amd64 python3-crypto amd64 2.6.1-9+b1 [263 kB]\n",
      "Get:137 http://deb.debian.org/debian buster/main amd64 python3-six all 1.12.0-1 [15.7 kB]\n",
      "Get:138 http://deb.debian.org/debian buster/main amd64 python3-cryptography amd64 2.6.1-3+deb10u2 [219 kB]\n",
      "Get:139 http://deb.debian.org/debian buster/main amd64 python3-dbus amd64 1.2.8-3 [103 kB]\n",
      "Get:140 http://deb.debian.org/debian buster/main amd64 python3.7-dev amd64 3.7.3-2+deb10u1 [510 kB]\n",
      "Get:141 http://deb.debian.org/debian buster/main amd64 python3-dev amd64 3.7.3-1 [1264 B]\n",
      "Get:142 http://deb.debian.org/debian buster/main amd64 python3-entrypoints all 0.3-1 [5508 B]\n",
      "Get:143 http://deb.debian.org/debian buster/main amd64 python3-gi amd64 3.30.4-1 [180 kB]\n",
      "Get:144 http://deb.debian.org/debian buster/main amd64 python3-secretstorage all 2.3.1-2 [14.2 kB]\n",
      "Get:145 http://deb.debian.org/debian buster/main amd64 python3-keyring all 17.1.1-1 [43.1 kB]\n",
      "Get:146 http://deb.debian.org/debian buster/main amd64 python3-keyrings.alt all 3.1.1-1 [18.2 kB]\n",
      "Get:147 http://deb.debian.org/debian buster/main amd64 python3-pip all 18.1-5 [171 kB]\n",
      "Get:148 http://deb.debian.org/debian buster/main amd64 python3-pkg-resources all 40.8.0-1 [153 kB]\n",
      "Get:149 http://deb.debian.org/debian buster/main amd64 python3-setuptools all 40.8.0-1 [306 kB]\n",
      "Get:150 http://deb.debian.org/debian buster/main amd64 python3-wheel all 0.32.3-2 [19.4 kB]\n",
      "Get:151 http://deb.debian.org/debian buster/main amd64 python3-xdg all 0.25-5 [35.9 kB]\n",
      "Get:152 http://deb.debian.org/debian buster/main amd64 shared-mime-info amd64 1.10-1 [766 kB]\n",
      "Get:153 http://deb.debian.org/debian buster/main amd64 unzip amd64 6.0-23+deb10u1 [172 kB]\n",
      "Get:154 http://deb.debian.org/debian buster/main amd64 xdg-user-dirs amd64 0.17-2 [53.8 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 178 MB in 3s (65.8 MB/s)\n",
      "Selecting previously unselected package perl-modules-5.28.\n",
      "(Reading database ... 6889 files and directories currently installed.)\n",
      "Preparing to unpack .../00-perl-modules-5.28_5.28.1-6_all.deb ...\n",
      "Unpacking perl-modules-5.28 (5.28.1-6) ...\n",
      "Selecting previously unselected package libgdbm6:amd64.\n",
      "Preparing to unpack .../01-libgdbm6_1.18.1-4_amd64.deb ...\n",
      "Unpacking libgdbm6:amd64 (1.18.1-4) ...\n",
      "Selecting previously unselected package libgdbm-compat4:amd64.\n",
      "Preparing to unpack .../02-libgdbm-compat4_1.18.1-4_amd64.deb ...\n",
      "Unpacking libgdbm-compat4:amd64 (1.18.1-4) ...\n",
      "Selecting previously unselected package libperl5.28:amd64.\n",
      "Preparing to unpack .../03-libperl5.28_5.28.1-6_amd64.deb ...\n",
      "Unpacking libperl5.28:amd64 (5.28.1-6) ...\n",
      "Selecting previously unselected package perl.\n",
      "Preparing to unpack .../04-perl_5.28.1-6_amd64.deb ...\n",
      "Unpacking perl (5.28.1-6) ...\n",
      "Selecting previously unselected package libpython2.7-minimal:amd64.\n",
      "Preparing to unpack .../05-libpython2.7-minimal_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython2.7-minimal:amd64 (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package python2.7-minimal.\n",
      "Preparing to unpack .../06-python2.7-minimal_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking python2.7-minimal (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package python2-minimal.\n",
      "Preparing to unpack .../07-python2-minimal_2.7.16-1_amd64.deb ...\n",
      "Unpacking python2-minimal (2.7.16-1) ...\n",
      "Selecting previously unselected package python-minimal.\n",
      "Preparing to unpack .../08-python-minimal_2.7.16-1_amd64.deb ...\n",
      "Unpacking python-minimal (2.7.16-1) ...\n",
      "Selecting previously unselected package mime-support.\n",
      "Preparing to unpack .../09-mime-support_3.62_all.deb ...\n",
      "Unpacking mime-support (3.62) ...\n",
      "Selecting previously unselected package libexpat1:amd64.\n",
      "Preparing to unpack .../10-libexpat1_2.2.6-2+deb10u1_amd64.deb ...\n",
      "Unpacking libexpat1:amd64 (2.2.6-2+deb10u1) ...\n",
      "Selecting previously unselected package readline-common.\n",
      "Preparing to unpack .../11-readline-common_7.0-5_all.deb ...\n",
      "Unpacking readline-common (7.0-5) ...\n",
      "Selecting previously unselected package libreadline7:amd64.\n",
      "Preparing to unpack .../12-libreadline7_7.0-5_amd64.deb ...\n",
      "Unpacking libreadline7:amd64 (7.0-5) ...\n",
      "Selecting previously unselected package libsqlite3-0:amd64.\n",
      "Preparing to unpack .../13-libsqlite3-0_3.27.2-3_amd64.deb ...\n",
      "Unpacking libsqlite3-0:amd64 (3.27.2-3) ...\n",
      "Selecting previously unselected package libpython2.7-stdlib:amd64.\n",
      "Preparing to unpack .../14-libpython2.7-stdlib_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython2.7-stdlib:amd64 (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package python2.7.\n",
      "Preparing to unpack .../15-python2.7_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking python2.7 (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package libpython2-stdlib:amd64.\n",
      "Preparing to unpack .../16-libpython2-stdlib_2.7.16-1_amd64.deb ...\n",
      "Unpacking libpython2-stdlib:amd64 (2.7.16-1) ...\n",
      "Selecting previously unselected package libpython-stdlib:amd64.\n",
      "Preparing to unpack .../17-libpython-stdlib_2.7.16-1_amd64.deb ...\n",
      "Unpacking libpython-stdlib:amd64 (2.7.16-1) ...\n",
      "Setting up libpython2.7-minimal:amd64 (2.7.16-2+deb10u1) ...\n",
      "Setting up python2.7-minimal (2.7.16-2+deb10u1) ...\n",
      "Linking and byte-compiling packages for runtime python2.7...\n",
      "Setting up python2-minimal (2.7.16-1) ...\n",
      "Selecting previously unselected package python2.\n",
      "(Reading database ... 9661 files and directories currently installed.)\n",
      "Preparing to unpack .../python2_2.7.16-1_amd64.deb ...\n",
      "Unpacking python2 (2.7.16-1) ...\n",
      "Setting up python-minimal (2.7.16-1) ...\n",
      "Selecting previously unselected package python.\n",
      "(Reading database ... 9694 files and directories currently installed.)\n",
      "Preparing to unpack .../python_2.7.16-1_amd64.deb ...\n",
      "Unpacking python (2.7.16-1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package liblocale-gettext-perl.\n",
      "Preparing to unpack .../liblocale-gettext-perl_1.07-3+b4_amd64.deb ...\n",
      "Unpacking liblocale-gettext-perl (1.07-3+b4) ...\n",
      "Selecting previously unselected package libpython3.7-minimal:amd64.\n",
      "Preparing to unpack .../libpython3.7-minimal_3.7.3-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython3.7-minimal:amd64 (3.7.3-2+deb10u1) ...\n",
      "Selecting previously unselected package python3.7-minimal.\n",
      "Preparing to unpack .../python3.7-minimal_3.7.3-2+deb10u1_amd64.deb ...\n",
      "Unpacking python3.7-minimal (3.7.3-2+deb10u1) ...\n",
      "Setting up libpython3.7-minimal:amd64 (3.7.3-2+deb10u1) ...\n",
      "Setting up libexpat1:amd64 (2.2.6-2+deb10u1) ...\n",
      "Setting up python3.7-minimal (3.7.3-2+deb10u1) ...\n",
      "Selecting previously unselected package python3-minimal.\n",
      "(Reading database ... 9963 files and directories currently installed.)\n",
      "Preparing to unpack .../python3-minimal_3.7.3-1_amd64.deb ...\n",
      "Unpacking python3-minimal (3.7.3-1) ...\n",
      "Selecting previously unselected package libmpdec2:amd64.\n",
      "Preparing to unpack .../libmpdec2_2.4.2-2_amd64.deb ...\n",
      "Unpacking libmpdec2:amd64 (2.4.2-2) ...\n",
      "Selecting previously unselected package libpython3.7-stdlib:amd64.\n",
      "Preparing to unpack .../libpython3.7-stdlib_3.7.3-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython3.7-stdlib:amd64 (3.7.3-2+deb10u1) ...\n",
      "Selecting previously unselected package python3.7.\n",
      "Preparing to unpack .../python3.7_3.7.3-2+deb10u1_amd64.deb ...\n",
      "Unpacking python3.7 (3.7.3-2+deb10u1) ...\n",
      "Selecting previously unselected package libpython3-stdlib:amd64.\n",
      "Preparing to unpack .../libpython3-stdlib_3.7.3-1_amd64.deb ...\n",
      "Unpacking libpython3-stdlib:amd64 (3.7.3-1) ...\n",
      "Setting up python3-minimal (3.7.3-1) ...\n",
      "Selecting previously unselected package python3.\n",
      "(Reading database ... 10375 files and directories currently installed.)\n",
      "Preparing to unpack .../000-python3_3.7.3-1_amd64.deb ...\n",
      "Unpacking python3 (3.7.3-1) ...\n",
      "Selecting previously unselected package netbase.\n",
      "Preparing to unpack .../001-netbase_5.6_all.deb ...\n",
      "Unpacking netbase (5.6) ...\n",
      "Selecting previously unselected package bzip2.\n",
      "Preparing to unpack .../002-bzip2_1.0.6-9.2~deb10u1_amd64.deb ...\n",
      "Unpacking bzip2 (1.0.6-9.2~deb10u1) ...\n",
      "Selecting previously unselected package libapparmor1:amd64.\n",
      "Preparing to unpack .../003-libapparmor1_2.13.2-10_amd64.deb ...\n",
      "Unpacking libapparmor1:amd64 (2.13.2-10) ...\n",
      "Selecting previously unselected package libdbus-1-3:amd64.\n",
      "Preparing to unpack .../004-libdbus-1-3_1.12.16-1_amd64.deb ...\n",
      "Unpacking libdbus-1-3:amd64 (1.12.16-1) ...\n",
      "Selecting previously unselected package dbus.\n",
      "Preparing to unpack .../005-dbus_1.12.16-1_amd64.deb ...\n",
      "Unpacking dbus (1.12.16-1) ...\n",
      "Selecting previously unselected package libmagic-mgc.\n",
      "Preparing to unpack .../006-libmagic-mgc_1%3a5.35-4+deb10u1_amd64.deb ...\n",
      "Unpacking libmagic-mgc (1:5.35-4+deb10u1) ...\n",
      "Selecting previously unselected package libmagic1:amd64.\n",
      "Preparing to unpack .../007-libmagic1_1%3a5.35-4+deb10u1_amd64.deb ...\n",
      "Unpacking libmagic1:amd64 (1:5.35-4+deb10u1) ...\n",
      "Selecting previously unselected package file.\n",
      "Preparing to unpack .../008-file_1%3a5.35-4+deb10u1_amd64.deb ...\n",
      "Unpacking file (1:5.35-4+deb10u1) ...\n",
      "Selecting previously unselected package krb5-locales.\n",
      "Preparing to unpack .../009-krb5-locales_1.17-3_all.deb ...\n",
      "Unpacking krb5-locales (1.17-3) ...\n",
      "Selecting previously unselected package manpages.\n",
      "Preparing to unpack .../010-manpages_4.16-2_all.deb ...\n",
      "Unpacking manpages (4.16-2) ...\n",
      "Selecting previously unselected package xz-utils.\n",
      "Preparing to unpack .../011-xz-utils_5.2.4-1_amd64.deb ...\n",
      "Unpacking xz-utils (5.2.4-1) ...\n",
      "Selecting previously unselected package binutils-common:amd64.\n",
      "Preparing to unpack .../012-binutils-common_2.31.1-16_amd64.deb ...\n",
      "Unpacking binutils-common:amd64 (2.31.1-16) ...\n",
      "Selecting previously unselected package libbinutils:amd64.\n",
      "Preparing to unpack .../013-libbinutils_2.31.1-16_amd64.deb ...\n",
      "Unpacking libbinutils:amd64 (2.31.1-16) ...\n",
      "Selecting previously unselected package binutils-x86-64-linux-gnu.\n",
      "Preparing to unpack .../014-binutils-x86-64-linux-gnu_2.31.1-16_amd64.deb ...\n",
      "Unpacking binutils-x86-64-linux-gnu (2.31.1-16) ...\n",
      "Selecting previously unselected package binutils.\n",
      "Preparing to unpack .../015-binutils_2.31.1-16_amd64.deb ...\n",
      "Unpacking binutils (2.31.1-16) ...\n",
      "Selecting previously unselected package libc-dev-bin.\n",
      "Preparing to unpack .../016-libc-dev-bin_2.28-10_amd64.deb ...\n",
      "Unpacking libc-dev-bin (2.28-10) ...\n",
      "Selecting previously unselected package linux-libc-dev:amd64.\n",
      "Preparing to unpack .../017-linux-libc-dev_4.19.118-2+deb10u1_amd64.deb ...\n",
      "Unpacking linux-libc-dev:amd64 (4.19.118-2+deb10u1) ...\n",
      "Selecting previously unselected package libc6-dev:amd64.\n",
      "Preparing to unpack .../018-libc6-dev_2.28-10_amd64.deb ...\n",
      "Unpacking libc6-dev:amd64 (2.28-10) ...\n",
      "Selecting previously unselected package libisl19:amd64.\n",
      "Preparing to unpack .../019-libisl19_0.20-2_amd64.deb ...\n",
      "Unpacking libisl19:amd64 (0.20-2) ...\n",
      "Selecting previously unselected package libmpfr6:amd64.\n",
      "Preparing to unpack .../020-libmpfr6_4.0.2-1_amd64.deb ...\n",
      "Unpacking libmpfr6:amd64 (4.0.2-1) ...\n",
      "Selecting previously unselected package libmpc3:amd64.\n",
      "Preparing to unpack .../021-libmpc3_1.1.0-1_amd64.deb ...\n",
      "Unpacking libmpc3:amd64 (1.1.0-1) ...\n",
      "Selecting previously unselected package cpp-8.\n",
      "Preparing to unpack .../022-cpp-8_8.3.0-6_amd64.deb ...\n",
      "Unpacking cpp-8 (8.3.0-6) ...\n",
      "Selecting previously unselected package cpp.\n",
      "Preparing to unpack .../023-cpp_4%3a8.3.0-1_amd64.deb ...\n",
      "Unpacking cpp (4:8.3.0-1) ...\n",
      "Selecting previously unselected package libcc1-0:amd64.\n",
      "Preparing to unpack .../024-libcc1-0_8.3.0-6_amd64.deb ...\n",
      "Unpacking libcc1-0:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libgomp1:amd64.\n",
      "Preparing to unpack .../025-libgomp1_8.3.0-6_amd64.deb ...\n",
      "Unpacking libgomp1:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libitm1:amd64.\n",
      "Preparing to unpack .../026-libitm1_8.3.0-6_amd64.deb ...\n",
      "Unpacking libitm1:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libatomic1:amd64.\n",
      "Preparing to unpack .../027-libatomic1_8.3.0-6_amd64.deb ...\n",
      "Unpacking libatomic1:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libasan5:amd64.\n",
      "Preparing to unpack .../028-libasan5_8.3.0-6_amd64.deb ...\n",
      "Unpacking libasan5:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package liblsan0:amd64.\n",
      "Preparing to unpack .../029-liblsan0_8.3.0-6_amd64.deb ...\n",
      "Unpacking liblsan0:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libtsan0:amd64.\n",
      "Preparing to unpack .../030-libtsan0_8.3.0-6_amd64.deb ...\n",
      "Unpacking libtsan0:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libubsan1:amd64.\n",
      "Preparing to unpack .../031-libubsan1_8.3.0-6_amd64.deb ...\n",
      "Unpacking libubsan1:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libmpx2:amd64.\n",
      "Preparing to unpack .../032-libmpx2_8.3.0-6_amd64.deb ...\n",
      "Unpacking libmpx2:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libquadmath0:amd64.\n",
      "Preparing to unpack .../033-libquadmath0_8.3.0-6_amd64.deb ...\n",
      "Unpacking libquadmath0:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package libgcc-8-dev:amd64.\n",
      "Preparing to unpack .../034-libgcc-8-dev_8.3.0-6_amd64.deb ...\n",
      "Unpacking libgcc-8-dev:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package gcc-8.\n",
      "Preparing to unpack .../035-gcc-8_8.3.0-6_amd64.deb ...\n",
      "Unpacking gcc-8 (8.3.0-6) ...\n",
      "Selecting previously unselected package gcc.\n",
      "Preparing to unpack .../036-gcc_4%3a8.3.0-1_amd64.deb ...\n",
      "Unpacking gcc (4:8.3.0-1) ...\n",
      "Selecting previously unselected package libstdc++-8-dev:amd64.\n",
      "Preparing to unpack .../037-libstdc++-8-dev_8.3.0-6_amd64.deb ...\n",
      "Unpacking libstdc++-8-dev:amd64 (8.3.0-6) ...\n",
      "Selecting previously unselected package g++-8.\n",
      "Preparing to unpack .../038-g++-8_8.3.0-6_amd64.deb ...\n",
      "Unpacking g++-8 (8.3.0-6) ...\n",
      "Selecting previously unselected package g++.\n",
      "Preparing to unpack .../039-g++_4%3a8.3.0-1_amd64.deb ...\n",
      "Unpacking g++ (4:8.3.0-1) ...\n",
      "Selecting previously unselected package make.\n",
      "Preparing to unpack .../040-make_4.2.1-1.2_amd64.deb ...\n",
      "Unpacking make (4.2.1-1.2) ...\n",
      "Selecting previously unselected package libdpkg-perl.\n",
      "Preparing to unpack .../041-libdpkg-perl_1.19.7_all.deb ...\n",
      "Unpacking libdpkg-perl (1.19.7) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package patch.\n",
      "Preparing to unpack .../042-patch_2.7.6-3+deb10u1_amd64.deb ...\n",
      "Unpacking patch (2.7.6-3+deb10u1) ...\n",
      "Selecting previously unselected package dpkg-dev.\n",
      "Preparing to unpack .../043-dpkg-dev_1.19.7_all.deb ...\n",
      "Unpacking dpkg-dev (1.19.7) ...\n",
      "Selecting previously unselected package build-essential.\n",
      "Preparing to unpack .../044-build-essential_12.6_amd64.deb ...\n",
      "Unpacking build-essential (12.6) ...\n",
      "Selecting previously unselected package libkeyutils1:amd64.\n",
      "Preparing to unpack .../045-libkeyutils1_1.6-6_amd64.deb ...\n",
      "Unpacking libkeyutils1:amd64 (1.6-6) ...\n",
      "Selecting previously unselected package libkrb5support0:amd64.\n",
      "Preparing to unpack .../046-libkrb5support0_1.17-3_amd64.deb ...\n",
      "Unpacking libkrb5support0:amd64 (1.17-3) ...\n",
      "Selecting previously unselected package libk5crypto3:amd64.\n",
      "Preparing to unpack .../047-libk5crypto3_1.17-3_amd64.deb ...\n",
      "Unpacking libk5crypto3:amd64 (1.17-3) ...\n",
      "Selecting previously unselected package libkrb5-3:amd64.\n",
      "Preparing to unpack .../048-libkrb5-3_1.17-3_amd64.deb ...\n",
      "Unpacking libkrb5-3:amd64 (1.17-3) ...\n",
      "Selecting previously unselected package libgssapi-krb5-2:amd64.\n",
      "Preparing to unpack .../049-libgssapi-krb5-2_1.17-3_amd64.deb ...\n",
      "Unpacking libgssapi-krb5-2:amd64 (1.17-3) ...\n",
      "Selecting previously unselected package libsasl2-modules-db:amd64.\n",
      "Preparing to unpack .../050-libsasl2-modules-db_2.1.27+dfsg-1+deb10u1_amd64.deb ...\n",
      "Unpacking libsasl2-modules-db:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Selecting previously unselected package libsasl2-2:amd64.\n",
      "Preparing to unpack .../051-libsasl2-2_2.1.27+dfsg-1+deb10u1_amd64.deb ...\n",
      "Unpacking libsasl2-2:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Selecting previously unselected package libldap-common.\n",
      "Preparing to unpack .../052-libldap-common_2.4.47+dfsg-3+deb10u2_all.deb ...\n",
      "Unpacking libldap-common (2.4.47+dfsg-3+deb10u2) ...\n",
      "Selecting previously unselected package libldap-2.4-2:amd64.\n",
      "Preparing to unpack .../053-libldap-2.4-2_2.4.47+dfsg-3+deb10u2_amd64.deb ...\n",
      "Unpacking libldap-2.4-2:amd64 (2.4.47+dfsg-3+deb10u2) ...\n",
      "Selecting previously unselected package libnghttp2-14:amd64.\n",
      "Preparing to unpack .../054-libnghttp2-14_1.36.0-2+deb10u1_amd64.deb ...\n",
      "Unpacking libnghttp2-14:amd64 (1.36.0-2+deb10u1) ...\n",
      "Selecting previously unselected package libpsl5:amd64.\n",
      "Preparing to unpack .../055-libpsl5_0.20.2-2_amd64.deb ...\n",
      "Unpacking libpsl5:amd64 (0.20.2-2) ...\n",
      "Selecting previously unselected package librtmp1:amd64.\n",
      "Preparing to unpack .../056-librtmp1_2.4+20151223.gitfa8646d.1-2_amd64.deb ...\n",
      "Unpacking librtmp1:amd64 (2.4+20151223.gitfa8646d.1-2) ...\n",
      "Selecting previously unselected package libssh2-1:amd64.\n",
      "Preparing to unpack .../057-libssh2-1_1.8.0-2.1_amd64.deb ...\n",
      "Unpacking libssh2-1:amd64 (1.8.0-2.1) ...\n",
      "Selecting previously unselected package libcurl4:amd64.\n",
      "Preparing to unpack .../058-libcurl4_7.64.0-4+deb10u1_amd64.deb ...\n",
      "Unpacking libcurl4:amd64 (7.64.0-4+deb10u1) ...\n",
      "Selecting previously unselected package curl.\n",
      "Preparing to unpack .../059-curl_7.64.0-4+deb10u1_amd64.deb ...\n",
      "Unpacking curl (7.64.0-4+deb10u1) ...\n",
      "Selecting previously unselected package python3-lib2to3.\n",
      "Preparing to unpack .../060-python3-lib2to3_3.7.3-1_all.deb ...\n",
      "Unpacking python3-lib2to3 (3.7.3-1) ...\n",
      "Selecting previously unselected package python3-distutils.\n",
      "Preparing to unpack .../061-python3-distutils_3.7.3-1_all.deb ...\n",
      "Unpacking python3-distutils (3.7.3-1) ...\n",
      "Selecting previously unselected package dh-python.\n",
      "Preparing to unpack .../062-dh-python_3.20190308_all.deb ...\n",
      "Unpacking dh-python (3.20190308) ...\n",
      "Selecting previously unselected package libassuan0:amd64.\n",
      "Preparing to unpack .../063-libassuan0_2.5.2-1_amd64.deb ...\n",
      "Unpacking libassuan0:amd64 (2.5.2-1) ...\n",
      "Selecting previously unselected package gpgconf.\n",
      "Preparing to unpack .../064-gpgconf_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpgconf (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package libksba8:amd64.\n",
      "Preparing to unpack .../065-libksba8_1.3.5-2_amd64.deb ...\n",
      "Unpacking libksba8:amd64 (1.3.5-2) ...\n",
      "Selecting previously unselected package libnpth0:amd64.\n",
      "Preparing to unpack .../066-libnpth0_1.6-1_amd64.deb ...\n",
      "Unpacking libnpth0:amd64 (1.6-1) ...\n",
      "Selecting previously unselected package dirmngr.\n",
      "Preparing to unpack .../067-dirmngr_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking dirmngr (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package libfakeroot:amd64.\n",
      "Preparing to unpack .../068-libfakeroot_1.23-1_amd64.deb ...\n",
      "Unpacking libfakeroot:amd64 (1.23-1) ...\n",
      "Selecting previously unselected package fakeroot.\n",
      "Preparing to unpack .../069-fakeroot_1.23-1_amd64.deb ...\n",
      "Unpacking fakeroot (1.23-1) ...\n",
      "Selecting previously unselected package libglib2.0-0:amd64.\n",
      "Preparing to unpack .../070-libglib2.0-0_2.58.3-2+deb10u2_amd64.deb ...\n",
      "Unpacking libglib2.0-0:amd64 (2.58.3-2+deb10u2) ...\n",
      "Selecting previously unselected package libgirepository-1.0-1:amd64.\n",
      "Preparing to unpack .../071-libgirepository-1.0-1_1.58.3-2_amd64.deb ...\n",
      "Unpacking libgirepository-1.0-1:amd64 (1.58.3-2) ...\n",
      "Selecting previously unselected package gir1.2-glib-2.0:amd64.\n",
      "Preparing to unpack .../072-gir1.2-glib-2.0_1.58.3-2_amd64.deb ...\n",
      "Unpacking gir1.2-glib-2.0:amd64 (1.58.3-2) ...\n",
      "Selecting previously unselected package gnupg-l10n.\n",
      "Preparing to unpack .../073-gnupg-l10n_2.2.12-1+deb10u1_all.deb ...\n",
      "Unpacking gnupg-l10n (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gnupg-utils.\n",
      "Preparing to unpack .../074-gnupg-utils_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gnupg-utils (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gpg.\n",
      "Preparing to unpack .../075-gpg_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpg (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package pinentry-curses.\n",
      "Preparing to unpack .../076-pinentry-curses_1.1.0-2_amd64.deb ...\n",
      "Unpacking pinentry-curses (1.1.0-2) ...\n",
      "Selecting previously unselected package gpg-agent.\n",
      "Preparing to unpack .../077-gpg-agent_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpg-agent (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gpg-wks-client.\n",
      "Preparing to unpack .../078-gpg-wks-client_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpg-wks-client (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gpg-wks-server.\n",
      "Preparing to unpack .../079-gpg-wks-server_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpg-wks-server (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gpgsm.\n",
      "Preparing to unpack .../080-gpgsm_2.2.12-1+deb10u1_amd64.deb ...\n",
      "Unpacking gpgsm (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package gnupg.\n",
      "Preparing to unpack .../081-gnupg_2.2.12-1+deb10u1_all.deb ...\n",
      "Unpacking gnupg (2.2.12-1+deb10u1) ...\n",
      "Selecting previously unselected package libalgorithm-diff-perl.\n",
      "Preparing to unpack .../082-libalgorithm-diff-perl_1.19.03-2_all.deb ...\n",
      "Unpacking libalgorithm-diff-perl (1.19.03-2) ...\n",
      "Selecting previously unselected package libalgorithm-diff-xs-perl.\n",
      "Preparing to unpack .../083-libalgorithm-diff-xs-perl_0.04-5+b1_amd64.deb ...\n",
      "Unpacking libalgorithm-diff-xs-perl (0.04-5+b1) ...\n",
      "Selecting previously unselected package libalgorithm-merge-perl.\n",
      "Preparing to unpack .../084-libalgorithm-merge-perl_0.08-3_all.deb ...\n",
      "Unpacking libalgorithm-merge-perl (0.08-3) ...\n",
      "Selecting previously unselected package libexpat1-dev:amd64.\n",
      "Preparing to unpack .../085-libexpat1-dev_2.2.6-2+deb10u1_amd64.deb ...\n",
      "Unpacking libexpat1-dev:amd64 (2.2.6-2+deb10u1) ...\n",
      "Selecting previously unselected package libfile-fcntllock-perl.\n",
      "Preparing to unpack .../086-libfile-fcntllock-perl_0.22-3+b5_amd64.deb ...\n",
      "Unpacking libfile-fcntllock-perl (0.22-3+b5) ...\n",
      "Selecting previously unselected package libglib2.0-data.\n",
      "Preparing to unpack .../087-libglib2.0-data_2.58.3-2+deb10u2_all.deb ...\n",
      "Unpacking libglib2.0-data (2.58.3-2+deb10u2) ...\n",
      "Selecting previously unselected package libicu63:amd64.\n",
      "Preparing to unpack .../088-libicu63_63.1-6+deb10u1_amd64.deb ...\n",
      "Unpacking libicu63:amd64 (63.1-6+deb10u1) ...\n",
      "Selecting previously unselected package libpython2.7:amd64.\n",
      "Preparing to unpack .../089-libpython2.7_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython2.7:amd64 (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package libpython2.7-dev:amd64.\n",
      "Preparing to unpack .../090-libpython2.7-dev_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython2.7-dev:amd64 (2.7.16-2+deb10u1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libpython2-dev:amd64.\n",
      "Preparing to unpack .../091-libpython2-dev_2.7.16-1_amd64.deb ...\n",
      "Unpacking libpython2-dev:amd64 (2.7.16-1) ...\n",
      "Selecting previously unselected package libpython-dev:amd64.\n",
      "Preparing to unpack .../092-libpython-dev_2.7.16-1_amd64.deb ...\n",
      "Unpacking libpython-dev:amd64 (2.7.16-1) ...\n",
      "Selecting previously unselected package libpython3.7:amd64.\n",
      "Preparing to unpack .../093-libpython3.7_3.7.3-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython3.7:amd64 (3.7.3-2+deb10u1) ...\n",
      "Selecting previously unselected package libpython3.7-dev:amd64.\n",
      "Preparing to unpack .../094-libpython3.7-dev_3.7.3-2+deb10u1_amd64.deb ...\n",
      "Unpacking libpython3.7-dev:amd64 (3.7.3-2+deb10u1) ...\n",
      "Selecting previously unselected package libpython3-dev:amd64.\n",
      "Preparing to unpack .../095-libpython3-dev_3.7.3-1_amd64.deb ...\n",
      "Unpacking libpython3-dev:amd64 (3.7.3-1) ...\n",
      "Selecting previously unselected package libsasl2-modules:amd64.\n",
      "Preparing to unpack .../096-libsasl2-modules_2.1.27+dfsg-1+deb10u1_amd64.deb ...\n",
      "Unpacking libsasl2-modules:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Selecting previously unselected package libxml2:amd64.\n",
      "Preparing to unpack .../097-libxml2_2.9.4+dfsg1-7+b3_amd64.deb ...\n",
      "Unpacking libxml2:amd64 (2.9.4+dfsg1-7+b3) ...\n",
      "Selecting previously unselected package manpages-dev.\n",
      "Preparing to unpack .../098-manpages-dev_4.16-2_all.deb ...\n",
      "Unpacking manpages-dev (4.16-2) ...\n",
      "Selecting previously unselected package publicsuffix.\n",
      "Preparing to unpack .../099-publicsuffix_20190415.1030-1_all.deb ...\n",
      "Unpacking publicsuffix (20190415.1030-1) ...\n",
      "Selecting previously unselected package python2.7-dev.\n",
      "Preparing to unpack .../100-python2.7-dev_2.7.16-2+deb10u1_amd64.deb ...\n",
      "Unpacking python2.7-dev (2.7.16-2+deb10u1) ...\n",
      "Selecting previously unselected package python2-dev.\n",
      "Preparing to unpack .../101-python2-dev_2.7.16-1_amd64.deb ...\n",
      "Unpacking python2-dev (2.7.16-1) ...\n",
      "Selecting previously unselected package python-dev.\n",
      "Preparing to unpack .../102-python-dev_2.7.16-1_amd64.deb ...\n",
      "Unpacking python-dev (2.7.16-1) ...\n",
      "Selecting previously unselected package python-pip-whl.\n",
      "Preparing to unpack .../103-python-pip-whl_18.1-5_all.deb ...\n",
      "Unpacking python-pip-whl (18.1-5) ...\n",
      "Selecting previously unselected package python-psutil.\n",
      "Preparing to unpack .../104-python-psutil_5.5.1-1_amd64.deb ...\n",
      "Unpacking python-psutil (5.5.1-1) ...\n",
      "Selecting previously unselected package python3-asn1crypto.\n",
      "Preparing to unpack .../105-python3-asn1crypto_0.24.0-1_all.deb ...\n",
      "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
      "Selecting previously unselected package python3-cffi-backend.\n",
      "Preparing to unpack .../106-python3-cffi-backend_1.12.2-1_amd64.deb ...\n",
      "Unpacking python3-cffi-backend (1.12.2-1) ...\n",
      "Selecting previously unselected package python3-crypto.\n",
      "Preparing to unpack .../107-python3-crypto_2.6.1-9+b1_amd64.deb ...\n",
      "Unpacking python3-crypto (2.6.1-9+b1) ...\n",
      "Selecting previously unselected package python3-six.\n",
      "Preparing to unpack .../108-python3-six_1.12.0-1_all.deb ...\n",
      "Unpacking python3-six (1.12.0-1) ...\n",
      "Selecting previously unselected package python3-cryptography.\n",
      "Preparing to unpack .../109-python3-cryptography_2.6.1-3+deb10u2_amd64.deb ...\n",
      "Unpacking python3-cryptography (2.6.1-3+deb10u2) ...\n",
      "Selecting previously unselected package python3-dbus.\n",
      "Preparing to unpack .../110-python3-dbus_1.2.8-3_amd64.deb ...\n",
      "Unpacking python3-dbus (1.2.8-3) ...\n",
      "Selecting previously unselected package python3.7-dev.\n",
      "Preparing to unpack .../111-python3.7-dev_3.7.3-2+deb10u1_amd64.deb ...\n",
      "Unpacking python3.7-dev (3.7.3-2+deb10u1) ...\n",
      "Selecting previously unselected package python3-dev.\n",
      "Preparing to unpack .../112-python3-dev_3.7.3-1_amd64.deb ...\n",
      "Unpacking python3-dev (3.7.3-1) ...\n",
      "Selecting previously unselected package python3-entrypoints.\n",
      "Preparing to unpack .../113-python3-entrypoints_0.3-1_all.deb ...\n",
      "Unpacking python3-entrypoints (0.3-1) ...\n",
      "Selecting previously unselected package python3-gi.\n",
      "Preparing to unpack .../114-python3-gi_3.30.4-1_amd64.deb ...\n",
      "Unpacking python3-gi (3.30.4-1) ...\n",
      "Selecting previously unselected package python3-secretstorage.\n",
      "Preparing to unpack .../115-python3-secretstorage_2.3.1-2_all.deb ...\n",
      "Unpacking python3-secretstorage (2.3.1-2) ...\n",
      "Selecting previously unselected package python3-keyring.\n",
      "Preparing to unpack .../116-python3-keyring_17.1.1-1_all.deb ...\n",
      "Unpacking python3-keyring (17.1.1-1) ...\n",
      "Selecting previously unselected package python3-keyrings.alt.\n",
      "Preparing to unpack .../117-python3-keyrings.alt_3.1.1-1_all.deb ...\n",
      "Unpacking python3-keyrings.alt (3.1.1-1) ...\n",
      "Selecting previously unselected package python3-pip.\n",
      "Preparing to unpack .../118-python3-pip_18.1-5_all.deb ...\n",
      "Unpacking python3-pip (18.1-5) ...\n",
      "Selecting previously unselected package python3-pkg-resources.\n",
      "Preparing to unpack .../119-python3-pkg-resources_40.8.0-1_all.deb ...\n",
      "Unpacking python3-pkg-resources (40.8.0-1) ...\n",
      "Selecting previously unselected package python3-setuptools.\n",
      "Preparing to unpack .../120-python3-setuptools_40.8.0-1_all.deb ...\n",
      "Unpacking python3-setuptools (40.8.0-1) ...\n",
      "Selecting previously unselected package python3-wheel.\n",
      "Preparing to unpack .../121-python3-wheel_0.32.3-2_all.deb ...\n",
      "Unpacking python3-wheel (0.32.3-2) ...\n",
      "Selecting previously unselected package python3-xdg.\n",
      "Preparing to unpack .../122-python3-xdg_0.25-5_all.deb ...\n",
      "Unpacking python3-xdg (0.25-5) ...\n",
      "Selecting previously unselected package shared-mime-info.\n",
      "Preparing to unpack .../123-shared-mime-info_1.10-1_amd64.deb ...\n",
      "Unpacking shared-mime-info (1.10-1) ...\n",
      "Selecting previously unselected package unzip.\n",
      "Preparing to unpack .../124-unzip_6.0-23+deb10u1_amd64.deb ...\n",
      "Unpacking unzip (6.0-23+deb10u1) ...\n",
      "Selecting previously unselected package xdg-user-dirs.\n",
      "Preparing to unpack .../125-xdg-user-dirs_0.17-2_amd64.deb ...\n",
      "Unpacking xdg-user-dirs (0.17-2) ...\n",
      "Setting up perl-modules-5.28 (5.28.1-6) ...\n",
      "Setting up libksba8:amd64 (1.3.5-2) ...\n",
      "Setting up libkeyutils1:amd64 (1.6-6) ...\n",
      "Setting up libapparmor1:amd64 (2.13.2-10) ...\n",
      "Setting up libpsl5:amd64 (0.20.2-2) ...\n",
      "Setting up mime-support (3.62) ...\n",
      "Setting up xdg-user-dirs (0.17-2) ...\n",
      "Setting up libmagic-mgc (1:5.35-4+deb10u1) ...\n",
      "Setting up libglib2.0-0:amd64 (2.58.3-2+deb10u2) ...\n",
      "No schema files found: doing nothing.\n",
      "Setting up manpages (4.16-2) ...\n",
      "Setting up unzip (6.0-23+deb10u1) ...\n",
      "Setting up libsqlite3-0:amd64 (3.27.2-3) ...\n",
      "Setting up libsasl2-modules:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Setting up binutils-common:amd64 (2.31.1-16) ...\n",
      "Setting up libnghttp2-14:amd64 (1.36.0-2+deb10u1) ...\n",
      "Setting up libmagic1:amd64 (1:5.35-4+deb10u1) ...\n",
      "Setting up linux-libc-dev:amd64 (4.19.118-2+deb10u1) ...\n",
      "Setting up libnpth0:amd64 (1.6-1) ...\n",
      "Setting up krb5-locales (1.17-3) ...\n",
      "Setting up file (1:5.35-4+deb10u1) ...\n",
      "Setting up libassuan0:amd64 (2.5.2-1) ...\n",
      "Setting up libgomp1:amd64 (8.3.0-6) ...\n",
      "Setting up bzip2 (1.0.6-9.2~deb10u1) ...\n",
      "Setting up libldap-common (2.4.47+dfsg-3+deb10u2) ...\n",
      "Setting up libicu63:amd64 (63.1-6+deb10u1) ...\n",
      "Setting up libfakeroot:amd64 (1.23-1) ...\n",
      "Setting up libkrb5support0:amd64 (1.17-3) ...\n",
      "Setting up libsasl2-modules-db:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Setting up fakeroot (1.23-1) ...\n",
      "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/fakeroot.1.gz because associated file /usr/share/man/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/faked.1.gz because associated file /usr/share/man/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/fakeroot.1.gz because associated file /usr/share/man/es/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/es/man1/faked.1.gz because associated file /usr/share/man/es/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/fakeroot.1.gz because associated file /usr/share/man/fr/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/fr/man1/faked.1.gz because associated file /usr/share/man/fr/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/fakeroot.1.gz because associated file /usr/share/man/sv/man1/fakeroot-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/sv/man1/faked.1.gz because associated file /usr/share/man/sv/man1/faked-sysv.1.gz (of link group fakeroot) doesn't exist\n",
      "Setting up libasan5:amd64 (8.3.0-6) ...\n",
      "Setting up libglib2.0-data (2.58.3-2+deb10u2) ...\n",
      "Setting up make (4.2.1-1.2) ...\n",
      "Setting up libmpfr6:amd64 (4.0.2-1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up gnupg-l10n (2.2.12-1+deb10u1) ...\n",
      "Setting up librtmp1:amd64 (2.4+20151223.gitfa8646d.1-2) ...\n",
      "Setting up libdbus-1-3:amd64 (1.12.16-1) ...\n",
      "Setting up dbus (1.12.16-1) ...\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up xz-utils (5.2.4-1) ...\n",
      "update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist\n",
      "Setting up libquadmath0:amd64 (8.3.0-6) ...\n",
      "Setting up libmpc3:amd64 (1.1.0-1) ...\n",
      "Setting up libatomic1:amd64 (8.3.0-6) ...\n",
      "Setting up patch (2.7.6-3+deb10u1) ...\n",
      "Setting up libk5crypto3:amd64 (1.17-3) ...\n",
      "Setting up libsasl2-2:amd64 (2.1.27+dfsg-1+deb10u1) ...\n",
      "Setting up libmpx2:amd64 (8.3.0-6) ...\n",
      "Setting up libubsan1:amd64 (8.3.0-6) ...\n",
      "Setting up libisl19:amd64 (0.20-2) ...\n",
      "Setting up libgirepository-1.0-1:amd64 (1.58.3-2) ...\n",
      "Setting up libssh2-1:amd64 (1.8.0-2.1) ...\n",
      "Setting up netbase (5.6) ...\n",
      "Setting up python-pip-whl (18.1-5) ...\n",
      "Setting up libkrb5-3:amd64 (1.17-3) ...\n",
      "Setting up libmpdec2:amd64 (2.4.2-2) ...\n",
      "Setting up libbinutils:amd64 (2.31.1-16) ...\n",
      "Setting up cpp-8 (8.3.0-6) ...\n",
      "Setting up libc-dev-bin (2.28-10) ...\n",
      "Setting up readline-common (7.0-5) ...\n",
      "Setting up publicsuffix (20190415.1030-1) ...\n",
      "Setting up libxml2:amd64 (2.9.4+dfsg1-7+b3) ...\n",
      "Setting up libcc1-0:amd64 (8.3.0-6) ...\n",
      "Setting up liblocale-gettext-perl (1.07-3+b4) ...\n",
      "Setting up liblsan0:amd64 (8.3.0-6) ...\n",
      "Setting up libitm1:amd64 (8.3.0-6) ...\n",
      "Setting up libreadline7:amd64 (7.0-5) ...\n",
      "Setting up libgdbm6:amd64 (1.18.1-4) ...\n",
      "Setting up gnupg-utils (2.2.12-1+deb10u1) ...\n",
      "Setting up binutils-x86-64-linux-gnu (2.31.1-16) ...\n",
      "Setting up libtsan0:amd64 (8.3.0-6) ...\n",
      "Setting up pinentry-curses (1.1.0-2) ...\n",
      "Setting up manpages-dev (4.16-2) ...\n",
      "Setting up libpython3.7-stdlib:amd64 (3.7.3-2+deb10u1) ...\n",
      "Setting up libpython3.7:amd64 (3.7.3-2+deb10u1) ...\n",
      "Setting up libldap-2.4-2:amd64 (2.4.47+dfsg-3+deb10u2) ...\n",
      "Setting up binutils (2.31.1-16) ...\n",
      "Setting up libpython2.7-stdlib:amd64 (2.7.16-2+deb10u1) ...\n",
      "Setting up shared-mime-info (1.10-1) ...\n",
      "Setting up libgssapi-krb5-2:amd64 (1.17-3) ...\n",
      "Setting up libgdbm-compat4:amd64 (1.18.1-4) ...\n",
      "Setting up gir1.2-glib-2.0:amd64 (1.58.3-2) ...\n",
      "Setting up libgcc-8-dev:amd64 (8.3.0-6) ...\n",
      "Setting up libperl5.28:amd64 (5.28.1-6) ...\n",
      "Setting up cpp (4:8.3.0-1) ...\n",
      "Setting up gpgconf (2.2.12-1+deb10u1) ...\n",
      "Setting up libcurl4:amd64 (7.64.0-4+deb10u1) ...\n",
      "Setting up libc6-dev:amd64 (2.28-10) ...\n",
      "Setting up curl (7.64.0-4+deb10u1) ...\n",
      "Setting up gpg (2.2.12-1+deb10u1) ...\n",
      "Setting up libpython3-stdlib:amd64 (3.7.3-1) ...\n",
      "Setting up libstdc++-8-dev:amd64 (8.3.0-6) ...\n",
      "Setting up python3.7 (3.7.3-2+deb10u1) ...\n",
      "Setting up libpython2.7:amd64 (2.7.16-2+deb10u1) ...\n",
      "Setting up gcc-8 (8.3.0-6) ...\n",
      "Setting up gpg-agent (2.2.12-1+deb10u1) ...\n",
      "Setting up python2.7 (2.7.16-2+deb10u1) ...\n",
      "Setting up libpython2-stdlib:amd64 (2.7.16-1) ...\n",
      "Setting up gpgsm (2.2.12-1+deb10u1) ...\n",
      "Setting up python3 (3.7.3-1) ...\n",
      "running python rtupdate hooks for python3.7...\n",
      "running python post-rtupdate hooks for python3.7...\n",
      "Setting up python3-xdg (0.25-5) ...\n",
      "Setting up python3-wheel (0.32.3-2) ...\n",
      "Setting up python2 (2.7.16-1) ...\n",
      "Setting up gcc (4:8.3.0-1) ...\n",
      "Setting up python3-six (1.12.0-1) ...\n",
      "Setting up dirmngr (2.2.12-1+deb10u1) ...\n",
      "Setting up libpython-stdlib:amd64 (2.7.16-1) ...\n",
      "Setting up perl (5.28.1-6) ...\n",
      "Setting up libexpat1-dev:amd64 (2.2.6-2+deb10u1) ...\n",
      "Setting up python3-gi (3.30.4-1) ...\n",
      "Setting up libdpkg-perl (1.19.7) ...\n",
      "Setting up gpg-wks-server (2.2.12-1+deb10u1) ...\n",
      "Setting up g++-8 (8.3.0-6) ...\n",
      "Setting up python3-crypto (2.6.1-9+b1) ...\n",
      "Setting up python3-lib2to3 (3.7.3-1) ...\n",
      "Setting up python (2.7.16-1) ...\n",
      "Setting up python3-asn1crypto (0.24.0-1) ...\n",
      "Setting up python3-cffi-backend (1.12.2-1) ...\n",
      "Setting up python3-pkg-resources (40.8.0-1) ...\n",
      "Setting up python3-entrypoints (0.3-1) ...\n",
      "Setting up python3-distutils (3.7.3-1) ...\n",
      "Setting up dh-python (3.20190308) ...\n",
      "Setting up python3-dbus (1.2.8-3) ...\n",
      "Setting up libpython2.7-dev:amd64 (2.7.16-2+deb10u1) ...\n",
      "Setting up python3-setuptools (40.8.0-1) ...\n",
      "Setting up gpg-wks-client (2.2.12-1+deb10u1) ...\n",
      "Setting up libfile-fcntllock-perl (0.22-3+b5) ...\n",
      "Setting up libalgorithm-diff-perl (1.19.03-2) ...\n",
      "Setting up libpython3.7-dev:amd64 (3.7.3-2+deb10u1) ...\n",
      "Setting up python3.7-dev (3.7.3-2+deb10u1) ...\n",
      "Setting up dpkg-dev (1.19.7) ...\n",
      "Setting up python3-cryptography (2.6.1-3+deb10u2) ...\n",
      "Setting up python3-pip (18.1-5) ...\n",
      "Setting up python-psutil (5.5.1-1) ...\n",
      "Setting up g++ (4:8.3.0-1) ...\n",
      "update-alternatives: using /usr/bin/g++ to provide /usr/bin/c++ (c++) in auto mode\n",
      "Setting up python3-keyrings.alt (3.1.1-1) ...\n",
      "Setting up gnupg (2.2.12-1+deb10u1) ...\n",
      "Setting up build-essential (12.6) ...\n",
      "Setting up libpython2-dev:amd64 (2.7.16-1) ...\n",
      "Setting up libalgorithm-diff-xs-perl (0.04-5+b1) ...\n",
      "Setting up libalgorithm-merge-perl (0.08-3) ...\n",
      "Setting up python2.7-dev (2.7.16-2+deb10u1) ...\n",
      "Setting up libpython3-dev:amd64 (3.7.3-1) ...\n",
      "Setting up python2-dev (2.7.16-1) ...\n",
      "Setting up libpython-dev:amd64 (2.7.16-1) ...\n",
      "Setting up python3-secretstorage (2.3.1-2) ...\n",
      "Setting up python3-dev (3.7.3-1) ...\n",
      "Setting up python3-keyring (17.1.1-1) ...\n",
      "Setting up python-dev (2.7.16-1) ...\n",
      "Processing triggers for libc-bin (2.28-10) ...\n",
      "Removing intermediate container d5bb702f8a9c\n",
      " ---> d89d8588c27a\n",
      "Step 4/33 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Running in 1566f0905e65\n",
      "Collecting py4j\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
      "Collecting psutil==5.6.5\n",
      "  Downloading https://files.pythonhosted.org/packages/03/9a/95c4b3d0424426e5fd94b5302ff74cea44d5d4f53466e1228ac8e73e14b4/psutil-5.6.5.tar.gz (447kB)\n",
      "Collecting numpy==1.17.4\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/af/4fc72f9d38e43b092e91e5b8cb9956d25b2e3ff8c75aed95df5569e4734e/numpy-1.17.4-cp37-cp37m-manylinux1_x86_64.whl (20.0MB)\n",
      "Building wheels for collected packages: psutil\n",
      "  Running setup.py bdist_wheel for psutil: started\n",
      "  Running setup.py bdist_wheel for psutil: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/33/48/b6/72b7243c5caf65b7d5b460e9fad82b1256992284e870b7db59\n",
      "Successfully built psutil\n",
      "Installing collected packages: py4j, psutil, numpy\n",
      "Successfully installed numpy-1.17.4 psutil-5.6.5 py4j-0.10.9\n",
      "Removing intermediate container 1566f0905e65\n",
      " ---> 9c4ed3a036d1\n",
      "Step 5/33 : RUN apt-get clean\n",
      " ---> Running in bfdedbdbc58c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing intermediate container bfdedbdbc58c\n",
      " ---> 4c0b61a8e5a6\n",
      "Step 6/33 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Running in 6fdeb1acf1b5\n",
      "Removing intermediate container 6fdeb1acf1b5\n",
      " ---> a29d440c3512\n",
      "Step 7/33 : ENV PYTHONHASHSEED 0\n",
      " ---> Running in f589f82dd066\n",
      "Removing intermediate container f589f82dd066\n",
      " ---> 44e45fa97bce\n",
      "Step 8/33 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Running in 2e7018cc9cd8\n",
      "Removing intermediate container 2e7018cc9cd8\n",
      " ---> 3a635a1b310f\n",
      "Step 9/33 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Running in c7891304c4d1\n",
      "Removing intermediate container c7891304c4d1\n",
      " ---> 00533a1ded45\n",
      "Step 10/33 : ENV HADOOP_VERSION 3.2.1\n",
      " ---> Running in 802d0b42e56a\n",
      "Removing intermediate container 802d0b42e56a\n",
      " ---> 56e303fe9daa\n",
      "Step 11/33 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Running in d11b038bf94a\n",
      "Removing intermediate container d11b038bf94a\n",
      " ---> bd2df5877dee\n",
      "Step 12/33 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Running in 81bdac5f7f05\n",
      "Removing intermediate container 81bdac5f7f05\n",
      " ---> 6189ad488052\n",
      "Step 13/33 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Running in 4d8560cf32af\n",
      "Removing intermediate container 4d8560cf32af\n",
      " ---> 12a5e364a8ed\n",
      "Step 14/33 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Running in c7216e6fa8c0\n",
      "Removing intermediate container c7216e6fa8c0\n",
      " ---> 98ba1276c7f8\n",
      "Step 15/33 : ENV SPARK_VERSION 2.4.6\n",
      " ---> Running in bb4f26beee46\n",
      "Removing intermediate container bb4f26beee46\n",
      " ---> 64cb230b7a57\n",
      "Step 16/33 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Running in 8f2e0f95c431\n",
      "Removing intermediate container 8f2e0f95c431\n",
      " ---> 1940b57ce3d8\n",
      "Step 17/33 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Running in 5586b37fc41f\n",
      "Removing intermediate container 5586b37fc41f\n",
      " ---> 35f8ea4b7276\n",
      "Step 18/33 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Running in 518ddd55acf4\n",
      "Removing intermediate container 518ddd55acf4\n",
      " ---> 6b6747835e06\n",
      "Step 19/33 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Running in 0137b90927cc\n",
      "Removing intermediate container 0137b90927cc\n",
      " ---> a29a1fcdd319\n",
      "Step 20/33 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Running in 66dca14355c4\n",
      "Removing intermediate container 66dca14355c4\n",
      " ---> bcbfea97284f\n",
      "Step 21/33 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Running in 4d58e3045106\n",
      "Removing intermediate container 4d58e3045106\n",
      " ---> 0ec54dee1cae\n",
      "Step 22/33 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Running in dc6707c00361\n",
      "Removing intermediate container dc6707c00361\n",
      " ---> 33266959a9b5\n",
      "Step 23/33 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Running in 7f60ca8f9cd1\n",
      "Removing intermediate container 7f60ca8f9cd1\n",
      " ---> 83967dfd82da\n",
      "Step 24/33 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Running in 4abac451916b\n",
      "Removing intermediate container 4abac451916b\n",
      " ---> c7bce1cee8ef\n",
      "Step 25/33 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Running in c787b25844ce\n",
      "Removing intermediate container c787b25844ce\n",
      " ---> aa534af44386\n",
      "Step 26/33 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Running in df0f1e8f82e7\n",
      "Removing intermediate container df0f1e8f82e7\n",
      " ---> 41f36fd517f3\n",
      "Step 27/33 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Running in bbbf70997839\n",
      "Removing intermediate container bbbf70997839\n",
      " ---> 8a79798967a5\n",
      "Step 28/33 : COPY program /opt/program\n",
      " ---> a96bd7b13dc8\n",
      "Step 29/33 : RUN chmod +x /opt/program/submit\n",
      " ---> Running in 8131da2aeadc\n",
      "Removing intermediate container 8131da2aeadc\n",
      " ---> 1191da13b726\n",
      "Step 30/33 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> 83c0fcf5d3ab\n",
      "Step 31/33 : COPY jars /usr/jars\n",
      " ---> ad5e078b70f4\n",
      "Step 32/33 : WORKDIR $SPARK_HOME\n",
      " ---> Running in cafdce8eecaf\n",
      "Removing intermediate container cafdce8eecaf\n",
      " ---> f965ad0a531d\n",
      "Step 33/33 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Running in fb6a16c13c3c\n",
      "Removing intermediate container fb6a16c13c3c\n",
      " ---> 94bebe2567ed\n",
      "Successfully built 94bebe2567ed\n",
      "Successfully tagged amazon-reviews-spark-analyzer:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Spark container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393371431575.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ECR repository and push docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryNotFoundException) when calling the DescribeRepositories operation: The repository with name 'amazon-reviews-spark-analyzer' does not exist in the registry with id '393371431575'\n",
      "{\n",
      "    \"repository\": {\n",
      "        \"repositoryArn\": \"arn:aws:ecr:us-west-2:393371431575:repository/amazon-reviews-spark-analyzer\",\n",
      "        \"registryId\": \"393371431575\",\n",
      "        \"repositoryName\": \"amazon-reviews-spark-analyzer\",\n",
      "        \"repositoryUri\": \"393371431575.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer\",\n",
      "        \"createdAt\": 1595702964.0,\n",
      "        \"imageTagMutability\": \"MUTABLE\",\n",
      "        \"imageScanningConfiguration\": {\n",
      "            \"scanOnPush\": false\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [393371431575.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer]\n",
      "\n",
      "\u001b[1B634d3e47: Preparing \n",
      "\u001b[1B6d81b659: Preparing \n",
      "\u001b[1B99a25dd4: Preparing \n",
      "\u001b[1Be84826f8: Preparing \n",
      "\u001b[1B04eaa3b2: Preparing \n",
      "\u001b[1B5d56d659: Preparing \n",
      "\u001b[1B20285432: Preparing \n",
      "\u001b[1B333168eb: Preparing \n",
      "\u001b[1B2c370ca9: Preparing \n",
      "\u001b[1B37da49ee: Preparing \n",
      "\u001b[1Bd076f217: Preparing \n",
      "\u001b[1Bc95dcfbb: Preparing \n",
      "\u001b[1B38d128fe: Preparing \n",
      "\u001b[1Be510e849: Preparing \n",
      "\u001b[6B37da49ee: Pushed   490.3MB/481.8MB\u001b[12A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[15A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[8A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2KPushing  46.14MB/103.6MB\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[4A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2KPushing  241.5MB/481.8MB\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2Klatest: digest: sha256:bc75c26db7fbaab89b11de50edf7dcb70fb90c203276e5335f73836002a0b337 size: 3472\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run our Analysis Job as a SageMaker Processing Job\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built with our Spark script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "from __future__ import unicode_literals\r\n",
      "\r\n",
      "import time\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import shutil\r\n",
      "import csv\r\n",
      "\r\n",
      "import pyspark\r\n",
      "from pyspark.sql import SparkSession\r\n",
      "from pyspark.sql.functions import *\r\n",
      "\r\n",
      "def main():\r\n",
      "    args_iter = iter(sys.argv[1:])\r\n",
      "    args = dict(zip(args_iter, args_iter))\r\n",
      "    \r\n",
      "    # Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\r\n",
      "    s3_input_data = args['s3_input_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_input_data)\r\n",
      "    s3_output_analyze_data = args['s3_output_analyze_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_output_analyze_data)\r\n",
      "    \r\n",
      "    spark = SparkSession.builder \\\r\n",
      "        .appName(\"Amazon_Reviews_Spark_Analyzer\") \\\r\n",
      "        .getOrCreate()\r\n",
      "\r\n",
      "    # Invoke Main from preprocess-deequ.jar\r\n",
      "    getattr(spark._jvm.SparkAmazonReviewsAnalyzer, \"run\")(s3_input_data, s3_output_analyze_data)\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!cat preprocess-deequ.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.runners.\u001b[39;49;00m{\u001b[04m\u001b[32mAnalysisRunner\u001b[39;49;00m, \u001b[04m\u001b[32mAnalyzerContext\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.runners.AnalyzerContext.successMetricsAsDataFrame\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.\u001b[39;49;00m{\u001b[04m\u001b[32mCompliance\u001b[39;49;00m, \u001b[04m\u001b[32mCorrelation\u001b[39;49;00m, \u001b[04m\u001b[32mSize\u001b[39;49;00m, \u001b[04m\u001b[32mCompleteness\u001b[39;49;00m, \u001b[04m\u001b[32mMean\u001b[39;49;00m, \u001b[04m\u001b[32mApproxCountDistinct\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.\u001b[39;49;00m{\u001b[04m\u001b[32mVerificationSuite\u001b[39;49;00m, \u001b[04m\u001b[32mVerificationResult\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.VerificationResult.checkResultsAsDataFrame\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.checks.\u001b[39;49;00m{\u001b[04m\u001b[32mCheck\u001b[39;49;00m, \u001b[04m\u001b[32mCheckLevel\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.suggestions.\u001b[39;49;00m{\u001b[04m\u001b[32mConstraintSuggestionRunner\u001b[39;49;00m, \u001b[04m\u001b[32mRules\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.SaveMode\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.types.\u001b[39;49;00m{\u001b[04m\u001b[32mStructType\u001b[39;49;00m, \u001b[04m\u001b[32mStructField\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mobject\u001b[39;49;00m \u001b[04m\u001b[32mSparkAmazonReviewsAnalyzer\u001b[39;49;00m {\r\n",
      "  \u001b[34mdef\u001b[39;49;00m run(s3InputData\u001b[34m:\u001b[39;49;00m \u001b[36mString\u001b[39;49;00m, s3OutputAnalyzeData\u001b[34m:\u001b[39;49;00m \u001b[36mString\u001b[39;49;00m)\u001b[34m:\u001b[39;49;00m \u001b[36mUnit\u001b[39;49;00m = {\r\n",
      "\r\n",
      "    \u001b[04m\u001b[32mSystem\u001b[39;49;00m.out.println(\u001b[33ms\"\u001b[39;49;00m\u001b[33ms3_input_data: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3InputData\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[04m\u001b[32mSystem\u001b[39;49;00m.out.println(\u001b[33ms\"\u001b[39;49;00m\u001b[33ms3_output_analyze_data: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "      \r\n",
      "    \u001b[34mval\u001b[39;49;00m spark \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mSparkSession\u001b[39;49;00m\r\n",
      "      .builder\r\n",
      "      .appName(\u001b[33m\"SparkAmazonReviewsAnalyzer\"\u001b[39;49;00m)\r\n",
      "      .getOrCreate()\r\n",
      "    \r\n",
      "    \u001b[34mval\u001b[39;49;00m schema \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mStructType\u001b[39;49;00m(\u001b[04m\u001b[32mArray\u001b[39;49;00m(\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"marketplace\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"customer_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_parent\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_title\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_category\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"helpful_votes\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"vine\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"verified_purchase\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_headline\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_body\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_date\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "    ))\r\n",
      "      \r\n",
      "    \u001b[34mval\u001b[39;49;00m dataset \u001b[34m=\u001b[39;49;00m spark.read.option(\u001b[33m\"sep\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "                            .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[33m\"true\"\u001b[39;49;00m)\r\n",
      "                            .option(\u001b[33m\"quote\"\u001b[39;49;00m, \u001b[33m\"\"\u001b[39;49;00m)\r\n",
      "                            .schema(schema)\r\n",
      "                            .csv(s3InputData)\r\n",
      "\r\n",
      "    \u001b[37m// define analyzers that compute metrics\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m analysisResult\u001b[34m:\u001b[39;49;00m \u001b[36mAnalyzerContext\u001b[39;49;00m = { \u001b[04m\u001b[32mAnalysisRunner\u001b[39;49;00m\r\n",
      "          .onData(dataset)\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mSize\u001b[39;49;00m())\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCompleteness\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mApproxCountDistinct\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mMean\u001b[39;49;00m(\u001b[33m\"star_rating\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCompliance\u001b[39;49;00m(\u001b[33m\"top star_rating\"\u001b[39;49;00m, \u001b[33m\"star_rating >= 4.0\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCorrelation\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[33m\"star_rating\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCorrelation\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[33m\"helpful_votes\"\u001b[39;49;00m))\r\n",
      "          \u001b[37m// compute metrics\u001b[39;49;00m\r\n",
      "          .run()\r\n",
      "        }\r\n",
      "\r\n",
      "    \u001b[37m// retrieve successfully computed metrics as a Spark data frame\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m metrics \u001b[34m=\u001b[39;49;00m successMetricsAsDataFrame(spark, analysisResult)\r\n",
      "    metrics.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    metrics\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)      \r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/dataset-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m// define data quality checks,\u001b[39;49;00m\r\n",
      "    \u001b[37m// compute metrics \u001b[39;49;00m\r\n",
      "    \u001b[37m// verify check conditions\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m verificationResult\u001b[34m:\u001b[39;49;00m \u001b[36mVerificationResult\u001b[39;49;00m = { \u001b[04m\u001b[32mVerificationSuite\u001b[39;49;00m()\r\n",
      "          \u001b[37m// data to run the verification on\u001b[39;49;00m\r\n",
      "          .onData(dataset)\r\n",
      "          .addCheck(\r\n",
      "            \u001b[04m\u001b[32mCheck\u001b[39;49;00m(\u001b[04m\u001b[32mCheckLevel\u001b[39;49;00m.\u001b[04m\u001b[32mError\u001b[39;49;00m, \u001b[33m\"Review Check\"\u001b[39;49;00m) \r\n",
      "              .hasSize(\u001b[34m_\u001b[39;49;00m >= \u001b[34m200000\u001b[39;49;00m) \u001b[37m// at least 200.000 rows\u001b[39;49;00m\r\n",
      "              .hasMin(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[34m_\u001b[39;49;00m == \u001b[34m1.0\u001b[39;49;00m) \u001b[37m// min is 1.0\u001b[39;49;00m\r\n",
      "              .hasMax(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[34m_\u001b[39;49;00m == \u001b[34m5.0\u001b[39;49;00m) \u001b[37m// max is 5.0\u001b[39;49;00m\r\n",
      "              .isComplete(\u001b[33m\"review_id\"\u001b[39;49;00m) \u001b[37m// should never be NULL\u001b[39;49;00m\r\n",
      "              .isUnique(\u001b[33m\"review_id\"\u001b[39;49;00m) \u001b[37m// should not contain duplicates\u001b[39;49;00m\r\n",
      "              .isComplete(\u001b[33m\"marketplace\"\u001b[39;49;00m) \u001b[37m// should never be NULL\u001b[39;49;00m\r\n",
      "              .isContainedIn(\u001b[33m\"marketplace\"\u001b[39;49;00m, \u001b[04m\u001b[32mArray\u001b[39;49;00m(\u001b[33m\"US\"\u001b[39;49;00m, \u001b[33m\"UK\"\u001b[39;49;00m, \u001b[33m\"DE\"\u001b[39;49;00m, \u001b[33m\"JP\"\u001b[39;49;00m, \u001b[33m\"FR\"\u001b[39;49;00m)) \r\n",
      "              )\r\n",
      "          .run()\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[37m// convert check results to a Spark data frame\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m resultsDataFrame \u001b[34m=\u001b[39;49;00m checkResultsAsDataFrame(spark, verificationResult)\r\n",
      "    resultsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    resultsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/constraint-checks\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m// generate the success metrics as a dataframe\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m verificationSuccessMetricsDataFrame \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mVerificationResult\u001b[39;49;00m\r\n",
      "      .successMetricsAsDataFrame(spark, verificationResult)\r\n",
      "\r\n",
      "    verificationSuccessMetricsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    verificationSuccessMetricsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/success-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)      \r\n",
      "\r\n",
      "    \u001b[37m// We ask deequ to compute constraint suggestions for us on the data\u001b[39;49;00m\r\n",
      "    \u001b[37m// using a default set of rules for constraint suggestion\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m suggestionsResult \u001b[34m=\u001b[39;49;00m { \u001b[04m\u001b[32mConstraintSuggestionRunner\u001b[39;49;00m()\r\n",
      "          .onData(dataset)\r\n",
      "          .addConstraintRules(\u001b[04m\u001b[32mRules\u001b[39;49;00m.\u001b[04m\u001b[32mDEFAULT\u001b[39;49;00m)\r\n",
      "          .run()\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mspark.implicits._\u001b[39;49;00m \u001b[37m// for toDS method below\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m// We can now investigate the constraints that Deequ suggested. \u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m suggestionsDataFrame \u001b[34m=\u001b[39;49;00m suggestionsResult.constraintSuggestions.flatMap { \r\n",
      "          \u001b[34mcase\u001b[39;49;00m (column, suggestions) \u001b[34m=>\u001b[39;49;00m \r\n",
      "            suggestions.map { constraint \u001b[34m=>\u001b[39;49;00m\r\n",
      "              (column, constraint.description, constraint.codeForConstraint)\r\n",
      "            } \r\n",
      "    }.toSeq.toDS()\r\n",
      "      \r\n",
      "    suggestionsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    suggestionsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)      \r\n",
      "      .write      \r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)  \r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/constraint-suggestions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)      \r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize deequ/preprocess-deequ.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-analyzer',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.2xlarge',\n",
    "                            env={\n",
    "                                'mode': 'jar',\n",
    "                                'main_class': 'Main'\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-393371431575/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 17:13:26   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-07-25 17:13:29   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  amazon-reviews-spark-analyzer-2020-07-25-18-49-52\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "processing_job_name = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "\n",
    "print('Processing job name:  {}'.format(processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_analyze_data = 's3://{}/{}/output'.format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_analyze_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark Processing Job\n",
    "\n",
    "_Notes on Invoking from Lambda:_\n",
    "* However, if we use the boto3 SDK (ie. with a Lambda), we need to copy the `preprocess.py` file to S3 and specify the everything include --py-files, etc.\n",
    "* We would need to do the following before invoking the Lambda:\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/code/preprocess.py\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/py_files/preprocess.py\n",
    "* Then reference the s3://<location> above in the --py-files, etc.\n",
    "* See Lambda example code in this same project for more details.\n",
    "\n",
    "_Notes on not using ProcessingInput and Output:_\n",
    "* Since Spark natively reads/writes from/to S3 using s3a://, we can avoid the copy required by ProcessingInput and ProcessingOutput (FullyReplicated or ShardedByS3Key) and just specify the S3 input and output buckets/prefixes._\"\n",
    "* See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "* If we use ProcessingInput, the data will be copied to each node (which we don't want in this case since Spark already handles this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-deequ.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_analyze_data', s3_output_analyze_data,\n",
    "              ],\n",
    "              # See https://github.com/aws/sagemaker-python-sdk/issues/1341 for why we need to specify a dummy-output\n",
    "              outputs=[\n",
    "                  ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                   output_name='dummy-output',\n",
    "                                   source='/opt/ml/processing/output')\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, processing_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "s3_job_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, s3_job_output_prefix, region)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-393371431575/spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '393371431575.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-deequ.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-west-2-393371431575/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output']}, 'Environment': {'main_class': 'Main', 'mode': 'jar'}, 'RoleArn': 'arn:aws:iam::393371431575:role/TeamRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:393371431575:processing-job/spark-amazon-reviews-analyzer-2020-07-25-18-49-52-819', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 7, 25, 18, 49, 53, 361000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 7, 25, 18, 49, 53, 361000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '0a28de8b-4be7-400d-a7a5-1b2cd866724f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0a28de8b-4be7-400d-a7a5-1b2cd866724f', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1628', 'date': 'Sat, 25 Jul 2020 18:49:53 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:05,581 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.68.27\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.2.1/etc/hadoop:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.375.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-aws-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/yarn:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop\u001b[0m\n",
      "\u001b[34m-yarn-server-nodemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_262\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:05,587 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:05,658 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-d5f432f8-6dc0-47ad-9f27-6a461043557a\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,002 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,015 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,016 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,016 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,020 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,020 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,020 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,021 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,061 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,071 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,071 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,076 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,079 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Jul 25 18:53:06\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,080 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,080 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,081 INFO util.GSet: 2.0% max memory 13.7 GB = 280.5 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,081 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,155 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,155 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,161 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,162 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,162 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,162 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,162 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,162 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,162 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,162 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,162 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,162 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,163 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,185 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,185 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,185 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,185 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,196 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,197 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,197 INFO util.GSet: 1.0% max memory 13.7 GB = 140.2 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,197 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,427 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,427 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,427 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,427 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,431 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,433 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,436 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,436 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,437 INFO util.GSet: 0.25% max memory 13.7 GB = 35.1 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,437 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,444 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,444 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,444 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,447 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,447 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,448 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,448 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,448 INFO util.GSet: 0.029999999329447746% max memory 13.7 GB = 4.2 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,449 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,468 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1749496151-10.0.68.27-1595703186462\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,479 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,501 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,591 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,602 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,605 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:06,605 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.68.27\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-0-68-27.us-west-2.compute.internal]\u001b[0m\n",
      "\u001b[34mip-10-0-68-27.us-west-2.compute.internal: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:17,579 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-393371431575/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,127 INFO spark.SparkContext: Running Spark version 2.4.6\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,146 INFO spark.SparkContext: Submitted application: Amazon_Reviews_Spark_Analyzer\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,183 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,183 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,183 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,183 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,183 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,407 INFO util.Utils: Successfully started service 'sparkDriver' on port 44687.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,424 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,435 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,437 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,437 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,443 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-0eb840ce-aca1-45eb-aeae-4dfd559bfbbd\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,455 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,493 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,548 INFO util.log: Logging initialized @1865ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,593 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,604 INFO server.Server: Started @1922ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,616 INFO server.AbstractConnector: Started ServerConnector@74f1bae5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,616 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,634 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5194e618{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,635 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9aaf373{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,635 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48e9e006{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,637 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f0ce844{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,638 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18e2d273{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,638 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49f9c315{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,639 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15155060{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,640 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64ba74c6{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,641 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b13ae61{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,641 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28e100f9{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,642 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2caf354c{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,642 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b84685b{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,643 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5da2adec{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,644 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d648869{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,644 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2834d7dc{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,645 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10caa431{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,645 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@584fa609{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,646 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1314aa9{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,646 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f35f764{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,647 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a9b219f{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,651 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fc1a92b{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,652 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1506ea8d{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,653 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ed22c4c{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,654 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c608af4{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,654 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e0a828f{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:18,655 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.68.27:4040\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,218 INFO client.RMProxy: Connecting to ResourceManager at /10.0.68.27:8032\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,490 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,549 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,549 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,563 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (63625 MB per container)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,564 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,564 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,567 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,571 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:19,614 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:20,452 INFO yarn.Client: Uploading resource file:/tmp/spark-9b0df38c-1ac5-4c62-911d-afd941d13331/__spark_libs__7378937716927676928.zip -> hdfs://10.0.68.27/user/root/.sparkStaging/application_1595703196239_0001/__spark_libs__7378937716927676928.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:20,743 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,262 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,423 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/pyspark.zip -> hdfs://10.0.68.27/user/root/.sparkStaging/application_1595703196239_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,430 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,446 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.68.27/user/root/.sparkStaging/application_1595703196239_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,453 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,561 INFO yarn.Client: Uploading resource file:/tmp/spark-9b0df38c-1ac5-4c62-911d-afd941d13331/__spark_conf__706207047212413484.zip -> hdfs://10.0.68.27/user/root/.sparkStaging/application_1595703196239_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,568 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,596 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,596 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,596 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,596 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:21,596 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:22,323 INFO yarn.Client: Submitting application application_1595703196239_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:22,512 INFO impl.YarnClientImpl: Submitted application application_1595703196239_0001\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:22,514 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1595703196239_0001 and attemptId None\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 18:53:23,519 INFO yarn.Client: Application report for application_1595703196239_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:23,521 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1595703202412\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1595703196239_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:24,523 INFO yarn.Client: Application report for application_1595703196239_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:25,526 INFO yarn.Client: Application report for application_1595703196239_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:26,528 INFO yarn.Client: Application report for application_1595703196239_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:26,730 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1595703196239_0001), /proxy/application_1595703196239_0001\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:26,885 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,531 INFO yarn.Client: Application report for application_1595703196239_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,531 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.107.56\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1595703202412\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1595703196239_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,532 INFO cluster.YarnClientSchedulerBackend: Application application_1595703196239_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,537 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42927.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,538 INFO netty.NettyBlockTransferService: Server created on 10.0.68.27:42927\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,538 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,575 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.68.27, 42927, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,577 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.68.27:42927 with 366.3 MB RAM, BlockManagerId(driver, 10.0.68.27, 42927, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,579 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.68.27, 42927, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,579 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.68.27, 42927, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,674 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:27,679 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41935c0e{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:30,328 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.107.56:47616) with ID 1\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:30,412 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:36157 with 24.1 GB RAM, BlockManagerId(1, algo-2, 36157, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,709 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,895 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.4.6/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,895 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.4.6/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,900 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,901 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39865ef7{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,901 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,902 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62a003c3{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,902 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,903 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e9e8725{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,903 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,903 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5385de72{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,904 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:48,904 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@94b98cf{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:49,203 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34ms3_input_data: s3a://sagemaker-us-west-2-393371431575/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3_output_analyze_data: s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:49,216 WARN sql.SparkSession$Builder: Using an existing SparkSession; some spark core configurations may not take effect.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:49,344 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:49,387 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:49,387 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:50,564 INFO datasources.InMemoryFileIndex: It took 94 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:51,485 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:51,487 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:51,489 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string, star_rating: int, helpful_votes: int, total_votes: int ... 2 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:51,494 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:51,540 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:51,575 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:51,887 INFO codegen.CodeGenerator: Code generated in 206.167162 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,027 INFO codegen.CodeGenerator: Code generated in 9.498514 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,068 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 400.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,109 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,111 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.68.27:42927 (size: 42.8 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,113 INFO spark.SparkContext: Created broadcast 0 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,130 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,332 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,345 INFO scheduler.DAGScheduler: Registering RDD 3 (collect at AnalysisRunner.scala:303) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,346 INFO scheduler.DAGScheduler: Got job 0 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,347 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,347 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,348 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,351 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,365 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,367 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.6 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,368 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.68.27:42927 (size: 13.6 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,369 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,380 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,380 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,519 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,521 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:52,681 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:36157 (size: 13.6 KB, free: 24.1 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 18:53:53,303 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:36157 (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,423 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3902 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,777 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4379 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,778 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,779 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (collect at AnalysisRunner.scala:303) finished in 4.418 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,780 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,780 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,780 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,780 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,782 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,788 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 36.9 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,790 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,790 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.68.27:42927 (size: 15.3 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,790 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,791 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,791 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,795 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,821 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:36157 (size: 15.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,842 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,996 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 202 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,996 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:56,997 INFO scheduler.DAGScheduler: ResultStage 1 (collect at AnalysisRunner.scala:303) finished in 0.211 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,000 INFO scheduler.DAGScheduler: Job 0 finished: collect at AnalysisRunner.scala:303, took 4.667843 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,149 INFO codegen.CodeGenerator: Code generated in 20.019564 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,205 INFO codegen.CodeGenerator: Code generated in 8.315253 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,217 INFO codegen.CodeGenerator: Code generated in 8.83521 ms\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+-------------------+\u001b[0m\n",
      "\u001b[34m|entity     |instance                 |name               |value              |\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+-------------------+\u001b[0m\n",
      "\u001b[34m|Column     |review_id                |Completeness       |1.0                |\u001b[0m\n",
      "\u001b[34m|Column     |review_id                |ApproxCountDistinct|238027.0           |\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,star_rating  |Correlation        |-0.0808806564857777|\u001b[0m\n",
      "\u001b[34m|Dataset    |*                        |Size               |247515.0           |\u001b[0m\n",
      "\u001b[34m|Column     |star_rating              |Mean               |3.7237056340019796 |\u001b[0m\n",
      "\u001b[34m|Column     |top star_rating          |Compliance         |0.6633375755004747 |\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,helpful_votes|Correlation        |0.9805294402834748 |\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+-------------------+\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,346 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,347 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,347 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,348 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200725185357_0000}; taskId=attempt_20200725185357_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@e00c7ce}; outputPath=s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/dataset-metrics, workPath=s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/dataset-metrics/_temporary/0/_temporary/attempt_20200725185357_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/dataset-metrics\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,348 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,854 INFO codegen.CodeGenerator: Code generated in 7.87534 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,889 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:77\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,890 INFO scheduler.DAGScheduler: Registering RDD 9 (csv at preprocess-deequ.scala:77) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,890 INFO scheduler.DAGScheduler: Got job 1 (csv at preprocess-deequ.scala:77) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,890 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (csv at preprocess-deequ.scala:77)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,890 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,890 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,891 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at csv at preprocess-deequ.scala:77), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,896 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.1 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,897 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.0 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,898 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.68.27:42927 (size: 3.0 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,898 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,898 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at csv at preprocess-deequ.scala:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,899 INFO cluster.YarnScheduler: Adding task set 2.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,901 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,901 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8116 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,901 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8205 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,902 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,902 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 7, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8229 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,914 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:36157 (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,926 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 25 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,926 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 25 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,927 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 28 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,928 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 7) in 26 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,928 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 26 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,928 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,929 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (csv at preprocess-deequ.scala:77) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,929 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,929 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,929 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,929 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,929 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (ShuffledRowRDD[10] at csv at preprocess-deequ.scala:77), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,951 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 245.1 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,953 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 90.4 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,953 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.68.27:42927 (size: 90.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,954 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,954 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (ShuffledRowRDD[10] at csv at preprocess-deequ.scala:77) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,954 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,955 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,963 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:36157 (size: 90.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:57,990 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:58,999 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 1044 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,000 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,000 INFO scheduler.DAGScheduler: ResultStage 3 (csv at preprocess-deequ.scala:77) finished in 1.070 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,001 INFO scheduler.DAGScheduler: Job 1 finished: csv at preprocess-deequ.scala:77, took 1.111994 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,427 INFO datasources.FileFormatWriter: Write Job 4645bc91-2706-46a3-8d03-815c2363d907 committed.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,431 INFO datasources.FileFormatWriter: Finished processing stats for write job 4645bc91-2706-46a3-8d03-815c2363d907.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,557 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,558 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,558 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, review_id: string, star_rating: int ... 1 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,558 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,583 INFO codegen.CodeGenerator: Code generated in 10.776639 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,634 INFO codegen.CodeGenerator: Code generated in 23.482605 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,669 INFO codegen.CodeGenerator: Code generated in 22.299508 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,676 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 400.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,690 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,691 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.68.27:42927 (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,691 INFO spark.SparkContext: Created broadcast 5 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,692 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,704 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,705 INFO scheduler.DAGScheduler: Registering RDD 15 (collect at AnalysisRunner.scala:303) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,705 INFO scheduler.DAGScheduler: Got job 2 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,705 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,705 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,705 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,706 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[15] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,710 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.4 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,712 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.3 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,712 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.68.27:42927 (size: 9.3 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,712 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,713 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[15] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,713 INFO cluster.YarnScheduler: Adding task set 4.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,714 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 9, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,714 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 10, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,724 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:36157 (size: 9.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:53:59,765 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:36157 (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:00,834 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 10) in 1120 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,177 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 9) in 1463 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,177 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,178 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (collect at AnalysisRunner.scala:303) finished in 1.471 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,178 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,178 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,178 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,178 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,178 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,180 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.7 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,181 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,181 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.68.27:42927 (size: 5.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,182 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,182 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,182 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,183 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 11, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,193 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:36157 (size: 5.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,196 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,221 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 11) in 38 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,221 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,222 INFO scheduler.DAGScheduler: ResultStage 5 (collect at AnalysisRunner.scala:303) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,222 INFO scheduler.DAGScheduler: Job 2 finished: collect at AnalysisRunner.scala:303, took 1.518094 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,266 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,267 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,267 INFO datasources.FileSourceStrategy: Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,267 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,283 INFO codegen.CodeGenerator: Code generated in 9.407964 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,294 INFO codegen.CodeGenerator: Code generated in 8.60557 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,299 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 400.9 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,316 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,321 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,325 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.68.27:42927 (size: 42.8 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,326 INFO spark.SparkContext: Created broadcast 8 from count at GroupingAnalyzers.scala:76\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,326 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,326 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.68.27:42927 in memory (size: 9.3 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,330 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:36157 in memory (size: 9.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.SparkContext: Starting job: count at GroupingAnalyzers.scala:76\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,339 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,340 INFO scheduler.DAGScheduler: Registering RDD 21 (count at GroupingAnalyzers.scala:76) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,340 INFO scheduler.DAGScheduler: Got job 3 (count at GroupingAnalyzers.scala:76) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,340 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (count at GroupingAnalyzers.scala:76)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,340 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,340 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 6)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,340 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[21] at count at GroupingAnalyzers.scala:76), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.0 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned shuffle 1\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,343 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,344 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.9 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned shuffle 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.68.27:42927 (size: 6.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,347 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[21] at count at GroupingAnalyzers.scala:76) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO cluster.YarnScheduler: Adding task set 6.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,348 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,349 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,349 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,352 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:36157 in memory (size: 5.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,357 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.68.27:42927 in memory (size: 5.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,363 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:36157 (size: 6.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,370 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,375 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:36157 in memory (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,376 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.68.27:42927 in memory (size: 3.0 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,384 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:36157 (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,385 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,388 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:36157 in memory (size: 15.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,388 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.68.27:42927 in memory (size: 15.3 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,398 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,398 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,398 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,398 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,398 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,398 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,398 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,398 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,398 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,399 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.68.27:42927 in memory (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,401 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:36157 in memory (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,404 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,404 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,404 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,408 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.68.27:42927 in memory (size: 90.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,409 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:36157 in memory (size: 90.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,423 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:01,994 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 13) in 645 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,223 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 874 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,223 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,224 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (count at GroupingAnalyzers.scala:76) finished in 0.882 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,224 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,224 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,224 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 7)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,224 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,224 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[24] at count at GroupingAnalyzers.scala:76), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,226 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.3 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,227 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.9 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,228 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.68.27:42927 (size: 3.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,228 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,228 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[24] at count at GroupingAnalyzers.scala:76) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,228 INFO cluster.YarnScheduler: Adding task set 7.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,229 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,236 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:36157 (size: 3.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,238 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,254 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 14) in 25 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,254 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,254 INFO scheduler.DAGScheduler: ResultStage 7 (count at GroupingAnalyzers.scala:76) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,255 INFO scheduler.DAGScheduler: Job 3 finished: count at GroupingAnalyzers.scala:76, took 0.915795 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,344 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,344 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(review_id#2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,344 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,346 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(review_id)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,363 INFO codegen.CodeGenerator: Code generated in 5.038816 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,374 INFO codegen.CodeGenerator: Code generated in 7.748844 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,403 INFO codegen.CodeGenerator: Code generated in 17.761594 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,444 INFO codegen.CodeGenerator: Code generated in 25.573457 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,448 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 400.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,459 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,459 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.68.27:42927 (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,460 INFO spark.SparkContext: Created broadcast 11 from collect at AnalysisRunner.scala:499\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,461 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,485 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:499\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,486 INFO scheduler.DAGScheduler: Registering RDD 27 (collect at AnalysisRunner.scala:499) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,486 INFO scheduler.DAGScheduler: Registering RDD 30 (collect at AnalysisRunner.scala:499) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,486 INFO scheduler.DAGScheduler: Got job 4 (collect at AnalysisRunner.scala:499) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,486 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (collect at AnalysisRunner.scala:499)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,486 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,487 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,487 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:499), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,490 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 27.2 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,491 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 12.7 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,491 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.68.27:42927 (size: 12.7 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,492 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,492 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:499) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,492 INFO cluster.YarnScheduler: Adding task set 8.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,493 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,493 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 16, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,505 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:36157 (size: 12.7 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:02,621 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:36157 (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 18:54:03,672 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 16) in 1179 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,075 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 1582 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,075 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,075 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (collect at AnalysisRunner.scala:499) finished in 1.587 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,075 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,076 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,076 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 9, ResultStage 10)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,076 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,076 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[30] at collect at AnalysisRunner.scala:499), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,091 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 29.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,092 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.1 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,093 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.68.27:42927 (size: 14.1 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,093 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,095 INFO scheduler.DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[30] at collect at AnalysisRunner.scala:499) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,095 INFO cluster.YarnScheduler: Adding task set 9.0 with 200 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,099 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 17, algo-2, executor 1, partition 0, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,099 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 18, algo-2, executor 1, partition 1, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,100 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 19, algo-2, executor 1, partition 2, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,100 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 9.0 (TID 20, algo-2, executor 1, partition 3, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,100 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 9.0 (TID 21, algo-2, executor 1, partition 4, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,108 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:36157 (size: 14.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,122 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,169 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 9.0 (TID 22, algo-2, executor 1, partition 5, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,169 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 9.0 (TID 21) in 69 ms on algo-2 (executor 1) (1/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,170 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 9.0 (TID 23, algo-2, executor 1, partition 6, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,170 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 19) in 70 ms on algo-2 (executor 1) (2/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,188 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 9.0 (TID 24, algo-2, executor 1, partition 7, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,189 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 9.0 (TID 20) in 89 ms on algo-2 (executor 1) (3/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,189 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 9.0 (TID 25, algo-2, executor 1, partition 8, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,190 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 18) in 91 ms on algo-2 (executor 1) (4/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,191 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 9.0 (TID 26, algo-2, executor 1, partition 9, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,192 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 17) in 93 ms on algo-2 (executor 1) (5/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,208 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 9.0 (TID 27, algo-2, executor 1, partition 10, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,209 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 9.0 (TID 28, algo-2, executor 1, partition 11, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,209 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 9.0 (TID 22) in 40 ms on algo-2 (executor 1) (6/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,209 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 9.0 (TID 25) in 20 ms on algo-2 (executor 1) (7/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,209 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 9.0 (TID 29, algo-2, executor 1, partition 12, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,210 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 9.0 (TID 26) in 19 ms on algo-2 (executor 1) (8/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,212 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 9.0 (TID 30, algo-2, executor 1, partition 13, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,213 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 9.0 (TID 23) in 43 ms on algo-2 (executor 1) (9/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,213 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 9.0 (TID 31, algo-2, executor 1, partition 14, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,214 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 9.0 (TID 24) in 26 ms on algo-2 (executor 1) (10/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,224 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 9.0 (TID 32, algo-2, executor 1, partition 15, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,224 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 9.0 (TID 27) in 16 ms on algo-2 (executor 1) (11/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,235 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 9.0 (TID 33, algo-2, executor 1, partition 16, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,235 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 9.0 (TID 29) in 26 ms on algo-2 (executor 1) (12/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,236 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 9.0 (TID 34, algo-2, executor 1, partition 17, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,236 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 9.0 (TID 31) in 23 ms on algo-2 (executor 1) (13/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,237 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 9.0 (TID 35, algo-2, executor 1, partition 18, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,237 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 9.0 (TID 28) in 29 ms on algo-2 (executor 1) (14/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,238 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 9.0 (TID 36, algo-2, executor 1, partition 19, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,238 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 9.0 (TID 30) in 26 ms on algo-2 (executor 1) (15/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,238 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 9.0 (TID 37, algo-2, executor 1, partition 20, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,239 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 9.0 (TID 32) in 15 ms on algo-2 (executor 1) (16/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,249 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 9.0 (TID 38, algo-2, executor 1, partition 21, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,250 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 9.0 (TID 34) in 14 ms on algo-2 (executor 1) (17/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,251 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 9.0 (TID 39, algo-2, executor 1, partition 22, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,252 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 9.0 (TID 33) in 17 ms on algo-2 (executor 1) (18/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,254 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 9.0 (TID 40, algo-2, executor 1, partition 23, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,255 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 9.0 (TID 36) in 18 ms on algo-2 (executor 1) (19/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,257 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 9.0 (TID 41, algo-2, executor 1, partition 24, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,257 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 9.0 (TID 37) in 19 ms on algo-2 (executor 1) (20/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,260 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 9.0 (TID 42, algo-2, executor 1, partition 25, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,260 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 9.0 (TID 35) in 23 ms on algo-2 (executor 1) (21/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,266 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 9.0 (TID 43, algo-2, executor 1, partition 26, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,266 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 9.0 (TID 38) in 17 ms on algo-2 (executor 1) (22/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,268 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 9.0 (TID 44, algo-2, executor 1, partition 27, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,268 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 9.0 (TID 39) in 17 ms on algo-2 (executor 1) (23/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,273 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 9.0 (TID 45, algo-2, executor 1, partition 28, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,273 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 9.0 (TID 40) in 19 ms on algo-2 (executor 1) (24/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,273 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 9.0 (TID 46, algo-2, executor 1, partition 29, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,274 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 9.0 (TID 42) in 15 ms on algo-2 (executor 1) (25/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,279 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 9.0 (TID 47, algo-2, executor 1, partition 30, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,279 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 9.0 (TID 43) in 13 ms on algo-2 (executor 1) (26/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,282 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 9.0 (TID 48, algo-2, executor 1, partition 31, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,282 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 9.0 (TID 44) in 15 ms on algo-2 (executor 1) (27/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,283 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 9.0 (TID 49, algo-2, executor 1, partition 32, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,283 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 9.0 (TID 41) in 27 ms on algo-2 (executor 1) (28/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,286 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 9.0 (TID 50, algo-2, executor 1, partition 33, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,286 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 9.0 (TID 46) in 13 ms on algo-2 (executor 1) (29/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,292 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 9.0 (TID 51, algo-2, executor 1, partition 34, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,292 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 9.0 (TID 47) in 13 ms on algo-2 (executor 1) (30/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,297 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 9.0 (TID 52, algo-2, executor 1, partition 35, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,297 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 9.0 (TID 49) in 15 ms on algo-2 (executor 1) (31/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,298 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 9.0 (TID 53, algo-2, executor 1, partition 36, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,298 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 9.0 (TID 48) in 16 ms on algo-2 (executor 1) (32/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,299 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 9.0 (TID 54, algo-2, executor 1, partition 37, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,299 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 9.0 (TID 50) in 13 ms on algo-2 (executor 1) (33/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,301 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 9.0 (TID 55, algo-2, executor 1, partition 38, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,302 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 9.0 (TID 45) in 30 ms on algo-2 (executor 1) (34/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,307 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 9.0 (TID 56, algo-2, executor 1, partition 39, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,307 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 9.0 (TID 51) in 15 ms on algo-2 (executor 1) (35/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,312 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 9.0 (TID 57, algo-2, executor 1, partition 40, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,312 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 9.0 (TID 52) in 15 ms on algo-2 (executor 1) (36/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,314 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 9.0 (TID 58, algo-2, executor 1, partition 41, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,314 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 9.0 (TID 53) in 16 ms on algo-2 (executor 1) (37/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,315 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 9.0 (TID 59, algo-2, executor 1, partition 42, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,316 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 9.0 (TID 60, algo-2, executor 1, partition 43, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,316 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 9.0 (TID 55) in 15 ms on algo-2 (executor 1) (38/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,316 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 9.0 (TID 54) in 17 ms on algo-2 (executor 1) (39/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,320 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 9.0 (TID 61, algo-2, executor 1, partition 44, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,321 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 9.0 (TID 56) in 14 ms on algo-2 (executor 1) (40/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,324 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 9.0 (TID 62, algo-2, executor 1, partition 45, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,325 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 9.0 (TID 57) in 13 ms on algo-2 (executor 1) (41/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,327 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 9.0 (TID 63, algo-2, executor 1, partition 46, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,327 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 9.0 (TID 58) in 13 ms on algo-2 (executor 1) (42/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,331 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 9.0 (TID 64, algo-2, executor 1, partition 47, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,331 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 9.0 (TID 59) in 16 ms on algo-2 (executor 1) (43/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,332 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 9.0 (TID 65, algo-2, executor 1, partition 48, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,332 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 9.0 (TID 60) in 17 ms on algo-2 (executor 1) (44/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,338 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 9.0 (TID 66, algo-2, executor 1, partition 49, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,338 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 9.0 (TID 61) in 18 ms on algo-2 (executor 1) (45/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,341 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 9.0 (TID 67, algo-2, executor 1, partition 50, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,341 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 9.0 (TID 63) in 14 ms on algo-2 (executor 1) (46/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,343 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 9.0 (TID 68, algo-2, executor 1, partition 51, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,344 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 9.0 (TID 64) in 13 ms on algo-2 (executor 1) (47/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,344 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 9.0 (TID 69, algo-2, executor 1, partition 52, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,345 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 9.0 (TID 62) in 21 ms on algo-2 (executor 1) (48/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,348 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 9.0 (TID 70, algo-2, executor 1, partition 53, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,348 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 9.0 (TID 65) in 16 ms on algo-2 (executor 1) (49/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,350 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 9.0 (TID 71, algo-2, executor 1, partition 54, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,351 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 9.0 (TID 66) in 13 ms on algo-2 (executor 1) (50/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,356 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 9.0 (TID 72, algo-2, executor 1, partition 55, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,357 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 9.0 (TID 68) in 14 ms on algo-2 (executor 1) (51/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,358 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 9.0 (TID 73, algo-2, executor 1, partition 56, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,358 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 9.0 (TID 69) in 14 ms on algo-2 (executor 1) (52/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,362 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 9.0 (TID 74, algo-2, executor 1, partition 57, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,362 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 9.0 (TID 67) in 22 ms on algo-2 (executor 1) (53/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,363 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 9.0 (TID 75, algo-2, executor 1, partition 58, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,363 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 9.0 (TID 70) in 15 ms on algo-2 (executor 1) (54/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,365 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 9.0 (TID 76, algo-2, executor 1, partition 59, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,365 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 9.0 (TID 71) in 15 ms on algo-2 (executor 1) (55/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,368 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 9.0 (TID 77, algo-2, executor 1, partition 60, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,368 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 9.0 (TID 72) in 12 ms on algo-2 (executor 1) (56/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,378 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 9.0 (TID 78, algo-2, executor 1, partition 61, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,378 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 9.0 (TID 74) in 16 ms on algo-2 (executor 1) (57/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,379 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 9.0 (TID 79, algo-2, executor 1, partition 62, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,379 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 9.0 (TID 73) in 21 ms on algo-2 (executor 1) (58/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,380 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 9.0 (TID 80, algo-2, executor 1, partition 63, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,380 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 9.0 (TID 76) in 15 ms on algo-2 (executor 1) (59/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,380 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 9.0 (TID 81, algo-2, executor 1, partition 64, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,381 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 9.0 (TID 75) in 19 ms on algo-2 (executor 1) (60/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,388 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 9.0 (TID 82, algo-2, executor 1, partition 65, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,388 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 9.0 (TID 77) in 20 ms on algo-2 (executor 1) (61/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,392 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 9.0 (TID 83, algo-2, executor 1, partition 66, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,392 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 9.0 (TID 78) in 14 ms on algo-2 (executor 1) (62/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,399 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 9.0 (TID 84, algo-2, executor 1, partition 67, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,399 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 9.0 (TID 80) in 19 ms on algo-2 (executor 1) (63/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,402 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 9.0 (TID 85, algo-2, executor 1, partition 68, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,402 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 9.0 (TID 81) in 22 ms on algo-2 (executor 1) (64/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,404 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 9.0 (TID 86, algo-2, executor 1, partition 69, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,404 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 9.0 (TID 87, algo-2, executor 1, partition 70, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,404 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 9.0 (TID 79) in 25 ms on algo-2 (executor 1) (65/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,404 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 9.0 (TID 82) in 17 ms on algo-2 (executor 1) (66/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,405 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 9.0 (TID 88, algo-2, executor 1, partition 71, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,406 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 9.0 (TID 83) in 14 ms on algo-2 (executor 1) (67/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,418 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 9.0 (TID 89, algo-2, executor 1, partition 72, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,418 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 9.0 (TID 84) in 19 ms on algo-2 (executor 1) (68/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,419 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 9.0 (TID 90, algo-2, executor 1, partition 73, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,419 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 9.0 (TID 85) in 17 ms on algo-2 (executor 1) (69/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,422 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 9.0 (TID 91, algo-2, executor 1, partition 74, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,422 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 9.0 (TID 86) in 19 ms on algo-2 (executor 1) (70/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,422 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 9.0 (TID 92, algo-2, executor 1, partition 75, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,423 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 9.0 (TID 88) in 18 ms on algo-2 (executor 1) (71/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,429 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 9.0 (TID 93, algo-2, executor 1, partition 76, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,429 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 9.0 (TID 87) in 25 ms on algo-2 (executor 1) (72/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,433 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 9.0 (TID 94, algo-2, executor 1, partition 77, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,433 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 9.0 (TID 90) in 14 ms on algo-2 (executor 1) (73/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,434 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 9.0 (TID 95, algo-2, executor 1, partition 78, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,434 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 9.0 (TID 91) in 12 ms on algo-2 (executor 1) (74/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,437 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 9.0 (TID 96, algo-2, executor 1, partition 79, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,438 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 9.0 (TID 89) in 19 ms on algo-2 (executor 1) (75/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,438 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 9.0 (TID 97, algo-2, executor 1, partition 80, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,438 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 9.0 (TID 92) in 16 ms on algo-2 (executor 1) (76/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,441 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 9.0 (TID 98, algo-2, executor 1, partition 81, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,441 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 9.0 (TID 93) in 12 ms on algo-2 (executor 1) (77/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,448 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 9.0 (TID 99, algo-2, executor 1, partition 82, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,448 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 9.0 (TID 94) in 15 ms on algo-2 (executor 1) (78/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,449 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 9.0 (TID 100, algo-2, executor 1, partition 83, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,450 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 9.0 (TID 96) in 13 ms on algo-2 (executor 1) (79/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,455 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 9.0 (TID 101, algo-2, executor 1, partition 84, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,455 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 9.0 (TID 97) in 17 ms on algo-2 (executor 1) (80/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,455 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 9.0 (TID 102, algo-2, executor 1, partition 85, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,456 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 9.0 (TID 98) in 16 ms on algo-2 (executor 1) (81/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,461 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 9.0 (TID 103, algo-2, executor 1, partition 86, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,461 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 9.0 (TID 99) in 13 ms on algo-2 (executor 1) (82/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,463 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 9.0 (TID 104, algo-2, executor 1, partition 87, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,463 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 9.0 (TID 95) in 29 ms on algo-2 (executor 1) (83/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,463 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 9.0 (TID 105, algo-2, executor 1, partition 88, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,464 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 9.0 (TID 100) in 15 ms on algo-2 (executor 1) (84/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,466 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 9.0 (TID 106, algo-2, executor 1, partition 89, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,466 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 9.0 (TID 101) in 12 ms on algo-2 (executor 1) (85/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,470 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 9.0 (TID 107, algo-2, executor 1, partition 90, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,470 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 9.0 (TID 102) in 15 ms on algo-2 (executor 1) (86/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,476 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 9.0 (TID 108, algo-2, executor 1, partition 91, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,476 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 9.0 (TID 104) in 13 ms on algo-2 (executor 1) (87/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,476 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 9.0 (TID 109, algo-2, executor 1, partition 92, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,477 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 9.0 (TID 103) in 16 ms on algo-2 (executor 1) (88/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,480 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 9.0 (TID 110, algo-2, executor 1, partition 93, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,480 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 9.0 (TID 106) in 14 ms on algo-2 (executor 1) (89/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,481 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 9.0 (TID 111, algo-2, executor 1, partition 94, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,481 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 9.0 (TID 105) in 18 ms on algo-2 (executor 1) (90/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,485 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 9.0 (TID 112, algo-2, executor 1, partition 95, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,486 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 9.0 (TID 107) in 16 ms on algo-2 (executor 1) (91/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,491 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 9.0 (TID 113, algo-2, executor 1, partition 96, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,491 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 9.0 (TID 108) in 16 ms on algo-2 (executor 1) (92/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,493 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 9.0 (TID 114, algo-2, executor 1, partition 97, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,493 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 9.0 (TID 109) in 17 ms on algo-2 (executor 1) (93/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,494 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 9.0 (TID 115, algo-2, executor 1, partition 98, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,495 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 9.0 (TID 110) in 15 ms on algo-2 (executor 1) (94/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,495 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 9.0 (TID 116, algo-2, executor 1, partition 99, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,496 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 9.0 (TID 111) in 16 ms on algo-2 (executor 1) (95/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,499 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 9.0 (TID 117, algo-2, executor 1, partition 100, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,499 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 9.0 (TID 112) in 14 ms on algo-2 (executor 1) (96/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,507 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 9.0 (TID 118, algo-2, executor 1, partition 101, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,508 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 9.0 (TID 114) in 16 ms on algo-2 (executor 1) (97/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,510 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 9.0 (TID 119, algo-2, executor 1, partition 102, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,511 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 9.0 (TID 115) in 17 ms on algo-2 (executor 1) (98/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,513 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 9.0 (TID 120, algo-2, executor 1, partition 103, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,514 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 9.0 (TID 113) in 23 ms on algo-2 (executor 1) (99/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,514 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 9.0 (TID 121, algo-2, executor 1, partition 104, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,514 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 9.0 (TID 117) in 15 ms on algo-2 (executor 1) (100/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,522 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 9.0 (TID 122, algo-2, executor 1, partition 105, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,523 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 9.0 (TID 118) in 15 ms on algo-2 (executor 1) (101/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,523 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 9.0 (TID 123, algo-2, executor 1, partition 106, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,523 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 9.0 (TID 116) in 28 ms on algo-2 (executor 1) (102/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,523 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 9.0 (TID 124, algo-2, executor 1, partition 107, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,524 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 9.0 (TID 119) in 14 ms on algo-2 (executor 1) (103/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,526 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 9.0 (TID 125, algo-2, executor 1, partition 108, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,526 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 9.0 (TID 120) in 13 ms on algo-2 (executor 1) (104/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,529 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 9.0 (TID 126, algo-2, executor 1, partition 109, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,529 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 9.0 (TID 121) in 15 ms on algo-2 (executor 1) (105/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,535 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 9.0 (TID 127, algo-2, executor 1, partition 110, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,535 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 9.0 (TID 122) in 13 ms on algo-2 (executor 1) (106/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,537 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 9.0 (TID 128, algo-2, executor 1, partition 111, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,537 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 9.0 (TID 129, algo-2, executor 1, partition 112, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,538 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 9.0 (TID 123) in 14 ms on algo-2 (executor 1) (107/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,538 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 9.0 (TID 124) in 15 ms on algo-2 (executor 1) (108/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,538 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 9.0 (TID 130, algo-2, executor 1, partition 113, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,539 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 9.0 (TID 125) in 14 ms on algo-2 (executor 1) (109/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,545 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 9.0 (TID 131, algo-2, executor 1, partition 114, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,545 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 9.0 (TID 126) in 16 ms on algo-2 (executor 1) (110/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,548 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 9.0 (TID 132, algo-2, executor 1, partition 115, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,548 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 9.0 (TID 128) in 11 ms on algo-2 (executor 1) (111/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,548 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 9.0 (TID 133, algo-2, executor 1, partition 116, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,549 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 9.0 (TID 127) in 14 ms on algo-2 (executor 1) (112/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,550 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 9.0 (TID 134, algo-2, executor 1, partition 117, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,551 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 9.0 (TID 129) in 14 ms on algo-2 (executor 1) (113/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,551 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 9.0 (TID 135, algo-2, executor 1, partition 118, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,552 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 9.0 (TID 130) in 13 ms on algo-2 (executor 1) (114/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,561 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 9.0 (TID 136, algo-2, executor 1, partition 119, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,561 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 9.0 (TID 131) in 16 ms on algo-2 (executor 1) (115/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,566 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 9.0 (TID 137, algo-2, executor 1, partition 120, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,566 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 9.0 (TID 133) in 18 ms on algo-2 (executor 1) (116/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,567 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 9.0 (TID 138, algo-2, executor 1, partition 121, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,567 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 9.0 (TID 134) in 17 ms on algo-2 (executor 1) (117/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,568 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 9.0 (TID 139, algo-2, executor 1, partition 122, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,569 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 9.0 (TID 135) in 18 ms on algo-2 (executor 1) (118/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,569 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 9.0 (TID 140, algo-2, executor 1, partition 123, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,569 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 9.0 (TID 132) in 21 ms on algo-2 (executor 1) (119/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,572 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 9.0 (TID 141, algo-2, executor 1, partition 124, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,572 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 9.0 (TID 136) in 11 ms on algo-2 (executor 1) (120/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,580 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 9.0 (TID 142, algo-2, executor 1, partition 125, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,580 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 9.0 (TID 137) in 14 ms on algo-2 (executor 1) (121/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,580 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 9.0 (TID 143, algo-2, executor 1, partition 126, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,581 INFO scheduler.TaskSetManager: Finished task 121.0 in stage 9.0 (TID 138) in 14 ms on algo-2 (executor 1) (122/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,585 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 9.0 (TID 144, algo-2, executor 1, partition 127, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,585 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 9.0 (TID 140) in 16 ms on algo-2 (executor 1) (123/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,585 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 9.0 (TID 145, algo-2, executor 1, partition 128, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,585 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 9.0 (TID 141) in 13 ms on algo-2 (executor 1) (124/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,586 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 9.0 (TID 146, algo-2, executor 1, partition 129, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,586 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 9.0 (TID 139) in 18 ms on algo-2 (executor 1) (125/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,594 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 9.0 (TID 147, algo-2, executor 1, partition 130, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,594 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 9.0 (TID 143) in 14 ms on algo-2 (executor 1) (126/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,596 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 9.0 (TID 148, algo-2, executor 1, partition 131, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,596 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 9.0 (TID 142) in 16 ms on algo-2 (executor 1) (127/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,598 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 9.0 (TID 149, algo-2, executor 1, partition 132, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,598 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 9.0 (TID 145) in 13 ms on algo-2 (executor 1) (128/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,599 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 9.0 (TID 150, algo-2, executor 1, partition 133, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,599 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 9.0 (TID 146) in 13 ms on algo-2 (executor 1) (129/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,603 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 9.0 (TID 151, algo-2, executor 1, partition 134, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,603 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 9.0 (TID 144) in 19 ms on algo-2 (executor 1) (130/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,608 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 9.0 (TID 152, algo-2, executor 1, partition 135, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,608 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 9.0 (TID 147) in 14 ms on algo-2 (executor 1) (131/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,609 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 9.0 (TID 153, algo-2, executor 1, partition 136, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,609 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 9.0 (TID 148) in 13 ms on algo-2 (executor 1) (132/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,611 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 9.0 (TID 154, algo-2, executor 1, partition 137, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,611 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 9.0 (TID 149) in 13 ms on algo-2 (executor 1) (133/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,615 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 9.0 (TID 155, algo-2, executor 1, partition 138, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,615 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 9.0 (TID 151) in 13 ms on algo-2 (executor 1) (134/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,616 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 9.0 (TID 156, algo-2, executor 1, partition 139, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,616 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 9.0 (TID 150) in 17 ms on algo-2 (executor 1) (135/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,619 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 9.0 (TID 157, algo-2, executor 1, partition 140, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,619 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 9.0 (TID 152) in 11 ms on algo-2 (executor 1) (136/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,622 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 9.0 (TID 158, algo-2, executor 1, partition 141, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,622 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 9.0 (TID 153) in 14 ms on algo-2 (executor 1) (137/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,623 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 9.0 (TID 159, algo-2, executor 1, partition 142, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,624 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 9.0 (TID 154) in 12 ms on algo-2 (executor 1) (138/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,627 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 9.0 (TID 160, algo-2, executor 1, partition 143, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,628 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 9.0 (TID 155) in 12 ms on algo-2 (executor 1) (139/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,629 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 9.0 (TID 161, algo-2, executor 1, partition 144, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,629 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 9.0 (TID 156) in 13 ms on algo-2 (executor 1) (140/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,634 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 9.0 (TID 162, algo-2, executor 1, partition 145, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,634 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 9.0 (TID 157) in 15 ms on algo-2 (executor 1) (141/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,634 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 9.0 (TID 163, algo-2, executor 1, partition 146, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,634 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 9.0 (TID 158) in 12 ms on algo-2 (executor 1) (142/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,639 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 9.0 (TID 164, algo-2, executor 1, partition 147, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,639 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 9.0 (TID 165, algo-2, executor 1, partition 148, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,640 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 9.0 (TID 160) in 12 ms on algo-2 (executor 1) (143/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,640 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 9.0 (TID 159) in 17 ms on algo-2 (executor 1) (144/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,642 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 9.0 (TID 166, algo-2, executor 1, partition 149, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,642 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 9.0 (TID 161) in 13 ms on algo-2 (executor 1) (145/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,647 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 9.0 (TID 167, algo-2, executor 1, partition 150, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,647 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 9.0 (TID 163) in 13 ms on algo-2 (executor 1) (146/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,649 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 9.0 (TID 168, algo-2, executor 1, partition 151, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,649 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 9.0 (TID 162) in 16 ms on algo-2 (executor 1) (147/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,651 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 9.0 (TID 169, algo-2, executor 1, partition 152, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,652 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 9.0 (TID 165) in 13 ms on algo-2 (executor 1) (148/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,652 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 9.0 (TID 170, algo-2, executor 1, partition 153, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,653 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 9.0 (TID 164) in 14 ms on algo-2 (executor 1) (149/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,654 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 9.0 (TID 171, algo-2, executor 1, partition 154, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,654 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 9.0 (TID 166) in 12 ms on algo-2 (executor 1) (150/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,658 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 9.0 (TID 172, algo-2, executor 1, partition 155, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,659 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 9.0 (TID 167) in 12 ms on algo-2 (executor 1) (151/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,663 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 9.0 (TID 173, algo-2, executor 1, partition 156, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,663 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 9.0 (TID 169) in 12 ms on algo-2 (executor 1) (152/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,663 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 9.0 (TID 174, algo-2, executor 1, partition 157, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,664 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 9.0 (TID 170) in 12 ms on algo-2 (executor 1) (153/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,665 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 9.0 (TID 175, algo-2, executor 1, partition 158, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,665 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 9.0 (TID 168) in 16 ms on algo-2 (executor 1) (154/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,666 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 9.0 (TID 176, algo-2, executor 1, partition 159, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,667 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 9.0 (TID 171) in 13 ms on algo-2 (executor 1) (155/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,669 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 9.0 (TID 177, algo-2, executor 1, partition 160, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,669 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 9.0 (TID 172) in 11 ms on algo-2 (executor 1) (156/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,675 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 9.0 (TID 178, algo-2, executor 1, partition 161, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,675 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 9.0 (TID 173) in 13 ms on algo-2 (executor 1) (157/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,675 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 9.0 (TID 179, algo-2, executor 1, partition 162, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,676 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 9.0 (TID 174) in 13 ms on algo-2 (executor 1) (158/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,676 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 9.0 (TID 180, algo-2, executor 1, partition 163, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,676 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 9.0 (TID 175) in 11 ms on algo-2 (executor 1) (159/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,678 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 9.0 (TID 181, algo-2, executor 1, partition 164, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,678 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 9.0 (TID 176) in 12 ms on algo-2 (executor 1) (160/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,680 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 9.0 (TID 182, algo-2, executor 1, partition 165, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,680 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 9.0 (TID 177) in 11 ms on algo-2 (executor 1) (161/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,689 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 9.0 (TID 183, algo-2, executor 1, partition 166, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,689 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 9.0 (TID 178) in 15 ms on algo-2 (executor 1) (162/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,693 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 9.0 (TID 184, algo-2, executor 1, partition 167, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,693 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 9.0 (TID 180) in 17 ms on algo-2 (executor 1) (163/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,694 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 9.0 (TID 185, algo-2, executor 1, partition 168, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,694 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 9.0 (TID 179) in 19 ms on algo-2 (executor 1) (164/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,695 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 9.0 (TID 186, algo-2, executor 1, partition 169, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,695 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 9.0 (TID 181) in 17 ms on algo-2 (executor 1) (165/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,695 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 9.0 (TID 187, algo-2, executor 1, partition 170, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,696 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 9.0 (TID 182) in 16 ms on algo-2 (executor 1) (166/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,703 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 9.0 (TID 188, algo-2, executor 1, partition 171, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,703 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 9.0 (TID 183) in 14 ms on algo-2 (executor 1) (167/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,709 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 9.0 (TID 189, algo-2, executor 1, partition 172, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,709 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 9.0 (TID 184) in 16 ms on algo-2 (executor 1) (168/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,710 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 9.0 (TID 190, algo-2, executor 1, partition 173, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,710 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 9.0 (TID 186) in 15 ms on algo-2 (executor 1) (169/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,711 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 9.0 (TID 191, algo-2, executor 1, partition 174, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,711 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 9.0 (TID 185) in 17 ms on algo-2 (executor 1) (170/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,711 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 9.0 (TID 192, algo-2, executor 1, partition 175, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,712 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 9.0 (TID 187) in 17 ms on algo-2 (executor 1) (171/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,719 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 9.0 (TID 193, algo-2, executor 1, partition 176, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,719 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 9.0 (TID 188) in 16 ms on algo-2 (executor 1) (172/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,720 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 9.0 (TID 194, algo-2, executor 1, partition 177, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,721 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 9.0 (TID 189) in 12 ms on algo-2 (executor 1) (173/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,722 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 9.0 (TID 195, algo-2, executor 1, partition 178, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,723 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 9.0 (TID 192) in 12 ms on algo-2 (executor 1) (174/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,724 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 9.0 (TID 196, algo-2, executor 1, partition 179, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,724 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 9.0 (TID 191) in 13 ms on algo-2 (executor 1) (175/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,725 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 9.0 (TID 197, algo-2, executor 1, partition 180, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,725 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 9.0 (TID 190) in 15 ms on algo-2 (executor 1) (176/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,729 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 9.0 (TID 198, algo-2, executor 1, partition 181, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,729 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 9.0 (TID 193) in 10 ms on algo-2 (executor 1) (177/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,734 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 9.0 (TID 199, algo-2, executor 1, partition 182, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,734 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 9.0 (TID 194) in 14 ms on algo-2 (executor 1) (178/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,735 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 9.0 (TID 200, algo-2, executor 1, partition 183, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,735 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 9.0 (TID 195) in 13 ms on algo-2 (executor 1) (179/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,736 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 9.0 (TID 201, algo-2, executor 1, partition 184, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,736 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 9.0 (TID 196) in 12 ms on algo-2 (executor 1) (180/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,739 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 9.0 (TID 202, algo-2, executor 1, partition 185, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,739 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 9.0 (TID 198) in 10 ms on algo-2 (executor 1) (181/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,746 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 9.0 (TID 203, algo-2, executor 1, partition 186, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,746 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 9.0 (TID 204, algo-2, executor 1, partition 187, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,747 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 9.0 (TID 199) in 13 ms on algo-2 (executor 1) (182/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,747 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 9.0 (TID 205, algo-2, executor 1, partition 188, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,747 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 9.0 (TID 200) in 12 ms on algo-2 (executor 1) (183/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,747 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 9.0 (TID 197) in 22 ms on algo-2 (executor 1) (184/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,750 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 9.0 (TID 206, algo-2, executor 1, partition 189, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,750 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 9.0 (TID 202) in 11 ms on algo-2 (executor 1) (185/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,751 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 9.0 (TID 207, algo-2, executor 1, partition 190, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,751 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 9.0 (TID 201) in 15 ms on algo-2 (executor 1) (186/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,759 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 9.0 (TID 208, algo-2, executor 1, partition 191, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,759 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 9.0 (TID 204) in 13 ms on algo-2 (executor 1) (187/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,761 INFO scheduler.TaskSetManager: Starting task 192.0 in stage 9.0 (TID 209, algo-2, executor 1, partition 192, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,761 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 9.0 (TID 206) in 12 ms on algo-2 (executor 1) (188/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,763 INFO scheduler.TaskSetManager: Starting task 193.0 in stage 9.0 (TID 210, algo-2, executor 1, partition 193, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,763 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 9.0 (TID 205) in 16 ms on algo-2 (executor 1) (189/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,763 INFO scheduler.TaskSetManager: Starting task 194.0 in stage 9.0 (TID 211, algo-2, executor 1, partition 194, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,763 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 9.0 (TID 203) in 17 ms on algo-2 (executor 1) (190/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,765 INFO scheduler.TaskSetManager: Starting task 195.0 in stage 9.0 (TID 212, algo-2, executor 1, partition 195, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,765 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 9.0 (TID 207) in 14 ms on algo-2 (executor 1) (191/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,771 INFO scheduler.TaskSetManager: Starting task 196.0 in stage 9.0 (TID 213, algo-2, executor 1, partition 196, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,772 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 9.0 (TID 208) in 14 ms on algo-2 (executor 1) (192/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,772 INFO scheduler.TaskSetManager: Starting task 197.0 in stage 9.0 (TID 214, algo-2, executor 1, partition 197, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,772 INFO scheduler.TaskSetManager: Finished task 192.0 in stage 9.0 (TID 209) in 11 ms on algo-2 (executor 1) (193/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,773 INFO scheduler.TaskSetManager: Starting task 198.0 in stage 9.0 (TID 215, algo-2, executor 1, partition 198, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,773 INFO scheduler.TaskSetManager: Finished task 193.0 in stage 9.0 (TID 210) in 11 ms on algo-2 (executor 1) (194/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,775 INFO scheduler.TaskSetManager: Starting task 199.0 in stage 9.0 (TID 216, algo-2, executor 1, partition 199, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,775 INFO scheduler.TaskSetManager: Finished task 194.0 in stage 9.0 (TID 211) in 12 ms on algo-2 (executor 1) (195/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,779 INFO scheduler.TaskSetManager: Finished task 195.0 in stage 9.0 (TID 212) in 14 ms on algo-2 (executor 1) (196/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,782 INFO scheduler.TaskSetManager: Finished task 196.0 in stage 9.0 (TID 213) in 11 ms on algo-2 (executor 1) (197/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,788 INFO scheduler.TaskSetManager: Finished task 199.0 in stage 9.0 (TID 216) in 13 ms on algo-2 (executor 1) (198/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,790 INFO scheduler.TaskSetManager: Finished task 197.0 in stage 9.0 (TID 214) in 18 ms on algo-2 (executor 1) (199/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,790 INFO scheduler.TaskSetManager: Finished task 198.0 in stage 9.0 (TID 215) in 17 ms on algo-2 (executor 1) (200/200)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,790 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,790 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (collect at AnalysisRunner.scala:499) finished in 0.701 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,790 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,790 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,790 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,790 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,791 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:499), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,793 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.5 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,794 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.4 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,795 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.68.27:42927 (size: 4.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,795 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,795 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:499) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,795 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,796 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 217, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,802 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:36157 (size: 4.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,804 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,850 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 217) in 54 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,850 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,851 INFO scheduler.DAGScheduler: ResultStage 10 (collect at AnalysisRunner.scala:499) finished in 0.059 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,851 INFO scheduler.DAGScheduler: Job 4 finished: collect at AnalysisRunner.scala:499, took 2.365630 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,924 INFO codegen.CodeGenerator: Code generated in 14.472186 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,953 INFO codegen.CodeGenerator: Code generated in 6.645695 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:04,960 INFO codegen.CodeGenerator: Code generated in 5.082769 ms\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|check       |check_level|check_status|constraint                                                                                                                                         |constraint_status|constraint_message|\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |SizeConstraint(Size(None))                                                                                                                         |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |MinimumConstraint(Minimum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |MaximumConstraint(Maximum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |CompletenessConstraint(Completeness(review_id,None))                                                                                               |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |UniquenessConstraint(Uniqueness(List(review_id)))                                                                                                  |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |CompletenessConstraint(Completeness(marketplace,None))                                                                                             |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |ComplianceConstraint(Compliance(marketplace contained in US,UK,DE,JP,FR,`marketplace` IS NULL OR `marketplace` IN ('US','UK','DE','JP','FR'),None))|Success          |                  |\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,027 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,027 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,027 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,027 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200725185405_0000}; taskId=attempt_20200725185405_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@263d98c1}; outputPath=s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-checks, workPath=s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-checks/_temporary/0/_temporary/attempt_20200725185405_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-checks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,027 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,296 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:110\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,297 INFO scheduler.DAGScheduler: Registering RDD 36 (csv at preprocess-deequ.scala:110) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,297 INFO scheduler.DAGScheduler: Got job 5 (csv at preprocess-deequ.scala:110) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,297 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (csv at preprocess-deequ.scala:110)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,297 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,297 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,297 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[36] at csv at preprocess-deequ.scala:110), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,299 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.2 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,300 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.1 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,300 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.68.27:42927 (size: 3.1 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,300 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,301 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[36] at csv at preprocess-deequ.scala:110) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,301 INFO cluster.YarnScheduler: Adding task set 11.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,301 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 218, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8156 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,302 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 219, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8172 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,302 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 11.0 (TID 220, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8341 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,302 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 11.0 (TID 221, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8180 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,302 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 11.0 (TID 222, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8448 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,308 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:36157 (size: 3.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,313 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 219) in 12 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,313 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 11.0 (TID 220) in 11 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 11.0 (TID 221) in 12 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 218) in 13 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 11.0 (TID 222) in 12 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (csv at preprocess-deequ.scala:110) finished in 0.016 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,314 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (ShuffledRowRDD[37] at csv at preprocess-deequ.scala:110), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,335 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 245.3 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,336 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 90.5 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,337 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.68.27:42927 (size: 90.5 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,337 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,337 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (ShuffledRowRDD[37] at csv at preprocess-deequ.scala:110) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,337 INFO cluster.YarnScheduler: Adding task set 12.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,338 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 223, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,342 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:36157 (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:05,355 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,246 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 223) in 908 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,247 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,247 INFO scheduler.DAGScheduler: ResultStage 12 (csv at preprocess-deequ.scala:110) finished in 0.932 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,247 INFO scheduler.DAGScheduler: Job 5 finished: csv at preprocess-deequ.scala:110, took 0.951216 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,654 INFO datasources.FileFormatWriter: Write Job 4266551e-710b-4db5-9396-3d0df489902c committed.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,654 INFO datasources.FileFormatWriter: Finished processing stats for write job 4266551e-710b-4db5-9396-3d0df489902c.\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|entity |instance                               |name        |value   |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Uniqueness  |1.0     |\u001b[0m\n",
      "\u001b[34m|Dataset|*                                      |Size        |247515.0|\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Maximum     |5.0     |\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Minimum     |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace contained in US,UK,DE,JP,FR|Compliance  |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace                            |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,763 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,763 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,763 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,763 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200725185406_0000}; taskId=attempt_20200725185406_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2922851}; outputPath=s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/success-metrics, workPath=s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/success-metrics/_temporary/0/_temporary/attempt_20200725185406_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/success-metrics\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:06,763 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,057 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 319\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 422\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 364\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 346\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 329\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 375\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 345\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 339\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,058 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,059 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.68.27:42927 in memory (size: 3.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,061 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:36157 in memory (size: 3.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,062 INFO spark.ContextCleaner: Cleaned accumulator 305\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,062 INFO spark.ContextCleaner: Cleaned accumulator 395\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,062 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,062 INFO spark.ContextCleaner: Cleaned accumulator 338\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,063 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.68.27:42927 in memory (size: 4.4 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,064 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:36157 in memory (size: 4.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 408\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 312\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 384\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 321\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 378\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 322\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 335\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 340\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 414\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 342\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 388\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,066 INFO spark.ContextCleaner: Cleaned accumulator 380\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,067 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.68.27:42927 in memory (size: 3.1 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,067 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:36157 in memory (size: 3.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 391\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 365\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 299\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 366\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 402\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 323\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 425\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 310\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 356\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 382\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 421\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 383\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 308\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 392\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 353\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 361\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 377\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 369\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.ContextCleaner: Cleaned accumulator 294\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,069 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:122\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,070 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.68.27:42927 in memory (size: 12.7 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,071 INFO scheduler.DAGScheduler: Registering RDD 42 (csv at preprocess-deequ.scala:122) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,071 INFO scheduler.DAGScheduler: Got job 6 (csv at preprocess-deequ.scala:122) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,071 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:36157 in memory (size: 12.7 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,071 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (csv at preprocess-deequ.scala:122)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,071 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,071 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,071 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[42] at csv at preprocess-deequ.scala:122), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,073 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,074 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,074 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.68.27:42927 (size: 3.0 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,074 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,074 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[42] at csv at preprocess-deequ.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,074 INFO cluster.YarnScheduler: Adding task set 13.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,075 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 224, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,075 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 225, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,075 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 13.0 (TID 226, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8181 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,075 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 13.0 (TID 227, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,075 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 13.0 (TID 228, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8229 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 397\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 332\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 390\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 360\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 410\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 358\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 370\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 275\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 419\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 302\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 385\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 344\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 307\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 415\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned shuffle 4\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 386\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 409\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,077 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,079 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.68.27:42927 in memory (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,080 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:36157 in memory (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:36157 (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 411\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 282\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 423\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 309\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 313\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 427\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 362\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 269\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,083 INFO spark.ContextCleaner: Cleaned accumulator 324\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,084 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.68.27:42927 in memory (size: 14.1 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,086 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:36157 in memory (size: 14.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,087 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 13.0 (TID 226) in 12 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,087 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 13.0 (TID 228) in 12 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,087 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 225) in 12 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,087 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 13.0 (TID 227) in 12 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,088 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 224) in 13 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,088 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,088 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (csv at preprocess-deequ.scala:122) finished in 0.017 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,088 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,088 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,088 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 14)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,088 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,089 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (ShuffledRowRDD[43] at csv at preprocess-deequ.scala:122), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 317\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 426\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 295\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 413\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 351\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 318\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 268\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 367\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 303\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 352\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 280\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,091 INFO spark.ContextCleaner: Cleaned accumulator 297\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,094 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-2:36157 in memory (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,095 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.68.27:42927 in memory (size: 90.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 320\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 270\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 394\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 412\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 418\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned shuffle 5\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 416\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 286\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,100 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,103 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:36157 in memory (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,104 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.68.27:42927 in memory (size: 42.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,107 INFO spark.ContextCleaner: Cleaned accumulator 272\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,107 INFO spark.ContextCleaner: Cleaned accumulator 279\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,107 INFO spark.ContextCleaner: Cleaned accumulator 337\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,107 INFO spark.ContextCleaner: Cleaned accumulator 288\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,108 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.68.27:42927 in memory (size: 6.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,109 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:36157 in memory (size: 6.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,112 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 245.1 KB, free 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 314\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 348\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 298\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 285\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 376\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 341\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 359\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 283\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 296\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 400\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 281\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 374\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 420\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 304\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 371\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 290\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 379\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 315\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 284\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 406\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 311\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 274\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 404\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 405\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 271\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 316\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 403\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 289\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 354\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 336\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 387\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,113 INFO spark.ContextCleaner: Cleaned accumulator 399\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned shuffle 6\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 327\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 90.4 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 372\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 331\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 330\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 326\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 287\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 276\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 292\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 363\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 417\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 291\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 277\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 347\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 424\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 368\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 389\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.68.27:42927 (size: 90.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 396\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 350\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 381\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 428\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 293\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 398\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 401\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 301\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 357\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 333\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 355\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 334\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 300\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 273\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 325\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned accumulator 349\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO spark.ContextCleaner: Cleaned shuffle 3\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,114 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (ShuffledRowRDD[43] at csv at preprocess-deequ.scala:122) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO spark.ContextCleaner: Cleaned accumulator 343\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO spark.ContextCleaner: Cleaned accumulator 373\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO spark.ContextCleaner: Cleaned accumulator 407\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO spark.ContextCleaner: Cleaned accumulator 393\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO spark.ContextCleaner: Cleaned accumulator 328\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO spark.ContextCleaner: Cleaned accumulator 306\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO spark.ContextCleaner: Cleaned accumulator 278\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,115 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 229, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,120 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:36157 (size: 90.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:07,129 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,080 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 229) in 965 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,080 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,080 INFO scheduler.DAGScheduler: ResultStage 14 (csv at preprocess-deequ.scala:122) finished in 0.991 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,080 INFO scheduler.DAGScheduler: Job 6 finished: csv at preprocess-deequ.scala:122, took 1.011089 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,376 INFO datasources.FileFormatWriter: Write Job 417f5ad6-df86-46f6-a12f-f0deee0890e4 committed.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,376 INFO datasources.FileFormatWriter: Finished processing stats for write job 417f5ad6-df86-46f6-a12f-f0deee0890e4.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,540 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,540 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,540 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,541 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,581 INFO codegen.CodeGenerator: Code generated in 11.178821 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,588 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 400.9 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,599 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 42.8 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,599 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.68.27:42927 (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,600 INFO spark.SparkContext: Created broadcast 19 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,600 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,667 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,668 INFO scheduler.DAGScheduler: Registering RDD 49 (collect at AnalysisRunner.scala:303) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,668 INFO scheduler.DAGScheduler: Got job 7 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,668 INFO scheduler.DAGScheduler: Final stage: ResultStage 16 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,668 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,668 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,668 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,673 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 143.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,675 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 45.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,675 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.68.27:42927 (size: 45.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,675 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,676 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,676 INFO cluster.YarnScheduler: Adding task set 15.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,677 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 230, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,677 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 231, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,683 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:36157 (size: 45.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:08,711 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:36157 (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,129 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 231) in 3452 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,911 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 230) in 4234 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,912 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,912 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:303) finished in 4.243 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,912 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,912 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,912 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 16)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,912 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,912 INFO scheduler.DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,917 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 174.5 KB, free 364.7 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,918 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 56.4 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,918 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.68.27:42927 (size: 56.4 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,919 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,919 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,919 INFO cluster.YarnScheduler: Adding task set 16.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,920 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 232, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,925 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-2:36157 (size: 56.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:12,930 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,079 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 232) in 160 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,079 INFO cluster.YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,080 INFO scheduler.DAGScheduler: ResultStage 16 (collect at AnalysisRunner.scala:303) finished in 0.167 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,080 INFO scheduler.DAGScheduler: Job 7 finished: collect at AnalysisRunner.scala:303, took 4.412667 s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 18:54:13,186 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,186 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,186 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: string, product_parent: string, star_rating: int, helpful_votes: int, total_votes: int ... 3 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,187 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,243 INFO codegen.CodeGenerator: Code generated in 24.41274 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,254 INFO codegen.CodeGenerator: Code generated in 5.862414 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,257 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 400.9 KB, free 364.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,268 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,268 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.68.27:42927 (size: 42.8 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,269 INFO spark.SparkContext: Created broadcast 22 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,269 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,293 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,294 INFO scheduler.DAGScheduler: Registering RDD 56 (collect at AnalysisRunner.scala:303) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,294 INFO scheduler.DAGScheduler: Got job 8 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,294 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,294 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,294 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,295 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[56] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,296 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.2 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,297 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 13.9 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,297 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.68.27:42927 (size: 13.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,298 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,298 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[56] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,298 INFO cluster.YarnScheduler: Adding task set 17.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,298 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 233, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,299 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 234, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,304 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-2:36157 (size: 13.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:13,333 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-2:36157 (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:14,871 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 234) in 1572 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,318 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 233) in 2020 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,318 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,319 INFO scheduler.DAGScheduler: ShuffleMapStage 17 (collect at AnalysisRunner.scala:303) finished in 2.024 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,319 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,319 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,319 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 18)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,319 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,319 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[59] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,321 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 38.2 KB, free 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,322 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 15.5 KB, free 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,323 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.68.27:42927 (size: 15.5 KB, free: 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,323 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,323 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[59] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,323 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,324 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 235, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,328 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-2:36157 (size: 15.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,331 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,385 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 235) in 61 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,385 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,385 INFO scheduler.DAGScheduler: ResultStage 18 (collect at AnalysisRunner.scala:303) finished in 0.065 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,386 INFO scheduler.DAGScheduler: Job 8 finished: collect at AnalysisRunner.scala:303, took 2.092281 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,425 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,425 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,426 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,426 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,432 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 400.9 KB, free 363.7 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 498\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 520\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 582\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 499\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 555\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 449\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 503\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 610\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 484\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 539\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 501\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 526\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 443\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 493\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 571\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 487\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 561\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 542\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 556\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 486\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 512\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 516\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 492\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 507\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 508\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 433\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 502\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 590\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 586\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 537\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 448\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 471\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 458\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 577\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 444\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 524\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 442\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 510\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 541\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 538\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 594\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 477\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 464\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 573\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 596\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 523\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 521\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 465\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 548\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 564\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 570\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 432\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 466\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 430\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 440\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 545\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 509\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 591\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 552\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 581\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 535\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 565\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 589\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 604\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 460\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 536\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 587\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,447 INFO spark.ContextCleaner: Cleaned accumulator 514\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,448 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.68.27:42927 in memory (size: 15.5 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,450 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-2:36157 in memory (size: 15.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 467\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 601\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 603\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 598\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 494\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 605\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 549\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 497\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 575\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 489\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,451 INFO spark.ContextCleaner: Cleaned accumulator 611\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,453 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.68.27:42927 in memory (size: 42.8 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,453 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-2:36157 in memory (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 609\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 513\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 518\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 569\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 496\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 607\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 452\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 531\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 506\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 562\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 522\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 441\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 547\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 490\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 533\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 583\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 530\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 462\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 527\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 588\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 566\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 504\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 463\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,454 INFO spark.ContextCleaner: Cleaned accumulator 579\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned shuffle 8\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 478\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 584\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 450\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 472\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 483\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 567\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 436\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 469\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 550\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 597\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 578\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 454\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 435\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,455 INFO spark.ContextCleaner: Cleaned accumulator 453\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,456 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.68.27:42927 in memory (size: 56.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,457 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-2:36157 in memory (size: 56.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,457 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 42.8 KB, free 364.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,458 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.68.27:42927 (size: 42.8 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,458 INFO spark.ContextCleaner: Cleaned accumulator 593\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,458 INFO spark.ContextCleaner: Cleaned accumulator 528\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,458 INFO spark.ContextCleaner: Cleaned accumulator 600\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,458 INFO spark.ContextCleaner: Cleaned accumulator 560\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,458 INFO spark.ContextCleaner: Cleaned accumulator 437\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,458 INFO spark.SparkContext: Created broadcast 25 from rdd at ColumnProfiler.scala:533\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,458 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,459 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.68.27:42927 in memory (size: 3.0 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,460 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-2:36157 in memory (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,462 INFO spark.ContextCleaner: Cleaned accumulator 572\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,462 INFO spark.ContextCleaner: Cleaned accumulator 451\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,462 INFO spark.ContextCleaner: Cleaned accumulator 456\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,464 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.68.27:42927 in memory (size: 90.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,464 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-2:36157 in memory (size: 90.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned shuffle 9\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 568\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 608\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 500\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 491\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 544\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 459\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 557\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 540\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 431\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 511\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 481\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 429\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 532\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 599\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 445\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 602\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 543\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 558\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 495\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 585\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 468\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 606\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 474\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,471 INFO spark.ContextCleaner: Cleaned accumulator 439\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,472 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.68.27:42927 in memory (size: 42.8 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,473 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-2:36157 in memory (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,475 INFO spark.ContextCleaner: Cleaned accumulator 529\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,479 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-2:36157 in memory (size: 45.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,479 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.68.27:42927 in memory (size: 45.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,484 INFO spark.ContextCleaner: Cleaned accumulator 551\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,484 INFO spark.ContextCleaner: Cleaned accumulator 455\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,484 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.68.27:42927 in memory (size: 13.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,485 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-2:36157 in memory (size: 13.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 473\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 476\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 534\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 479\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 438\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 515\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 592\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 554\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 434\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 480\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 505\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 574\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 482\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 563\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:547\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 559\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 446\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 488\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 519\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 612\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 546\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 525\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 447\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 580\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,486 INFO spark.ContextCleaner: Cleaned accumulator 457\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,487 INFO spark.ContextCleaner: Cleaned accumulator 553\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,487 INFO spark.ContextCleaner: Cleaned accumulator 470\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,487 INFO spark.ContextCleaner: Cleaned accumulator 485\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,487 INFO spark.ContextCleaner: Cleaned accumulator 595\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,487 INFO spark.ContextCleaner: Cleaned accumulator 461\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,487 INFO spark.ContextCleaner: Cleaned accumulator 517\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,487 INFO spark.ContextCleaner: Cleaned shuffle 7\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,487 INFO spark.ContextCleaner: Cleaned accumulator 475\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,487 INFO spark.ContextCleaner: Cleaned accumulator 576\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,488 INFO scheduler.DAGScheduler: Registering RDD 66 (countByKey at ColumnProfiler.scala:547) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,489 INFO scheduler.DAGScheduler: Got job 9 (countByKey at ColumnProfiler.scala:547) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,489 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (countByKey at ColumnProfiler.scala:547)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,489 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,489 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,489 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[66] at countByKey at ColumnProfiler.scala:547), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,500 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 18.5 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,501 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 9.3 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,501 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.68.27:42927 (size: 9.3 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,501 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,504 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[66] at countByKey at ColumnProfiler.scala:547) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,504 INFO cluster.YarnScheduler: Adding task set 19.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,504 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 236, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,504 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 237, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:15,510 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-2:36157 (size: 9.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:16,323 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-2:36157 (size: 42.8 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:17,982 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 237) in 2478 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,616 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 236) in 3112 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,616 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,617 INFO scheduler.DAGScheduler: ShuffleMapStage 19 (countByKey at ColumnProfiler.scala:547) finished in 3.126 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,617 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,617 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,617 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 20)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,617 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,617 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (ShuffledRDD[67] at countByKey at ColumnProfiler.scala:547), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,618 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 3.0 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,619 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1795.0 B, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,619 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.68.27:42927 (size: 1795.0 B, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,619 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,619 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 20 (ShuffledRDD[67] at countByKey at ColumnProfiler.scala:547) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,619 INFO cluster.YarnScheduler: Adding task set 20.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,621 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 238, algo-2, executor 1, partition 0, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,621 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 239, algo-2, executor 1, partition 1, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,626 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-2:36157 (size: 1795.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,630 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,641 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 239) in 19 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,641 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 238) in 21 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,641 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,641 INFO scheduler.DAGScheduler: ResultStage 20 (countByKey at ColumnProfiler.scala:547) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,641 INFO scheduler.DAGScheduler: Job 9 finished: countByKey at ColumnProfiler.scala:547, took 3.154620 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,736 INFO codegen.CodeGenerator: Code generated in 15.810173 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,762 INFO codegen.CodeGenerator: Code generated in 7.212246 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,770 INFO codegen.CodeGenerator: Code generated in 5.5951 ms\u001b[0m\n",
      "\u001b[34m+----------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|_1              |_2                                                                          |_3                                                                                  |\u001b[0m\n",
      "\u001b[34m+----------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|review_id       |'review_id' is not null                                                     |.isComplete(\"review_id\")                                                            |\u001b[0m\n",
      "\u001b[34m|customer_id     |'customer_id' is not null                                                   |.isComplete(\"customer_id\")                                                          |\u001b[0m\n",
      "\u001b[34m|customer_id     |'customer_id' has type Integral                                             |.hasDataType(\"customer_id\", ConstrainableDataTypes.Integral)                        |\u001b[0m\n",
      "\u001b[34m|customer_id     |'customer_id' has no negative values                                        |.isNonNegative(\"customer_id\")                                                       |\u001b[0m\n",
      "\u001b[34m|review_date     |'review_date' is not null                                                   |.isComplete(\"review_date\")                                                          |\u001b[0m\n",
      "\u001b[34m|helpful_votes   |'helpful_votes' is not null                                                 |.isComplete(\"helpful_votes\")                                                        |\u001b[0m\n",
      "\u001b[34m|helpful_votes   |'helpful_votes' has no negative values                                      |.isNonNegative(\"helpful_votes\")                                                     |\u001b[0m\n",
      "\u001b[34m|star_rating     |'star_rating' is not null                                                   |.isComplete(\"star_rating\")                                                          |\u001b[0m\n",
      "\u001b[34m|star_rating     |'star_rating' has no negative values                                        |.isNonNegative(\"star_rating\")                                                       |\u001b[0m\n",
      "\u001b[34m|product_title   |'product_title' is not null                                                 |.isComplete(\"product_title\")                                                        |\u001b[0m\n",
      "\u001b[34m|review_headline |'review_headline' is not null                                               |.isComplete(\"review_headline\")                                                      |\u001b[0m\n",
      "\u001b[34m|product_id      |'product_id' is not null                                                    |.isComplete(\"product_id\")                                                           |\u001b[0m\n",
      "\u001b[34m|total_votes     |'total_votes' is not null                                                   |.isComplete(\"total_votes\")                                                          |\u001b[0m\n",
      "\u001b[34m|total_votes     |'total_votes' has no negative values                                        |.isNonNegative(\"total_votes\")                                                       |\u001b[0m\n",
      "\u001b[34m|product_category|'product_category' is not null                                              |.isComplete(\"product_category\")                                                     |\u001b[0m\n",
      "\u001b[34m|product_category|'product_category' has value range 'Digital_Video_Games', 'Digital_Software'|.isContainedIn(\"product_category\", Array(\"Digital_Video_Games\", \"Digital_Software\"))|\u001b[0m\n",
      "\u001b[34m|product_parent  |'product_parent' is not null                                                |.isComplete(\"product_parent\")                                                       |\u001b[0m\n",
      "\u001b[34m|product_parent  |'product_parent' has type Integral                                          |.hasDataType(\"product_parent\", ConstrainableDataTypes.Integral)                     |\u001b[0m\n",
      "\u001b[34m|product_parent  |'product_parent' has no negative values                                     |.isNonNegative(\"product_parent\")                                                    |\u001b[0m\n",
      "\u001b[34m|review_body     |'review_body' has less than 1% missing values                               |.hasCompleteness(\"review_body\", _ >= 0.99, Some(\"It should be above 0.99!\"))        |\u001b[0m\n",
      "\u001b[34m+----------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,864 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,864 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,864 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,864 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200725185418_0000}; taskId=attempt_20200725185418_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2b9cf0a8}; outputPath=s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-suggestions, workPath=s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-suggestions/_temporary/0/_temporary/attempt_20200725185418_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-suggestions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:18,864 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,117 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:151\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,118 INFO scheduler.DAGScheduler: Registering RDD 70 (csv at preprocess-deequ.scala:151) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,119 INFO scheduler.DAGScheduler: Got job 10 (csv at preprocess-deequ.scala:151) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,119 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (csv at preprocess-deequ.scala:151)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,119 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,119 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,119 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[70] at csv at preprocess-deequ.scala:151), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,120 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.0 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,121 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.0 KB, free 365.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,121 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.68.27:42927 (size: 3.0 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,121 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,121 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[70] at csv at preprocess-deequ.scala:151) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,121 INFO cluster.YarnScheduler: Adding task set 21.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,122 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 240, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8680 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,122 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 241, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8672 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,123 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 21.0 (TID 242, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8656 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,123 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 21.0 (TID 243, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8872 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,123 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 21.0 (TID 244, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8841 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,129 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-2:36157 (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,132 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 21.0 (TID 244) in 9 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,133 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 21.0 (TID 243) in 9 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,133 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 21.0 (TID 242) in 10 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,133 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 241) in 11 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,133 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 240) in 11 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,133 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,134 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (csv at preprocess-deequ.scala:151) finished in 0.015 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,134 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,134 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,134 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 22)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,134 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,134 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (ShuffledRowRDD[71] at csv at preprocess-deequ.scala:151), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,155 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 245.1 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,156 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 90.4 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,156 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.68.27:42927 (size: 90.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,157 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,157 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (ShuffledRowRDD[71] at csv at preprocess-deequ.scala:151) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,157 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,157 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 245, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,161 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-2:36157 (size: 90.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:19,175 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.107.56:47616\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,055 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 245) in 897 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,055 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,055 INFO scheduler.DAGScheduler: ResultStage 22 (csv at preprocess-deequ.scala:151) finished in 0.921 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,055 INFO scheduler.DAGScheduler: Job 10 finished: csv at preprocess-deequ.scala:151, took 0.937638 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,463 INFO datasources.FileFormatWriter: Write Job 13239605-f5e8-4a83-9459-b2600c9b0853 committed.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,464 INFO datasources.FileFormatWriter: Finished processing stats for write job 13239605-f5e8-4a83-9459-b2600c9b0853.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,492 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,496 INFO server.AbstractConnector: Stopped Spark@74f1bae5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,497 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.68.27:4040\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,501 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,512 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,512 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,514 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,515 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,519 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,526 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,526 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,527 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,531 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,545 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,545 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,546 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9b0df38c-1ac5-4c62-911d-afd941d13331\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,549 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9b0df38c-1ac5-4c62-911d-afd941d13331/pyspark-59022a25-84f5-42f7-b7ec-8d184d5f13fa\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,552 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-91f8e50e-b3e7-44b9-b812-dcbe0b8988f0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,555 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,556 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:54:20,556 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2020-07-25 18:54:21\u001b[0m\n",
      "\u001b[35mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mReceived end of job signal, exiting...\u001b[0m\n",
      "\u001b[34mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output \n",
    "\n",
    "## These are the quality checks on our dataset.\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 18:54:07          0 amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-checks/_SUCCESS\r\n",
      "2020-07-25 18:54:06        768 amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-checks/part-00000-c58055c7-5656-4ed3-a361-0efa8f8a1bde-c000.csv\r\n",
      "2020-07-25 18:54:21          0 amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-suggestions/_SUCCESS\r\n",
      "2020-07-25 18:54:20       2289 amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-suggestions/part-00000-a64adfea-ee96-4e1b-979e-f6345649748c-c000.csv\r\n",
      "2020-07-25 18:54:00          0 amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/dataset-metrics/_SUCCESS\r\n",
      "2020-07-25 18:53:59        364 amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/dataset-metrics/part-00000-b0d06b04-b7e1-40e2-a223-3272ca996b84-c000.csv\r\n",
      "2020-07-25 18:54:09          0 amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/success-metrics/_SUCCESS\r\n",
      "2020-07-25 18:54:08        277 amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/success-metrics/part-00000-dbcdf403-ebb8-4efb-8e41-6c8416b2906a-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive $s3_output_analyze_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Output from S3 to Local\n",
    "* dataset-metrics/\n",
    "* constraint-checks/\n",
    "* success-metrics/\n",
    "* constraint-suggestions/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 364 Bytes/3.6 KiB (10.5 KiB/s) with 4 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/dataset-metrics/part-00000-b0d06b04-b7e1-40e2-a223-3272ca996b84-c000.csv to amazon-reviews-spark-analyzer/dataset-metrics/part-00000-b0d06b04-b7e1-40e2-a223-3272ca996b84-c000.csv\r\n",
      "Completed 364 Bytes/3.6 KiB (10.5 KiB/s) with 3 file(s) remaining\r",
      "Completed 1.1 KiB/3.6 KiB (25.4 KiB/s) with 3 file(s) remaining  \r",
      "download: s3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-checks/part-00000-c58055c7-5656-4ed3-a361-0efa8f8a1bde-c000.csv to amazon-reviews-spark-analyzer/constraint-checks/part-00000-c58055c7-5656-4ed3-a361-0efa8f8a1bde-c000.csv\r\n",
      "Completed 1.1 KiB/3.6 KiB (25.4 KiB/s) with 2 file(s) remaining\r",
      "Completed 3.3 KiB/3.6 KiB (61.1 KiB/s) with 2 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/constraint-suggestions/part-00000-a64adfea-ee96-4e1b-979e-f6345649748c-c000.csv to amazon-reviews-spark-analyzer/constraint-suggestions/part-00000-a64adfea-ee96-4e1b-979e-f6345649748c-c000.csv\r\n",
      "Completed 3.3 KiB/3.6 KiB (61.1 KiB/s) with 1 file(s) remaining\r",
      "Completed 3.6 KiB/3.6 KiB (63.2 KiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-393371431575/amazon-reviews-spark-analyzer-2020-07-25-18-49-52/output/success-metrics/part-00000-dbcdf403-ebb8-4efb-8e41-6c8416b2906a-c000.csv to amazon-reviews-spark-analyzer/success-metrics/part-00000-dbcdf403-ebb8-4efb-8e41-6c8416b2906a-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $s3_output_analyze_data ./amazon-reviews-spark-analyzer/ --exclude=\"*\" --include=\"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(path, sep, header):\n",
    "    data = pd.concat([pd.read_csv(f, sep=sep, header=header) for f in glob.glob('{}/*.csv'.format(path))], ignore_index = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>SizeConstraint(Size(None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>MinimumConstraint(Minimum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>MaximumConstraint(Maximum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>CompletenessConstraint(Completeness(review_id,...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(review_id)))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>CompletenessConstraint(Completeness(marketplac...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>ComplianceConstraint(Compliance(marketplace co...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          check                                         constraint  \\\n",
       "0  Review Check                         SizeConstraint(Size(None))   \n",
       "1  Review Check       MinimumConstraint(Minimum(star_rating,None))   \n",
       "2  Review Check       MaximumConstraint(Maximum(star_rating,None))   \n",
       "3  Review Check  CompletenessConstraint(Completeness(review_id,...   \n",
       "4  Review Check  UniquenessConstraint(Uniqueness(List(review_id)))   \n",
       "5  Review Check  CompletenessConstraint(Completeness(marketplac...   \n",
       "6  Review Check  ComplianceConstraint(Compliance(marketplace co...   \n",
       "\n",
       "  constraint_status  constraint_message  \n",
       "0           Success                 NaN  \n",
       "1           Success                 NaN  \n",
       "2           Success                 NaN  \n",
       "3           Success                 NaN  \n",
       "4           Success                 NaN  \n",
       "5           Success                 NaN  \n",
       "6           Success                 NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_constraint_checks = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-checks/', sep='\\t', header=0)\n",
    "df_constraint_checks[['check', 'constraint', 'constraint_status', 'constraint_message']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>ApproxCountDistinct</td>\n",
       "      <td>238027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,star_rating</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>-0.080881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>247515.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Mean</td>\n",
       "      <td>3.723706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>top star_rating</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>0.663338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,helpful_votes</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>0.980529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        entity                   instance                 name          value\n",
       "0       Column                  review_id         Completeness       1.000000\n",
       "1       Column                  review_id  ApproxCountDistinct  238027.000000\n",
       "2  Mutlicolumn    total_votes,star_rating          Correlation      -0.080881\n",
       "3      Dataset                          *                 Size  247515.000000\n",
       "4       Column                star_rating                 Mean       3.723706\n",
       "5       Column            top star_rating           Compliance       0.663338\n",
       "6  Mutlicolumn  total_votes,helpful_votes          Correlation       0.980529"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/dataset-metrics/', sep='\\t', header=0)\n",
    "df_dataset_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Success Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Uniqueness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>247515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Maximum</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Minimum</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace contained in US,UK,DE,JP,FR</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity                                 instance          name     value\n",
       "0   Column                                review_id  Completeness       1.0\n",
       "1   Column                                review_id    Uniqueness       1.0\n",
       "2  Dataset                                        *          Size  247515.0\n",
       "3   Column                              star_rating       Maximum       5.0\n",
       "4   Column                              star_rating       Minimum       1.0\n",
       "5   Column  marketplace contained in US,UK,DE,JP,FR    Compliance       1.0\n",
       "6   Column                              marketplace  Completeness       1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_success_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/success-metrics/', sep='\\t', header=0)\n",
    "df_success_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>description</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review_id</td>\n",
       "      <td>'review_id' is not null</td>\n",
       "      <td>.isComplete(\\review_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' is not null</td>\n",
       "      <td>.isComplete(\\customer_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' has type Integral</td>\n",
       "      <td>.hasDataType(\\customer_id\\\", ConstrainableData...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' has no negative values</td>\n",
       "      <td>.isNonNegative(\\customer_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>review_date</td>\n",
       "      <td>'review_date' is not null</td>\n",
       "      <td>.isComplete(\\review_date\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>'helpful_votes' is not null</td>\n",
       "      <td>.isComplete(\\helpful_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>'helpful_votes' has no negative values</td>\n",
       "      <td>.isNonNegative(\\helpful_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>star_rating</td>\n",
       "      <td>'star_rating' is not null</td>\n",
       "      <td>.isComplete(\\star_rating\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>star_rating</td>\n",
       "      <td>'star_rating' has no negative values</td>\n",
       "      <td>.isNonNegative(\\star_rating\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>product_title</td>\n",
       "      <td>'product_title' is not null</td>\n",
       "      <td>.isComplete(\\product_title\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>review_headline</td>\n",
       "      <td>'review_headline' is not null</td>\n",
       "      <td>.isComplete(\\review_headline\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>product_id</td>\n",
       "      <td>'product_id' is not null</td>\n",
       "      <td>.isComplete(\\product_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>total_votes</td>\n",
       "      <td>'total_votes' is not null</td>\n",
       "      <td>.isComplete(\\total_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>total_votes</td>\n",
       "      <td>'total_votes' has no negative values</td>\n",
       "      <td>.isNonNegative(\\total_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>product_category</td>\n",
       "      <td>'product_category' is not null</td>\n",
       "      <td>.isComplete(\\product_category\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>product_category</td>\n",
       "      <td>'product_category' has value range 'Digital_Vi...</td>\n",
       "      <td>.isContainedIn(\\product_category\\\", Array(\\\"Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' is not null</td>\n",
       "      <td>.isComplete(\\product_parent\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' has type Integral</td>\n",
       "      <td>.hasDataType(\\product_parent\\\", ConstrainableD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' has no negative values</td>\n",
       "      <td>.isNonNegative(\\product_parent\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_body</td>\n",
       "      <td>'review_body' has less than 1% missing values</td>\n",
       "      <td>.hasCompleteness(\\review_body\\\", _ &gt;= 0.99, So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vine</td>\n",
       "      <td>'vine' is not null</td>\n",
       "      <td>.isComplete(\\vine\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vine</td>\n",
       "      <td>'vine' has value range 'N'</td>\n",
       "      <td>.isContainedIn(\\vine\\\", Array(\\\"N\\\"))\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>marketplace</td>\n",
       "      <td>'marketplace' is not null</td>\n",
       "      <td>.isComplete(\\marketplace\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>marketplace</td>\n",
       "      <td>'marketplace' has value range 'US'</td>\n",
       "      <td>.isContainedIn(\\marketplace\\\", Array(\\\"US\\\"))\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>'verified_purchase' is not null</td>\n",
       "      <td>.isComplete(\\verified_purchase\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>'verified_purchase' has value range 'Y', 'N'</td>\n",
       "      <td>.isContainedIn(\\verified_purchase\\\", Array(\\\"Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          column_name                                        description  \\\n",
       "0           review_id                            'review_id' is not null   \n",
       "1         customer_id                          'customer_id' is not null   \n",
       "2         customer_id                    'customer_id' has type Integral   \n",
       "3         customer_id               'customer_id' has no negative values   \n",
       "4         review_date                          'review_date' is not null   \n",
       "5       helpful_votes                        'helpful_votes' is not null   \n",
       "6       helpful_votes             'helpful_votes' has no negative values   \n",
       "7         star_rating                          'star_rating' is not null   \n",
       "8         star_rating               'star_rating' has no negative values   \n",
       "9       product_title                        'product_title' is not null   \n",
       "10    review_headline                      'review_headline' is not null   \n",
       "11         product_id                           'product_id' is not null   \n",
       "12        total_votes                          'total_votes' is not null   \n",
       "13        total_votes               'total_votes' has no negative values   \n",
       "14   product_category                     'product_category' is not null   \n",
       "15   product_category  'product_category' has value range 'Digital_Vi...   \n",
       "16     product_parent                       'product_parent' is not null   \n",
       "17     product_parent                 'product_parent' has type Integral   \n",
       "18     product_parent            'product_parent' has no negative values   \n",
       "19        review_body      'review_body' has less than 1% missing values   \n",
       "20               vine                                 'vine' is not null   \n",
       "21               vine                         'vine' has value range 'N'   \n",
       "22        marketplace                          'marketplace' is not null   \n",
       "23        marketplace                 'marketplace' has value range 'US'   \n",
       "24  verified_purchase                    'verified_purchase' is not null   \n",
       "25  verified_purchase       'verified_purchase' has value range 'Y', 'N'   \n",
       "\n",
       "                                                 code  \n",
       "0                          .isComplete(\\review_id\\\")\"  \n",
       "1                        .isComplete(\\customer_id\\\")\"  \n",
       "2   .hasDataType(\\customer_id\\\", ConstrainableData...  \n",
       "3                     .isNonNegative(\\customer_id\\\")\"  \n",
       "4                        .isComplete(\\review_date\\\")\"  \n",
       "5                      .isComplete(\\helpful_votes\\\")\"  \n",
       "6                   .isNonNegative(\\helpful_votes\\\")\"  \n",
       "7                        .isComplete(\\star_rating\\\")\"  \n",
       "8                     .isNonNegative(\\star_rating\\\")\"  \n",
       "9                      .isComplete(\\product_title\\\")\"  \n",
       "10                   .isComplete(\\review_headline\\\")\"  \n",
       "11                        .isComplete(\\product_id\\\")\"  \n",
       "12                       .isComplete(\\total_votes\\\")\"  \n",
       "13                    .isNonNegative(\\total_votes\\\")\"  \n",
       "14                  .isComplete(\\product_category\\\")\"  \n",
       "15  .isContainedIn(\\product_category\\\", Array(\\\"Di...  \n",
       "16                    .isComplete(\\product_parent\\\")\"  \n",
       "17  .hasDataType(\\product_parent\\\", ConstrainableD...  \n",
       "18                 .isNonNegative(\\product_parent\\\")\"  \n",
       "19  .hasCompleteness(\\review_body\\\", _ >= 0.99, So...  \n",
       "20                              .isComplete(\\vine\\\")\"  \n",
       "21             .isContainedIn(\\vine\\\", Array(\\\"N\\\"))\"  \n",
       "22                       .isComplete(\\marketplace\\\")\"  \n",
       "23     .isContainedIn(\\marketplace\\\", Array(\\\"US\\\"))\"  \n",
       "24                 .isComplete(\\verified_purchase\\\")\"  \n",
       "25  .isContainedIn(\\verified_purchase\\\", Array(\\\"Y...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_constraint_suggestions = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-suggestions/', sep='\\t', header=0)\n",
    "df_constraint_suggestions.columns=['column_name', 'description', 'code']\n",
    "df_constraint_suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
